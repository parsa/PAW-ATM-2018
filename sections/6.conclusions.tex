\section{Conclusions and Implications for Future Work}
\label{conclusions}

%\todo[inline, color=red!50]{
%Sketch: \\
%  Recap what was done. \\
%  Highlight accomplishments. \\
%  Conclude in way that feels like the work's finished. (Effects out of this work, any big questions) \\
%  Where do the results lead? \\
%  Keep it short \\
%  "just provide enough information as to a possible research path and why the path may be important" \\
%
%Should I mention there's an idea to put AGAS on hardware in the future?
%}

%\todo{Terrible sentence. Rewrite. Should probably be 2 or 3 sentences}
In this work we introduce AGAS, its subsystems, and a method to study how the
amount of time that is spent executing AGAS code. This is an overhead that does
not exist in PGAS systems that statically resolve global references during
compilation. We observe how AGAS's most expensive operations are affected as
the number of nodes increase.

To study AGAS's behavior we chose a multiphysics AMR application called
OctoTiger that generates and works with a significant number of objects. We
identify the performance metrics that expose AGAS's performance and use the
corresponding counters in HPX's Performance Counter framework to collect
performance data from our strong scaling experiments.

%\todo{Huh? So you fixed this problem? If you addressed the problems exposed through this work, this should be presented in the results section maybe under a resolution heading. Plots experiments showing if fixed should also be included. Doesn't belong in conclusion.}
Our observations show that in the cases of the three most expensive AGAS operations, resolve
GID, route, and decrement credit, the AGAS instance on locality 0 performs an
increasing amount work as the problem is strongly scaled. Among the three, parcel routing, the operation that is
affected the most, takes 0.0016\% of
%\todo{Hardly a bottleneck. Explain.}
execution time that is spent on routing parcels by AGAS-invoked tasks on locality 0 on
194 nodes. 

Using our methods we were able to identify the issues that pose possible
performance bottlenecks that will be investigated and reported in future work. Inclusion of a copy
of the locality namespace to each locality's AGAS cache in the current version
of HPX is one such optimization that reduces traffic forwarding to locality 0
due to not knowing the endpoint address for a locality.

The most important feature of AGAS is that it allows applications to
dynamically perform data load balancing at runtime without stopping the
execution and it also works with other components of HPX to deliver a
distributed asynchronous multithreaded system. We observe that in total AGAS
operations cause an average of 0.0036\% of overhead. This is hardly a
performance issue and indicates that using AGAS is a relatively inexpensive
choice for the boost in productivity it provides.

