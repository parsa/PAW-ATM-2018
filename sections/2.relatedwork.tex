\section{Related Work}
\label{related_work}

%\todo{You need a section on background of global address spaces, why they are needed, how do they work? Not everyone knows this.}
The MPI programming model provides the user with complete control over data
locality and performance. On the other hand, it does not have the
programmability and data referencing simplicity of shared-memory systems. The
global address space model combines the two models and enables processes to
access shared memory locations with a global address while maintaining an
explicit distinction between local and remote operations. This provides the
means to implement distributed versions of commonly used data structures like
arrays, sets, matrices, or any data structure that is based on pointers.

Some global address space implementations like Unified Parallel C (UPC)\cite{upc_org}, Co-Array
Fortran, Titanium\cite{yelick1998titanium}, and SHMEM\cite{shmem_feind1995,openshmem_chapman2010}, the global addresses are resolved to communication calls
during compilation. Because global address resolution is performed at compile
time, these implementations still do not provide the freedom programmers have
in a shared memory application. Additionally, they still use global barriers as
the synchronization mechanism, which does not trivially achieve shared memory
applications performance.

Asynchronous PGAS implementations (e.g. Charm++, UPC++, Chapel, and X10) follow
the asynchronous many task model to dynamically perform load balancing at
runtime. They allow multiple tasks to run within each operating system thread
and provide tools for controlling the memory layout and expressing
multidimensional, sparse, associative, or unstructured data structures.

In recent years, there has been a significant increase in utilization of
heterogeneous clusters that use GPUs and MICs in addition to
CPUs\cite{Lena2014,Yang2011266,Potluri2014,Sidelnik2011}. One approach to
manage such systems is using
solutions\cite{Rabenseifner2009,Yang2011266,Chorley2010} that separately use a
library like MPI for explicit communication between nodes and a choice of
shared-memory programming framework such as OpenMP\cite{openmp_org}, Kokkos\cite{kokkos_paper,kokkos_repo}, or
UPC. Other approaches include Asynchronous PGAS
runtimes\cite{Saraswat2010} and Charm++ that use dynamic
multithreading to avoid fragmenting the application development process to
separately manage communication and computation while maintaining portability
between various cluster configurations and providing access to heterogeneous
computing resources\cite{P0234R0}.

Most studies on distributed runtime systems do not include quantitative
analysis of the performance of their global address space system but present
the overall performance of applications using the respective model or
implementation. However, amongst the runtime systems mentioned only HPX has a
global address system that allows objects to be relocated at runtime. This
unique property calls for a closer look into HPX's Active Global Address Space
system behavior and performance and is the motivation behind this study.
