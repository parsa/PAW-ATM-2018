@inproceedings{Anderson2013,
abstract = {The addition of nuclear and neutrino physics to general rel-ativistic fluid codes allows for a more realistic description of hot nuclear matter in neutron star and black hole systems. This additional microphysics requires that each processor have access to large tables of data, such as equations of state, and in large simulations, the memory required to store these tables locally can become excessive unless an alternative ex-ecution model is used. In this work we present relativistic fluid evolutions of a neutron star obtained using a message driven multi-threaded execution model known as ParalleX. The goal of this work is to reduce the negative performance impact of distributing the tables. We introduce a component based on the notion of a " future " , or nonblocking encapsu-lated delayed computation, for accessing large tables of data, including out-of-core sized tables. The proposed technique does not impose substantial memory overhead and can hide increased network latency.},
address = {Boston, Massachusetts USA},
author = {Anderson, Matthew and Brodowicz, Maciej and Kaiser, Hartmut and Adelstein-Lelbach, Bryce and Sterling, Thomas and Kaiser, Hartmut and Adelstein-Lelbach, Bryce},
booktitle = {Parallel and Distributed Processing Symposium Workshops {\&} PhD Forum (IPDPSW), 2013 IEEE 27th International},
doi = {10.1109/IPDPSW.2013.162},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Anderson et al. - Unknown - Tabulated equations of state with a many-tasking execution model.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Anderson et al. - 2013 - Tabulated Equations of State with a Many-Tasking Execution Model.pdf:pdf},
keywords = {Astrophysics applications,Futures,HPX,ParalleX},
mendeley-tags = {Astrophysics applications,Futures,HPX,ParalleX},
pages = {1691--1699},
publisher = {IEEE Computer Society},
title = {{Tabulated equations of state with a many-tasking execution model}},
url = {http://stellar.cct.lsu.edu/pubs/tabulated{\_}eos.pdf http://dx.doi.org/10.1109/IPDPSW.2013.162},
year = {2013}
}
@article{Heller2013,
abstract = {In the prospect of the upcoming exa-scale era with millions of execution units, the question of how to deal with this level of parallelism efficiently is of time-critical relevance. State-of-the-Art parallelization techniques such as OpenMP and MPI are not guaran-teed to solve the expected problems of starvation, grow-ing latencies, overheads, and contention. On the other hand, new parallelization paradigms promise to effi-ciently hide latencies and contain starvation and con-tention. In this paper we analyze the performance of one novel parallelization strategy for shared and distributed memory machines. We will focus on shared memory ar-chitectures and compare the performance of the Par-alleX execution model against the quasi-standard Open-MP for a standard stencil-based problem. We compare in detail the OpenMP implementation of two applica-tions of Jacobi solvers (one based on regular grid and one based on an irregular grid structure) with the cor-responding implementation of these applications using HPX (High Performance ParalleX), the first feature-complete, open-source implementation of ParalleX, and analyze the results of both implementations on a multi-socket NUMA node.},
author = {Heller, T and Kaiser, H and Iglberger, Klaus},
doi = {10.1007/s00450-012-0217-1},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Heller, Kaiser, Iglberger - Unknown - Application of the ParalleX Execution Model to Stencil-based Problems.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Heller, Kaiser, Iglberger - 2013 - Application of the ParalleX Execution Model to Stencil-Based Problems.pdf:pdf},
journal = {Computer Science-Research and Development},
keywords = {Algorithms,Dataflow,Execution model,High,High Performance ParalleX (HPX),Iterative,Iterative solvers,OpenMP,ParalleX,ParalleX (HPX),Parallel,Parallel algorithms,Parallel runtime systems,Performance,Runtime,Solvers,Systems},
mendeley-tags = {Dataflow,Execution model,High Performance ParalleX (HPX),Iterative solvers,OpenMP,ParalleX,Parallel algorithms,Parallel runtime systems},
number = {2-3},
pages = {253--261},
publisher = {Springer},
title = {{Application of the ParalleX execution model to stencil-based problems}},
url = {http://www.cct.lsu.edu/{~}hkaiser http://www.zisc.uni-erlangen.de http://dx.doi.org/10.1007/s00450-012-0217-1},
volume = {28},
year = {2013}
}
@inproceedings{Barton2006,
abstract = {This paper describes the design and implementation of a scalable run-time system and an optimizing compiler for Unified Parallel C (UPC). An experimental evaluation on BlueGene/L{\textregistered}, a distributed-memory machine, demonstrates that the combination of the compiler with the runtime system produces programs with performance comparable to that of efficient MPI programs and good performance scalability up to hundreds of thousands of processors.Our runtime system design solves the problem of maintaining shared object consistency efficiently in a distributed memory machine. Our compiler infrastructure simplifies the code generated for parallel loops in UPC through the elimination of affinity tests, eliminates several levels of indirection for accesses to segments of shared arrays that the compiler can prove to be local, and implements remote update operations through a lower-cost asynchronous message. The performance evaluation uses three well-known benchmarks --- HPC RandomAccess, HPC STREAM and NAS CG --- to obtain scaling and absolute performance numbers for these benchmarks on up to 131072 processors, the full BlueGene/L machine. These results were used to win the HPC Challenge Competition at SC05 in Seattle WA, demonstrating that PGAS languages support both productivity and performance. Academic paper: Shared memory programming for large scale machines. Available from: https://www.researchgate.net/publication/220752151{\_}Shared{\_}memory{\_}programming{\_}for{\_}large{\_}scale{\_}machines [accessed Apr 27, 2017].},
address = {Ottawa, Ontario, Canada},
author = {Barton, Christopher and Cas{\c{c}}aval, C{\'{C}}lin and Alm{\'{a}}si, George and Zheng, Yili and Farreras, Montse and Chatterje, Siddhartha and Amaral, Jos{\'{e}} Nelson},
booktitle = {Proceedings of the 27th ACM SIGPLAN Conference on Programming Language Design and Implementation},
doi = {10.1145/1133981.1133995},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Barton et al. - 2006 - Shared Memory Programming for Large Scale Machines.pdf:pdf},
isbn = {1-59593-320-4},
keywords = {BlueGene,PGAS programming model,UPC},
number = {6},
pages = {108--117},
publisher = {ACM},
title = {{Shared Memory Programming for Large Scale Machines}},
url = {http://www.research.ibm.com/people/c/cascaval/pldi06.pdf http://citeseerx.ist.psu.edu.libezp.lib.lsu.edu/viewdoc/download?doi=10.1.1.923.8462{\&}rep=rep1{\&}type=pdf http://doi.acm.org/10.1145/1133981.1133995},
volume = {41},
year = {2006}
}
@article{Tabbal2011,
abstract = {Exascale systems, expected to emerge by the end of the next decade, will require the exploitation of billion-way par-allelism at multiple hierarchical levels in order to achieve the desired sustained performance. While traditional ap-proaches to performance evaluation involve measurements of existing applications on the available platforms, such a methodology is obviously unsuitable for architectures still at the brainstorming stage. The prediction of the future ma-chine performance is an important factor driving the design of both the execution hardware and software environment. A good way to start assessing the performance is to iden-tify the factors challenging the scalability of parallel appli-cations. We believe the root cause of these challenges is the incoherent coupling between the current enabling technolo-gies, such as Non-Uniform Memory Access of present mul-ticore nodes equipped with optional hardware accelerators and the decades older execution model, i.e., Communicating Sequential Processes (CSP). Supercomputing is in the midst of a much needed phase change and the High-Performance Computing community is slowly realizing the necessity for a new design dogma, as affirmed in the preliminary Exas-cale studies. In this paper, we present an overview of the ParalleX execution model and its complementary design ef-forts at the software and hardware levels, while including power draw of the system as the resource of utmost impor-tance. Since the interplay of hardware and software envi-ronment is quickly becoming one of the dominant factors in the design of well integrated, energy efficient, large-scale systems, we also explore the implications of the ParalleX model on the organization of parallel computing architec-tures. We also present scaling and performance results for an adaptive mesh refinement application developed using a ParalleX-compliant runtime system implementation, HPX.},
author = {Tabbal, Alexandre and Anderson, Matthew and Brodowicz, Maciej and Kaiser, Hartmut and Sterling, Thomas},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Tabbal et al. - Unknown - Preliminary Design Examination of the ParalleX System from a Software and Hardware Perspective.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Tabbal et al. - 2011 - Preliminary Design Examination of the ParalleX System from a Software and Hardware Perspective.pdf:pdf},
journal = {ACM SIGMETRICS Performance Evaluation Review},
keywords = {Execution Model,Model of Computation,ParalleX},
number = {4},
pages = {81--87},
publisher = {ACM},
title = {{Preliminary Design Examination of the ParalleX System from a Software and Hardware Perspective}},
url = {http://stellar.cct.lsu.edu/pubs/pmbs10.pdf},
volume = {38},
year = {2011}
}
@phdthesis{Habraken2013,
abstract = {The first exa-scale computers are predicted to arrive in 2018 but we are currently unable to fully utilise such massive parallelism. ParalleX is an execution model which attempts to solve this through hiding system-wide latencies, decoupling hardware execution resources from executing software tasks, and enabling runtime dynamic adaptive scheduling of these tasks. In order for this model to be evaluated High Performance ParalleX (HPX) has been developed, the first feature-complete, open-source implementation of ParalleX. In this thesis we design a capability-based protection system for HPX through the collection of requirements from scenarios and subsequently using these to select the primitive building blocks to secure the system. This allows a machine with read capabilities to monitor a simulation without influencing it for example. To prove the design its viable a partial implementation of the design is done within HPX. Afterwards the cost of the protection layer is evaluated by comparing unprotected and protected runs as increased latencies are expected due to the cryptographic overhead.},
author = {Habraken, Jeroen A. E.},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Habraken - 2013 - Adding capability-based security to High Performance ParalleX.pdf:pdf},
pages = {67},
school = {Technische Universiteit Eindhoven},
title = {{Adding capability-based security to High Performance ParalleX}},
url = {http://alexandria.tue.nl/extra1/afstversl/wsk-i/habraken2013.pdf https://pure.tue.nl/ws/files/46951143/761619-1.pdf http://repository.tue.nl/e2136860-e00f-4e56-8f58-8a28c8e07ca4{\%}0A},
year = {2013}
}
@phdthesis{Amatya2014,
abstract = {Advancements in cutting edge technologies have enabled better energy efficiency as
well as scaling computational power for the latest High Performance Computing (HPC)
systems. However, complexities due to hybrid architectures as well as emerging classes
of applications, have shown poor computational scalability using conventional execution
models. Thus, alternative means of computation, that addresses the bottlenecks in computation,
is warranted. More precisely, dynamic adaptive resource management feature, both
from systems' as well as applications' perspective, is essential for better computational
scalability and efficiency. This dissertation presents and expands the notion of Parallel
Processes as a placeholder for procedure definitions, targeted at one or more synchronous
domains, meta data for computation and resource management as well as infrastructure
for dynamic policy deployment. Additionally, it also presents use cases of Parallel Processes
for system resource management, task management, locality management, for load
balancing, access control, namespace management and performance measurement frameworks
in HPX runtime system. Further, this work lists design principles for scalability of
Active Global Address Space (AGAS), a necessary feature for Parallel Processes. Also,
to verify the usefulness of Parallel Processes, a preliminary performance evaluation of different
task scheduling policies has been carried out using two different applications. The
first application is Unbalanced Tree Search, a dynamic graph application based on a reference
implementation and implemented by this research in HPX. The other application is
MiniGhost, a stencil based application based on reference implementation, using bulk synchronous
parallel model. The results show that local and local-priority scheduling policies
in HPX provide better performance for different classes of applications. However under
certain application parameters, hierarchy scheduling policy performs better than both the
schedulers in UTS; meanwhile giving overall poor performance in Minighost. Similarly,
static scheduling policy show no performance gain in UTS, but shows scalable performance
for Minighost. Further, choice of task granularity for a given problem size also plays a role on the performance of applications, as witnessed in UTS. These observations support
the hypothesis of the need of a dynamic adaptive resource management infrastructure, for
deploying different policies and task granularities, for scalable distributed computing.},
author = {Amatya, Vinay C},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Amatya - 2014 - Parallel Processes in HPX Designing An Infrastructure for Adaptive Resource Management.pdf:pdf},
pages = {118},
school = {Louisiana State University},
title = {{Parallel Processes in HPX: Designing An Infrastructure for Adaptive Resource Management}},
url = {http://etd.lsu.edu/docs/available/etd-11172014-122205/unrestricted/amatya{\_}dissertation{\_}1.pdf},
year = {2014}
}
@phdthesis{Yang2014,
abstract = {The continuing technological progress resulted in a dramatic growth in aggregate computational
performance of the largest supercomputing systems. Unfortunately, these advances
did not translate to the required extent into accompanying I/O systems and little
more in terms of architecture or effective access latency. New classes of algorithms developed
for massively parallel applications, that gracefully handle the challenges of asynchrony,
heavily multi-threaded distributed codes, and message-driven computation, must
be matched by similar advances in I/O methods and algorithms to produce a well performing
and balanced supercomputing system. This dissertation proposes PXFS, a storage
model for persistent objects inspired by the ParalleX model of execution that addresses
many of these challenges. The PXFS model is designed to be asynchronous in nature to
comply with ParalleX model and proposes an active TupleSpace concept to hold all kinds
of metadata/meta-object for either storage objects or runtime objects. The new active
TupleSpace can also register ParalleX actions to be triggered under certain tuple operations.
An first implementation of PXFS utilizing a well-known Orange parallel file system
as its back-end via asynchronous I/O layer and the implementation of TupleSpace component
in HPX, the implementation of ParalleX. These details are also described along
with the preliminary performance data. A house-made micro benchmark is developed to
measure the disk I/O throughput of the PXFS asynchronous interface. The results show
perfect scalability and 3x to 20x times speedup of I/O throughput performance comparing
to OrangeFS synchronous user interface. Use cases of TupleSpace components are discussed
for real-world applications including micro check-pointing. By utilizing TupleSpace
in HPX applications for I/O, global barrier can be replaced with fine-grained parallelism
to overlap more computation with communication and greatly boost the performance and
efficiency. Also the dissertation showcases the distributed directory service in Orange file
system which process directory entries in parallel and effectively improves the directory
metada operations.},
author = {Yang, Shuangyang},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Yang - 2014 - A Persistent Storage Model for Extreme Computing.pdf:pdf},
pages = {126},
school = {Louisiana State University},
title = {{A Persistent Storage Model for Extreme Computing}},
url = {http://etd.lsu.edu/docs/available/etd-10312014-133428/unrestricted/syang{\_}dissertation.6.pdf},
year = {2014}
}
@phdthesis{Heller2012,
abstract = {In the prospect of the upcoming exascale era with millions of execution units, the question of how to deal with this level of parallelism efficiently is of time-critical relevance. State- of-the-Art parallelization techniques such as OpenMP and MPI are not guaranteed to solve the expected problems of starvation, growing latencies, and contention. On the other hand, new parallelization paradigms promise to efficiently hide latencies and contain starvation and contention. In this thesis an insight into ParalleX, a new, experimental parallel execution model, is given. ParalleX defines semantics and modes of operations for parallel applications ranging from large heterogeneous cluster to shared memory systems. Despite the discussion of mechanisms for distributed as well as shared memory systems, this thesis will be restricted to implementations and performance measurements on shared memory machines. The focus of this thesis lies in the implementation of a dynamic Data Flow processor within the ParalleX execution model, which presents a novel parallelization strategy. The viability of this approach is evaluated for a standard stencil-based problem, the Jacobi Method. Two applications of this algorithm will be discussed, one based on a regular and another based on an irregular grid. In order to evaluate the developed mechanisms, implementations with HPX (High Performance ParalleX), the first feature-complete, open-source implementation of ParalleX, and OpenMP, the quasi-standard for programming paradigm for shared memory machines, are compared. The performance of these applications will be evaluated on a multi-socket NUMA node.},
author = {Heller, Thomas},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Heller - 2012 - Implementation of Data Flow for the ParalleX Execution Model.pdf:pdf},
pages = {63},
school = {Friedrich-Alexander-Universit{\"{a}}t Erlangen-N{\"{u}}rnberg},
title = {{Implementation of Data Flow for the ParalleX Execution Model}},
url = {http://stellar.cct.lsu.edu/pubs/Heller{\_}MA12.pdf},
year = {2012}
}
@phdthesis{Raj2014,
abstract = {With High Performance Computing moving towards Exascale, where parallel applications will be required to run concurrently on millions of cores, every part of the computational model must perform ideally to achieve optimal performance. The task scheduler is one of such entities that could be enhanced to runtime application prerequisites. Not only the overheads associated with task scheduling vary depending on the task size but also are affected by the different schedulers used. One of the best strategies used to ameliorate performance and to decrease the overheads would be to adapt the scheduler to the application's needs or to vary the task size or a combination of both. In this work, we show how scheduling overheads vary with the different types of schedulers used in the High Performance ParalleX (HPX) runtime system. In addition to this, we show how the different schedulers perform with varying task size. This performance analysis focuses on understanding how the different schedulers perform for a specific HPX benchmark in turn helping us to settle on a scheduler that could give optimal performance. HPX is an ideal experimental platform as it makes use of asynchronous fine-grained task scheduling and incorporates a dynamic performance modeling capability. In addition to this, HPX offers performance counter capabilities with which we can characterize scheduling overheads.},
author = {Raj, Rekha},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Raj - 2014 - Performance Analysis with HPX.pdf:pdf},
pages = {36},
school = {Louisiana State University},
title = {{Performance Analysis with HPX}},
url = {http://stellar.cct.lsu.edu/wp-content/uploads/2015/02/Rekha{\_}project-report.pdf},
year = {2014}
}
@article{Loffler2016,
abstract = {We present a new method for parallelization of adaptive mesh refinement called Concurrent Structured Adaptive Mesh Refinement (CSAMR). This new method offers the lower computational cost (i.e. wall time × processor count) of subcycling in time, but with the runtime performance (i.e. smaller wall time) of evolving all levels at once using the time step of the finest level (which does more work than subcycling but has less parallelism). We demonstrate our algorithm's effectiveness using an adaptive mesh refine- ment code, AMSS-NCKU, and show performance on Blue Waters and other high performance clusters. For the class of problem considered in this paper, our algorithm achieves a speedup of 1.7–1.9 when the processor count for a given AMR run is doubled, consistent with our theoretical predictions. {\textcopyright}},
author = {L{\"{o}}ffler, Frank and Cao, Zhoujian and Brandt, Steven R. and Du, Zhihui},
doi = {10.1016/j.jocs.2016.05.003},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/L{\"{o}}ffler et al. - 2016 - A new parallelization scheme for adaptive mesh refinement.pdf:pdf},
issn = {18777503},
journal = {Journal of Computational Science},
keywords = {Adaptive mesh refinement,Parallel algorithms,Parallel application frameworks,Parallel applications},
pages = {79--88},
title = {{A new parallelization scheme for adaptive mesh refinement}},
url = {http://www.sciencedirect.com/science/article/pii/S1877750316300795 http://ac.els-cdn.com/S1877750316300795/1-s2.0-S1877750316300795-main.pdf?{\_}tid=3fa58066-2b8e-11e7-968e-00000aacb361{\&}acdnat=1493327741{\_}880f03df5844d0f1c058ef06bb21b8fc http://ac.els-cdn.com},
volume = {16},
year = {2016}
}
@phdthesis{Stark2011,
abstract = {Large-scale graph applications are of great national, commercial, and societal importance, with direct use in fields such as counter-intelligence, proteomics, and data mining. Unfortunately, graph-based problems exhibit certain basic characteristics that make them a poor match for conventional computing systems in terms of structure, scale, and semantics. Graph processing kernels emphasize sparse data structures and computations with irregular memory access patterns that destroy the temporal and spatial locality upon which modern processors rely for performance. Furthermore, applications in this area utilize large data sets, and have been shown to be more data intensive than typical floating-point applications, two properties which lead to inefficient utilization of the hierarchical memory system. Current approaches to processing large graph data sets leverage traditional HPC systems and programming models, for shared memory and message-passing computation, and are thus limited in efficiency, scalability, and programmability. The research presented in this thesis investigates the potential of a new model of execution that is hypothesized as a promising alternative for graph-based applications to conventional practices. A new approach to graph processing is developed and presented in this thesis. The application of the experimental ParalleX execution model to graph processing balances continuation-migration style fine-grain concurrency with constraint-based synchronization. A collection of parallel graph application kernels provide experiment control drivers for analysis and evaluation of this innovative strategy. Finally, an experimental software library for scalable graph processing, the ParalleX Graph Library, is defined using the HPX runtime system, providing an implementation of the key concepts and a framework for development of ParalleX-based graph applications.},
author = {Stark, Dylan},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Stark - 2007 - Advanced Semantics for Accelerated Graph Processing.pdf:pdf},
pages = {132},
school = {Louisiana State University},
title = {{Advanced Semantics for Accelerated Graph Processing}},
url = {http://etd.lsu.edu/docs/available/etd-04192011-164932/unrestricted/dissertation.pdf},
year = {2011}
}
@inproceedings{Heller2013a,
abstract = {With the general availability of PetaFLOP clusters and the advent of heterogeneous machines equipped with special accelerator cards such as the Xeon Phi[2], computer scientist face the difficult task of improving application scalability beyond what is possible with conventional techniques and programming models today. In addi-tion, the need for highly adaptive runtime algorithms and for appli-cations handling highly inhomogeneous data further impedes our ability to efficiently write code which performs and scales well. In this paper we present the advantages of using HPX[19, 3, 29], a general purpose parallel runtime system for applications of any scale as a backend for LibGeoDecomp[25] for implementing a three-dimensional N-Body simulation with local interactions. We compare scaling and performance results for this application while using the HPX and MPI backends for LibGeoDecomp. LibGeoDe-comp is a Library for Geometric Decomposition codes implement-ing the idea of a user supplied simulation model, where the library handles the spatial and temporal loops, and the data storage. The presented results are acquired from various homogeneous and heterogeneous runs including up to 1024 nodes (16384 conven-tional cores) combined with up to 16 Xeon Phi accelerators (3856 hardware threads) on TACC's Stampede supercomputer[1]. In the configuration using the HPX backend, more than 0.35 PFLOPS have been achieved, which corresponds to a parallel application efficiency of around 79{\%}. Our measurements demonstrate the ad-vantage of using the intrinsically asynchronous and message driven programming model exposed by HPX which enables better latency hiding, fine to medium grain parallelism, and constraint based syn-chronization. HPX's uniform programming model simplifies writ-ing highly parallel code for heterogeneous resources.},
address = {Denver, Colorado},
author = {Heller, Thomas and Kaiser, Hartmut and Sch{\"{a}}fer, Andreas and Fey, Dietmar},
booktitle = {Proceedings of the Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems},
doi = {10.1145/2530268.2530269},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Heller et al. - Unknown - Using HPX and LibGeoDecomp for Scaling HPC Applications on Heterogeneous Supercomputers.pdf:pdf},
keywords = {Application Frameworks,High Performance Computing,Parallel Runtime Systems},
pages = {1},
publisher = {ACM},
title = {{Using HPX and LibGeoDecomp for Scaling HPC Applications on Heterogeneous Supercomputers}},
url = {http://dx.doi.org/10.1145/2530268.2530269.},
year = {2013}
}
@phdthesis{Ghimire2014,
abstract = {High Performance Computation (HPC) requires a proper and efficient scheme for distribution of the computational workload across different computational nodes. The HPX (High Performance ParalleX) runtime system currently lacks a module that automates data distribution process so that the programmer does not have to manually perform data distribution. Further, there is no mechanism allowing to perform load balancing of computations. This thesis addresses that issue by designing and developing a user friendly programming interface conforming to the C++11/14 Standards and integrated with HPX which enables to specify various distribution parameters for a distributed vector. We present the three different distribution policies implemented so far: block, cyclic, and block-cyclic. These policies influence the way the distributed vector maps any global (linear) index into the vector onto a pair of values describing the number of the (possibly remote data partition) and the corresponding local index. We present performance analysis results from applying the different distribution policies to calculating the Mandelbrot set; an example of an ‘embarrassingly parallel' computation. For this benchmark we use an instance of a distributed vector where each element holds a tuple for the current index and the value of related to an individual pixel of the generated Mandelbrot plot. We compare the influence of different distribution policies and their corresponding parameters on the overall execution time of the calculation. We demonstrate that the block-cyclic distribution policy yields best results for calculating the Mandelbrot set as it more evenly load balances the computation across the computational nodes. The provided API and implementation gives the user a high level an abstraction for developing applications while hiding low-level data distribution details.},
author = {Ghimire, Bibek},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Ghimire - 2014 - Data Distribution In HPX.pdf:pdf},
pages = {41},
school = {Louisiana State University},
title = {{Data Distribution in HPX}},
url = {http://stellar.cct.lsu.edu/wp-content/uploads/2015/02/ghimirethesis.pdf},
year = {2014}
}
@inproceedings{Grubel2015,
abstract = {As High Performance Computing moves toward Exascale, where parallel applications will be expected to run on millions of cores concurrently, every component of the computational model must perform optimally. One such component, the task scheduler, can potentially be optimized to runtime application requirements. We focus our study using a task-based runtime system, one possible solution towards Exascale computation. Based on task size and scheduler, the overheads associated with task scheduling vary. Therefore, to minimize overheads and optimize performance, either the task size or the scheduler must adapt. In this paper, we focus on adapting the task size, which can be easily done statically and potentially done dynamically. To this end, we first show how scheduling overheads change with task size or granularity. We then propose and execute a methodology to characterize these overheads and dynamically measure the effects of task granularity. The HPX runtime system [1] employs asynchronous fine-grained task scheduling and incorporates a dynamic performance modeling capability, providing an ideal experimental platform. Using the performance counter capabilities in HPX, we characterize task scheduling overheads and show metrics to determine optimal task size. This is the first step toward the goal of dynamically adapting task size to optimize parallel performance.},
address = {Chicago, IL, USA},
author = {Grubel, Patricia and Kaiser, Hartmut and Cook, Jeanine and Serio, Adrian},
booktitle = {Workshop on Monitoring and Analysis for High Performance Computing Systems Plus Applications (HPCMASPA 2015), IEEE Cluster 2015},
doi = {10.1109/CLUSTER.2015.119},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Grubel et al. - Unknown - The Performance Implication of Task Size for Applications on the HPX Runtime System.pdf:pdf},
keywords = {HPX,ParalleX,Task Parallelism,Task Scheduling,—Task Granularity},
month = {sep},
organization = {New Mexico State University, Louisiana State University, Sandia National Labs, STE||AR Group},
pages = {682--689},
publisher = {IEEE Computer Society},
title = {{The Performance Implication of Task Size for Applications on the HPX Runtime System}},
url = {http://stellar.cct.lsu.edu/pubs/hpcmaspa2015.pdf http://dx.doi.org/10.1109/CLUSTER.2015.119},
year = {2015}
}
@article{Kadam2016,
abstract = {We demonstrate that rapidly rotating bipolytropic (composite polytropic) stars and toroidal discs can be obtained using Hachisu's self-consistent field technique. The core and the envelope in such a structure can have different polytropic indices and also different average molecular weights. The models converge for high T/|W| cases, where T is the kinetic energy and W is the gravitational energy of the system. The agreement between our numerical solutions with known analytical as well as previously calculated numerical results is excellent. We show that the uniform rotation lowers the maximum core mass fraction or the Sch{\"{o}}nberg–Chandrasekhar limit for a bipolytropic sequence. We also discuss the applications of this method to magnetic braking in low-mass stars with convective envelopes.},
author = {Kadam, Kundan and Motl, Patrick M. and Frank, Juhan and Clayton, Geoffrey C. and Marcello, Dominic C. and H., Schwarz},
doi = {10.1093/mnras/stw1814},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Kadam et al. - 2016 - A numerical method for generating rapidly rotating bipolytropic structures in equilibrium.pdf:pdf},
issn = {0035-8711},
journal = {MNRAS},
keywords = {equation of state – methods,numerical – stars,rotation},
month = {oct},
number = {2},
pages = {2237--2245},
publisher = {Oxford University Press},
title = {{A numerical method for generating rapidly rotating bipolytropic structures in equilibrium}},
url = {https://academic.oup.com/mnras/article-lookup/doi/10.1093/mnras/stw1814 https://oup.silverchair-cdn.com/oup/backfile/Content{\_}public/Journal/mnras/462/2/10.1093{\_}mnras{\_}stw1814/2/stw1814.pdf?Expires=1493419475{\&}Signature=INq4ORjWtWvZE{~}vAUKYncLgNE7EBWyPpn0s0xD},
volume = {462},
year = {2016}
}
@article{Anderson2011,
abstract = {Exascale systems, expected to emerge by the end of the next decade, will require the exploitation of billion-way parallelism at multiple hierarchical levels in order to achieve the desired sustained performance. The task of assessing future machine performance is approached by identifying the factors which currently challenge the scalability of parallel applications. It is suggested that the root cause of these challenges is the incoherent coupling between the current enabling technologies, such as Non-Uniform Memory Access of present multicore nodes equipped with optional hardware accelerators and the decades older execution model, i.e., the Communicating Sequential Processes (CSP) model best exemplified by the message passing interface (MPI) application programming interface. A new execution model, ParalleX, is introduced as an alternative to the CSP model. In this paper, an overview of the ParalleX execution model is presented along with details about a ParalleX-compliant runtime system implementation called High Performance ParalleX (HPX). Scaling and performance results for an adaptive mesh refinement numerical relativity application developed using HPX are discussed. The performance results of this HPX-based application are compared with a counterpart MPI-based mesh refinement code. The overheads associated with HPX are explored and hardware solutions are introduced for accelerating the runtime system.},
archivePrefix = {arXiv},
arxivId = {1109.5201},
author = {Anderson, Matthew and Brodowicz, Maciej and Kaiser, Hartmut and Sterling, Thomas},
eprint = {1109.5201},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Anderson et al. - Unknown - An Application Driven Analysis of the ParalleX Execution Model.5201v1:5201v1},
journal = {CoRR},
title = {{An Application Driven Analysis of the ParalleX Execution Model}},
url = {https://arxiv.org/pdf/1109.5201.pdf https://arxiv.org/pdf/1109.5201v1.pdf},
volume = {abs/1109.5},
year = {2011}
}
@inproceedings{Kaiser2014,
abstract = {The significant increase in complexity of Exascale platforms due to energy-constrained, billion-way parallelism, with major changes to processor and memory architecture, requires new energy-efficient and resilient programming techniques that are portable across mul-tiple future generations of machines. We believe that guarantee-ing adequate scalability, programmability, performance portability, resilience, and energy efficiency requires a fundamentally new ap-proach, combined with a transition path for existing scientific ap-plications, to fully explore the rewards of todays and tomorrows systems. We present HPX – a parallel runtime system which ex-tends the C++11/14 standard to facilitate distributed operations, enable fine-grained constraint based parallelism, and support run-time adaptive resource management. This provides a widely ac-cepted API enabling programmability, composability and perfor-mance portability of user applications. By employing a global ad-dress space, we seamlessly augment the standard to apply to a dis-tributed case. We present HPX's architecture, design decisions, and results selected from a diverse set of application runs showing superior performance, scalability, and efficiency over conventional practice.},
address = {Eugene, OR, USA},
author = {Kaiser, Hartmut and Heller, Thomas and Adelstein-Lelbach, Bryce and Serio, Adrian and Fey, Dietmar},
booktitle = {Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models},
doi = {10.1145/2676870.2676883},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Kaiser et al. - Unknown - HPX – A Task Based Programming Model in a Global Address Space.pdf:pdf},
keywords = {Exas-cale,Exascale,Global Address Space,High Performance Computing,Parallel Runtime Systems,Programming Models},
mendeley-tags = {Exascale,Global Address Space,High Performance Computing,Parallel Runtime Systems,Programming Models},
pages = {6},
publisher = {ACM},
title = {{HPX - A Task Based Programming Model in a Global Address Space}},
url = {http://stellar.cct.lsu.edu/pubs/pgas14.pdf http://doi.acm.org/10.1145/2676870.2676883},
year = {2014}
}
@inproceedings{Palumbo1992,
abstract = {The traditional computer-based message systems, messages are typically viewed as passive entities with no processing power. The users are thus responsible for explicitly creating, routing and processing each message instance. Messages, however, may be viewed as active objects which can do certain processing on their own. This paper explores the concept of “messages as objects” for office automation purposes.},
address = {Kansas City, Missouri, USA},
author = {Palumbo, Robert and Saiedian, Hossein and Zand, Mansour},
booktitle = {Proceedings of the 1992 ACM annual conference on Communications},
doi = {10.1145/131214.131261},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Palumbo, Saiedian, Zand - 1992 - The operational semantics of an active message system.pdf:pdf},
isbn = {0-89791-472-4},
organization = {University of Nebraska},
pages = {367--375},
publisher = {ACM},
series = {CSC '92},
title = {{The operational semantics of an active message system}},
url = {http://libezp.nmsu.edu:4009/10.1145/131214.131261 http://doi.acm.org/10.1145/131214.131261},
year = {1992}
}
@article{Dekate2012,
abstract = {The scalability and efficiency of graph applications are significantly constrained by conventional systems and their supporting programming models. Technology trends such as multicore, manycore, and heterogeneous system architectures are introducing further challenges and possibilities for emerging application domains such as graph applications. This paper explores the parallel execution of graphs that are generated using the Barnes-Hut algorithm to exemplify dynamic workloads. The workloads are expressed using the semantics of an exascale computing execution model called ParalleX. For comparison, results using conventional execution model semantics are also presented. We find improved load balancing during runtime and automatic parallelism discovery by using the advanced semantics for exascale computing.},
archivePrefix = {arXiv},
arxivId = {1109.5190},
author = {Dekate, Chirag and Anderson, Matthew and Brodowicz, Maciej and Kaiser, Hartmut and Adelstein-Lelbach, Bryce and Sterling, Thomas L},
doi = {10.1177/1094342012440585},
eprint = {1109.5190},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Dekate et al. - 2012 - Improving the Scalability of Parallel N-Body Applications with an Event-Driven Constraint-Based Execution Model.pdf:pdf},
journal = {Int. J. High Perform. Comput. Appl.},
keywords = {Barnes-Hut,N-body,parallelization,parallex},
number = {August 2012},
pages = {319--332},
title = {{Improving the Scalability of Parallel N-Body Applications with an Event-Driven Constraint-Based Execution Model}},
url = {http://dx.doi.org/10.1177/1094342012440585 http://dl.acm.org/citation.cfm?id=2331126},
volume = {26},
year = {2012}
}
@article{Tseng1995,
abstract = {This paper presents novel compiler optimizations for reducing synchronization overhead in compiler-parallelized scientific codes. A hybrid programming model is employed to combine the flexibility of the fork-join model with the precision and power of the single-program, multiple data (SPMD) model. By exploiting compile-time computation partitions, communication analysis can eliminate barrier synchronization or replace it with less expensive forms of synchronization. We show computation partitions and data communication can be represented as systems of symbolic linear inequalities for high flexibility and precision. These optimizations has been implemented in the Stanford SUIF compiler. We extensively evaluate their performance using standard benchmark suites. Experimental results show barrier synchronization is reduced 29{\%} on average and by several orders of magnitude for certain programs.},
author = {Tseng, Chau-Wen},
doi = {10.1145/209937.209952},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Tseng - 1995 - Compiler Optimizations for Eliminating Barrier Synchronization.pdf:pdf},
journal = {ACM SIGPLAN Notices},
number = {8},
pages = {144--155},
publisher = {ACM},
title = {{Compiler Optimizations for Eliminating Barrier Synchronization}},
url = {http://doi.acm.org/10.1145/209937.209952 http://dl.acm.org/citation.cfm?id=209952},
volume = {30},
year = {1995}
}
@inproceedings{Barik2013,
abstract = {Abstract: In this paper, we introduce novel compiler optimization techniques to reduce the number of operations performed in critical sections that occur in explicitly-parallel programs. Specifically, we focus on three code transformations: 1) Partial Strength Reduction (PSR) of critical sections to replace critical sections by non-critical sections on certain control flow paths; 2) Critical Load Elimination (CLE) to replace memory accesses within a critical section by accesses to scalar temporaries that contain values loaded outside the critical section; and 3) Non-critical Code Motion (NCM) to hoist thread-local computations out of critical sections. The effectiveness of the first two transformations is further increased by interprocedural analysis. The effectiveness of our techniques has been demonstrated for critical section constructs from three different explicitly-parallel programming models - the isolated construct in Habanero Java (HJ), the synchronized construct in standard Java, and transactions in the Java-based Deuce software transactional memory system. We used two SMP platforms (a 16-core Intel Xeon SMP and a 32-Core IBM Power7 SMP) to evaluate our optimizations on 17 explicitly-parallel benchmark programs that span all three models. Our results show that the optimizations introduced in this paper can deliver measurable performance improvements that increase in magnitude when the program is run with a larger number of processor cores. These results underscore the importance of optimizing critical sections, and the fact that the benefits from such optimizations will continue to increase with increasing numbers of cores in future many-core processors.},
address = {Edinburgh, UK},
author = {Barik, Rajkishore and Zhao, Jisheng and Sarkar, Vivek},
booktitle = {Proceedings of the 22nd international conference on Parallel architectures and compilation techniques},
doi = {10.1109/PACT.2013.6618801},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Barik, Zhao, Sarkar - 2013 - Interprocedural Strength Reduction of Critical Sections in Explicitly-Parallel Programs.pdf:pdf},
pages = {29--40},
publisher = {IEEE},
title = {{Interprocedural Strength Reduction of Critical Sections in Explicitly-Parallel Programs}},
url = {http://ieeexplore.ieee.org/document/6618801/},
year = {2013}
}
@phdthesis{Dotsenko2007,
abstract = {The Message Passing Interface (MPI) is the library-based programming model employed by most scalable parallel applications today; however, it is not easy to use. To simplify program development, Partitioned Global Address Space (PGAS) languages have emerged as promising alternatives to MPI. Co-array Fortran (CAF), Titanium, and Unified Parallel C are explicitly parallel single-program multiple-data languages that provide the abstraction of a global shared memory and enable programmers to use one-sided communication to access remote data. This thesis focuses on evaluating PGAS languages and explores new language features to simplify the development of high performance programs in CAF. To simplify program development, we explore extending CAF with abstractions for group, Cartesian, and graph communication topologies that we call co-spaces. The combination of co-spaces, textual barriers, and single values enables effective analysis and optimization of CAF programs. We present an algorithm for synchronization strength reduction (SSR), which replaces textual barriers with faster point-to-point synchronization. This optimization is both difficult and error-prone for developers to perform manually. SSR-optimized versions of Jacobi iteration and the NAS MG and CG benchmarks yield performance similar to that of our best hand-optimized variants and demonstrate significant improvement over their barrier-based counterparts. To simplify the development of codes that rely on producer-consumer communication, we explore extending CAF with multi-version variables (MVVs). MVVs increase programmer productivity by insulating application developers from the details of buffer management, communication, and synchronization. Sweep3D, NAS BT, and NAS SP codes expressed using MVVs are much simpler than the fastest hand-coded variants, and experiments show that they yield similar performance. To avoid exposing latency in distributed memory systems, we explore extending CAF with distributed multithreading (DMT) based on the concept of function shipping. Function shipping facilitates co-locating computation with data as well as executing several asynchronous activities in the remote and local memory. DMT uses co-subroutines/cofunctions to ship computation with either blocking or non-blocking semantics. A prototype implementation and experiments show that DMT simplifies development of parallel search algorithms and the performance of DMT-based RandomAccess exceeds that of the reference MPI implementation.},
author = {Dotsenko, Yuri},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Dotsenko - 2007 - Expressiveness, Programmability and Portable High Performance of Global Address Space Languages.pdf:pdf},
pages = {341},
school = {Rice University},
title = {{Expressiveness, Programmability and Portable High Performance of Global Address Space Languages}},
url = {http://dl.acm.org/citation.cfm?id=1571451},
year = {2007}
}
@incollection{Kale2009,
abstract = {This chapter contains sections titled: Introduction The Programming Model Automatic Adaptive Overlap: Adapting to Communication Latencies and Response Delays Adapting to load Variations in the Application Adapting to Changing Number of Available Processors Adaptive Communication Optimizations Adapting to Available Memory “Adapting” to Faults Language Extensions Summary References},
address = {Hoboken, NJ, USA},
author = {Kale, Laxmikant V and Zheng, Gengbin},
booktitle = {Advanced Computational Infrastructures for Parallel and Distributed Applications},
chapter = {13},
doi = {10.1002/9780470558027.ch13},
editor = {Parashar, M},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Kale, Zheng - 2009 - Charm and AMPI Adaptive Runtime Strategies via Migratable Objects.pdf:pdf},
isbn = {9780470072943},
keywords = {Charm++ and AMPI - adaptive runtime strategies via,adaptive MPI,adaptive implementation and MPI extension on Charm,automatic adaptive overlap - communication latenci},
pages = {265--282},
publisher = {Wiley-Interscience},
title = {{Charm++ and AMPI: Adaptive Runtime Strategies via Migratable Objects}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/9780470558027.ch13/summary},
year = {2009}
}
@article{Zhu1999,
abstract = {Many parallel architectures support a memory model where some memory accesses are local and, thus, inexpensive, while other memory accesses are remote and potentially quite expensive. In the case of memory references via pointers, it is often difficult to determine if the memory reference is guaranteed to be local and, thus, can be handled via an inexpensive memory operation. Determining which memory accesses are local can be done by the programmer, the compiler, or a combination of both. The overall goal is to minimize the work required by the programmer and have the compiler automate the process as much as possible. This paper reports on compiler techniques for determining when indirect memory references are local. The locality analysis has been implemented for a parallel dialect of C called EARTH-C, and it uses an algorithm inspired by type inference algorithms for fast points-to analysis. The algorithm statically estimates when an indirect reference via a pointer can be safely assumed to be a local access. The locality inference algorithm is also used to guide the automatic specialization of functions in order to take advantage of locality specific to particular calling contexts. In addition to these purely static techniques, we also suggest fine-grain and coarse-grain dynamic techniques. In this case, dynamic locality checks are inserted into the program and specialized code for the local case is inserted. In the fine-grain case, the checks are put around single memory references, while in the coarse-grain case the checks are put around larger program segments. The static locality analysis and automatic specialization has been implemented in the EARTH-C compiler, which produces low-level threaded code for the EARTH multithreaded architecture. Experimental results are presented for a set of benchmarks that operate on irregular, dynamically allocated data structures. Overall, the techniques give moderate to significant speedups, with the combination of static and dynamic techniques giving the best performance overall.},
author = {Zhu, Yingchun and Hendren, Laurie J},
doi = {10.1109/71.752778},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Zhu, Hendren - 1999 - Locality Analysis for Parallel C Programs.pdf:pdf},
journal = {Parallel and Distributed Systems, IEEE Transactions on},
keywords = {Algorithm design and analysis,Computer languages,Data structures,EARTH-C,Earth,Inference algorithms,Parallel architectures,Parallel processing,Parallel programming,Program processors,Programming profession,compiler,data structures,dynamic locality checks,dynamically allocated data structures,locality analysis,memory model,parallel C programs,parallel architectures,parallel programming,pointers,program segments,purely static techniques,type inference algorithms},
number = {2},
pages = {99--114},
publisher = {IEEE},
title = {{Locality Analysis for Parallel C Programs}},
url = {http://ieeexplore.ieee.org/document/752778/},
volume = {10},
year = {1999}
}
@inproceedings{Barik2011,
abstract = {X10 is a new object-oriented PGAS (Partitioned Global Address Space) programming language with support for distributed asynchronous dynamic parallelism that goes beyond past SPMD message-passing models such as MPI and SPMD PGAS models such as UPC and Co-Array Fortran. The concurrency constructs in X10 make it possible to express complex computation and communication structures with higher productivity than other distributed-memory programming models. However, this productivity often comes at the cost of high performance overhead when the language is used in its full generality. This paper introduces high-level compiler optimizations and transformations to reduce communication and synchronization overheads in distributed-memory implementations of X10 programs. Specifically, we focus on locality optimizations such as scalar replacement and task localization, combined with supporting transformations such as loop distribution, scalar expansion, loop tiling, and loop splitting. We have completed a prototype implementation of these high-level optimizations, and performed a performance evaluation that shows significant improvements in performance, scalability, communication volume and number of tasks. We evaluated the communication optimizations on three platforms: a 128-node Blue Gene/P cluster, a 32-node Nehalem cluster, and a 16-node Power7 cluster. On the Blue Gene/P cluster, we observed a maximum performance improvement of 31.46x relative to the unoptimized case (for the MolDyn benchmark). On the Nehalem cluster, we observed a maximum performance improvement of 3.01x (for the NQueens benchmark) and on the Power7 cluster, we observed a maximum performance improvement of 2.73x (for the MolDyn benchmark). In addition, there was no case in which the optimized code was slower than the unoptimized case. We also believe that the optimizations presented in this paper will be necessary for any high-productivity PGAS language based on modern object-oriented principles, that is designed for execution on future Extreme Scale systems that place a high premium on locality improvement for performance and energy efficiency.},
address = {Anchorage, AK, USA},
author = {Barik, Rajkishore and Zhao, Jisheng and Grove, David and Peshansky, Igor and Budimlic, Zoran and Sarkar, Vivek},
booktitle = {Parallel {\&} Distributed Processing Symposium (IPDPS), 2011 IEEE International},
doi = {10.1109/IPDPS.2011.105},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Barik et al. - 2011 - Communication Optimizations for Distributed-Memory X10 Programs.pdf:pdf},
keywords = {128-node Blue Gene/P cluster,16-node Power7 cluster,32-node Nehalem cluster,Arrays,Benchmark testing,Electronics packaging,MolDyn benchmark,NQueens benchmark,Object oriented modeling,Optimization,Reactive power,Synchronization,communication optimization,communication overhead reduction,concurrency constructs,concurrency control,distributed asynchronous dynamic parallelism,distributed memory systems,distributed-memory X10 programs,energy efficiency,extreme scale system,high-level compiler optimization,locality optimization,loop distribution,loop splitting,loop tiling,object-oriented partitioned global address space p,object-oriented principle,object-oriented programming,optimising compilers,scalar expansion,scalar replacement,synchronization overhead reduction,task localization},
pages = {1101--1113},
publisher = {IEEE},
title = {{Communication Optimizations for Distributed-Memory X10 Programs}},
year = {2011}
}
@inproceedings{Georganas2012,
abstract = {To efficiently scale dense linear algebra problems to future exascale systems, communication cost must be avoided or overlapped. Communication-avoiding 2.5D algorithms improve scalability by reducing inter-processor data transfer volume at the cost of extra memory usage. Communication overlap attempts to hide messaging latency by pipelining messages and overlapping with computational work. We study the interaction and compatibility of these two techniques for two matrix multiplication algorithms (Cannon and SUMMA), triangular solve, and Cholesky factorization. For each algorithm, we construct a detailed performance model that considers both critical path dependencies and idle time. We give novel implementations of 2.5D algorithms with overlap for each of these problems. Our software employs UPC, a partitioned global address space (PGAS) language that provides fast one-sided communication. We show communication avoidance and overlap provide a cumulative benefit as core counts scale, including results using over 24K cores of a Cray XE6 system.},
author = {Georganas, Evangelos and Gonz{\'{a}}lez-Dominguez, Jorge and Solomonik, Edgar and Zheng, Yili and Tourino, Juan and Yelick, Katherine},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
doi = {10.1109/SC.2012.32},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Georganas et al. - 2012 - Communication Avoiding and Overlapping for Numerical Linear Algebra.pdf:pdf},
keywords = {Bandwidth,Cholesky factorization,Cray XE6 system,Hardware,Layout,Linear algebra,Message systems,PGAS language,Partitioning algorithms,Program processors,UPC,communication cost,communication overlap,communication-avoiding 2.5D algorithms,computational work,dense linear algebra problems,electronic data interchange,exascale systems,interprocessor data transfer volume reduction,linear algebra,matrix multiplication,matrix multiplication algorithms,messaging latency,multiprocessing systems,numerical linear algebra,one-sided communication,partitioned global address space language,pipeline processing,pipelining messages,triangular solve},
pages = {1--11},
publisher = {IEEE Computer Society},
title = {{Communication Avoiding and Overlapping for Numerical Linear Algebra}},
url = {http://dl.acm.org/citation.cfm?id=2389132 http://ieeexplore.ieee.org/document/6468500/},
year = {2012}
}
@inproceedings{Sanz2012,
abstract = {Chapel is a parallel programming language designed to improve the productivity and ease of use of conventional and parallel computers. This language currently delivers sub optimal performance when executing codes that perform global data re-allocation operations on distributed memory architectures. This is mainly due to data communication that is done without aggregation (one message for each remote array element). In this work, we analyze Chapel's standard Block and Cyclic distribution modules and optimize the communication routines for array assignments by performing aggregation. Thanks to the expressive power of Chapel, the compiler and runtime have enough information to do communication aggregation without user intervention. The runtime relies on the low-level GAS Net networking layer, whose versions of one-sided bulk put/get routines that support strides are particularly useful for us. Experimental results conducted on Hector (a Cray XE6) and Jaguar (Cray XK6)reveal that the implemented techniques can lead to significant reductions in communication time.},
author = {Sanz, Alberto and Asenjo, Rafael and Lopez, Juan and Larrosa, Rafael and Navarro, Angeles and Litvinov, Vassily and Choi, Sung-Eun and Chamberlain, Bradford L},
booktitle = {Computer Architecture and High Performance Computing (SBAC-PAD), 2012 IEEE 24th International Symposium on},
doi = {10.1109/SBAC-PAD.2012.18},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Sanz et al. - 2012 - Global Data Re-allocation via Communication Aggregation in Chapel.pdf:pdf},
keywords = {Arrays,Block distribution,Chapel,Chapel standard block and cyclic distribution modu,Cray XE6,Cray XK6,Hector,Indexes,Jaguar,Productivity,Reactive power,Runtime,Standards,array assignment,code execution,communication aggregation,communication routine optimization,communication time reduction,compiler,cyclic distribution,data communication,data handling,data re-distribution,distributed memory architecture,global data reallocation operation,low-level GAS Net networking layer,one-sided bulk put-get routine,one-sided communications,parallel computer,parallel languages,parallel programming,parallel programming language,productivity,program compilers,remote array element,runtime,suboptimal performance,user intervention},
pages = {235--242},
publisher = {IEEE Computer Society},
title = {{Global Data Re-allocation via Communication Aggregation in Chapel}},
url = {http://dl.acm.org/citation.cfm?id=2419783{\&}preflayout=tabs http://dx.doi.org/10.1109/SBAC-PAD.2012.18},
year = {2012}
}
@inproceedings{Iancu2005,
abstract = {Hiding communication latency is an important optimization for parallel programs. Programmers or compilers achieve this by using non-blocking communication primitives and overlapping communication with computation or other communication operations. Using non-blocking communication raises two issues: performance and programmability. In terms of performance, optimizers need to find a good communication schedule and are sometimes constrained by lack of full application knowledge. In terms of programmability, efficiently managing nonblocking communication can prove cumbersome for complex applications. In this paper we present the design principles of HUNT, a runtime system designed to search and exploit some of the available overlap present at execution time in UPC programs. Using virtual memory support, our runtime implements demand-driven synchronization for data involved in communication operations. It also employs message decomposition and scheduling heuristics to transparently improve the non-blocking behavior of applications. We provide a user level implementation of HUNT on a variety of modern high performance computing systems. Results indicate that our approach is successful in finding some of the overlap available at execution time. While system and application characteristics influence performance, perhaps the determining factor is the time taken by the CPU to execute a signal handler. Demand driven synchronization at execution time eliminates the need for the explicit management of non-blocking communication. Besides increasing programmer productivity, this feature also simpli- fies compiler analysis for communication optimizations.},
author = {Iancu, Costin and Husbands, Parry and Hargrove, Paul},
booktitle = {Parallel Architectures and Compilation Techniques, 2005. PACT 2005. 14th International Conference on},
doi = {10.1109/PACT.2005.25},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Iancu, Husbands, Hargrove - 2005 - HUNTing the Overlap.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Iancu, Husbands, Hargrove - 2005 - HUNTing the Overlap.ppt:ppt},
pages = {279--290},
publisher = {IEEE Computer Society},
title = {{HUNTing the Overlap}},
url = {http://dl.acm.org/citation.cfm?id=1092222 http://dx.doi.org/10.1109/PACT.2005.25},
year = {2005}
}
@inproceedings{Chen2005,
abstract = {Global address space languages like UPC exhibit high performance and portability on a broad class of shared and distributed memory parallel architectures. The most scalable applications use bulk memory copies rather than individual reads and writes to the shared space, but finer grained sharing can be useful for scenarios such as dynamic load balancing, event signaling, and distributed hash tables. In this paper we present three optimization techniques for global address space programs with fine-grained communication: redundancy elimination, use of split-phase communication, and communication coalescing. Parallel UPC programs are analyzed using static single assignment form and a data flow graph, which are extended to handle the various shared and private pointer types that are available in UPC. The optimizations also take advantage of UPC{\'{y}}s relaxed memory consistency model, which reduces the need for cross thread analysis. We demonstrate the effectiveness of the analysis and optimizations using several benchmarks, which were chosen to reflect the kinds of fine grained, communication-intensive phases that exist in some larger applications. The optimizations show speedups of up to 70{\%} on three parallel systems, which represent three different types of cluster network technologies.},
author = {Chen, Wei-Yu and Iancu, Costin and Yelick, Katherine},
booktitle = {Parallel Architectures and Compilation Techniques, 2005. PACT 2005. 14th International Conference on},
doi = {10.1109/PACT.2005.13},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Iancu, Yelick - 2005 - Communication Optimizations for Fine-Grained UPC Applications.pdf:pdf},
isbn = {0-7695-2429-X},
pages = {267--278},
publisher = {IEEE Computer Society},
title = {{Communication Optimizations for Fine-Grained UPC Applications}},
url = {http://dl.acm.org/citation.cfm?id=1092221 http://dx.doi.org/10.1109/PACT.2005.13},
year = {2005}
}
@inproceedings{Zhu1998,
abstract = {This paper presents algorithms for reducing the communication overhead for parallel C programs that use dynamically-allocated data structures. The framework consists of an analysis phase called possible-placement analysis, and a transformation phase called communication selection.The fundamental idea of possible-placement analysis is to find all possible points for insertion of remote memory operations. Remote reads are propagated upwards, whereas remote writes are propagated downwards. Based on the results of the possible-placement analysis, the communication selection transformation selects the "best" place for inserting the communication, and determines if pipelining or blocking of communication should be performed.The framework has been implemented in the EARTH-McCAT optimizing/parallelizing C compiler, and experimental results are presented for five pointer-intensive benchmarks running on the EARTH-MANNA distributed-memory parallel architecture. These experiments show that the communication optimization can provide performance improvements of up to 16{\%} over the unoptimized benchmarks.},
address = {Montreal, Quebec, Canada},
author = {Zhu, Yingchun and Hendren, Laurie J},
booktitle = {Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation},
doi = {10.1145/277652.277723},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Zhu, Hendren - 1998 - Communication Optimizations for Parallel C Programs.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Zhu, Hendren - 1998 - Communication Optimizations for Parallel C Programs.zip:zip},
isbn = {0-89791-987-4},
issn = {0362-1340},
number = {5},
pages = {199--211},
publisher = {ACM},
title = {{Communication Optimizations for Parallel C Programs}},
url = {http://doi.acm.org/10.1145/277652.277723 http://dl.acm.org/citation.cfm?doid=277650.277723},
volume = {33},
year = {1998}
}
@inproceedings{Vadhiyar2000,
abstract = {The performance of the MPI's collective communications is critical in most MPI-based applications. A general algorithm for a given collective communication operation may not give good performance on all systems due to the differences in architectures, network parameters and the storage capacity of the underlying MPI implementation. In this paper we discuss an approach in which the collective communications are tuned for a given system by conducting a series of experiments on the system. We also discuss a dynamic topology method that uses the tuned static topology shape, but re-orders the logical addresses to compensate for changing run time variations. A series of experiments were conducted comparing our tuned collective communication operations to various native vendor MPI implementations. The use of the tuned collective communications resulted in about 30 percent to 650 percent improvement in performance over the native MPI implementations.},
author = {Vadhiyar, Sathish S and Fagg, Graham E and Dongarra, Jack},
booktitle = {Proceedings of the 2000 ACM/IEEE Conference on Supercomputing},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Vadhiyar, Fagg, Dongarra - 2000 - Automatically Tuned Collective Communications.pdf:pdf},
isbn = {0-7803-9802-5},
pages = {3},
publisher = {IEEE Computer Society},
title = {{Automatically Tuned Collective Communications}},
url = {http://dl.acm.org/citation.cfm?id=370049.370055},
year = {2000}
}
@inproceedings{Sharma2014,
abstract = {This paper presents modulo unrolling without unrolling (modulo unrolling WU), a method for message aggregation for parallel loops in message passing programs that use affine array accesses in Chapel, a Partitioned Global Address Space (PGAS) parallel programming language. Messages incur a non-trivial run time overhead, a significant component of which is independent of the size of the message. Therefore, aggregating messages improves performance. Our optimization for message aggregation is based on a technique known as modulo unrolling, pioneered by Barua [3], whose purpose was to ensure a statically predictable single tile number for each memory reference for tiled architectures, such as the MIT Raw Machine [18]. Modulo unrolling WU applies to data that is distributed in a cyclic or block-cyclic manner. In this paper, we adapt the aforementioned modulo unrolling technique to the difficult problem of efficiently compiling PGAS languages to message passing architectures. When applied to loops and data distributed cyclically or block-cyclically, modulo unrolling WU can decide when to aggregate messages thereby reducing the overall message count and runtime for a particular loop. Compared to other methods, modulo unrolling WU greatly simplifies the complex problem of automatic code generation of message passing code. It also results in substantial performance improvement compared to the non-optimized Chapel compiler. To implement this optimization in Chapel, we modify the leader and follower iterators in the Cyclic and Block Cyclic data distribution modules. Results were collected that compare the performance of Chapel programs optimized with modulo unrolling WU and Chapel programs using the existing Chapel data distributions. Data collected on a ten-locale cluster show that on average, modulo unrolling WU used with Chapel's Cyclic distribution results in 64 percent fewer messages and a 36 percent decrease in runtime for our suite of benchmarks. Similarly, modulo unrolling WU used with Chapel's Block Cyclic distribution results in 72 percent fewer messages and a 53 percent decrease in runtime.},
address = {Eugene, OR, USA},
author = {Sharma, Aroon and Smith, Darren and Koehler, Joshua and Barua, Rajeev and Ferguson, Michael},
booktitle = {Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models},
doi = {10.1145/2676870.2676877},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Sharma et al. - 2014 - Affine Loop Optimization Based on Modulo Unrolling in Chapel.pdf:pdf},
isbn = {978-1-4503-3247-7},
pages = {13:1--13:12},
publisher = {ACM},
title = {{Affine Loop Optimization Based on Modulo Unrolling in Chapel}},
url = {http://doi.acm.org/10.1145/2676870.2676877 http://dl.acm.org/citation.cfm?id=2676877},
year = {2014}
}
@inproceedings{Nishtala2009,
abstract = {In earlier work, we showed that the one-sided communication model found in PGAS languages (such as UPC) offers significant advantages in communication efficiency by decoupling data transfer from processor synchronization. We explore the use of the PGAS model on IBM BlueGene/P, an architecture that combines low-power, quad-core processors with extreme scalability. We demonstrate that the PGAS model, using a new port of the Berkeley UPC compiler and GASNet one-sided communication layer, outperforms two-sided (MPI) communication in both microbenchmarks and a case study of the communication-limited benchmark, NAS FT. We scale the benchmark up to 16,384 cores of the BlueGene/P and demonstrate that UPC consistently outperforms MPI by as much as 66{\%} for some processor configurations and an average of 32{\%}. In addition, the results demonstrate the scalability of the PGAS model and the Berkeley implementation of UPC, the viability of using it on machines with multicore nodes, and the effectiveness of the BG/P communication layer for supporting one-sided communication and PGAS languages.},
author = {Nishtala, Rajesh and Hargrove, Paul H and Bonachea, Dan O and Yelick, Katherine A},
booktitle = {Proceedings of the 2009 IEEE International Symposium on Parallel {\&}Distributed Processing},
doi = {10.1109/IPDPS.2009.5161076},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Nishtala et al. - 2009 - Scaling communication-intensive applications on BlueGeneP using one-sided communication and overlap.pdf:pdf},
isbn = {978-1-4244-3751-1},
pages = {1--12},
publisher = {IEEE Computer Society},
title = {{Scaling communication-intensive applications on BlueGene/P using one-sided communication and overlap}},
url = {http://dx.doi.org/10.1109/IPDPS.2009.5161076 http://dl.acm.org/citation.cfm?id=1587648},
year = {2009}
}
@inproceedings{Nishtala2009a,
abstract = {As the gap in performance between the processors and the memory systems continue to grow, the communication component of an application will dictate the overall application performance and scalability. Therefore it is useful to abstract common communication operations across cores as collective communication operations and tune them through a runtime library that can employ sophisticated automatic tuning techniques. Our focus of this paper is on collective communication in Partitioned Global Address Space languages which are a natural extension of the shared memory hardware of modern multicore systems. In particular we highlight how automatic tuning can lead to significant performance improvements and show how loosening the synchronization semantics of a collective can lead to a more efficient use of the memory system. We demonstrate that loosely synchronized collectives can realize consistent 3× speedups over their strictly synchronized counterparts on the highly threaded Sun Niagara2 for message sizes ranging from 8 bytes to 64kB. We thus argue that the synchronization requirements for a collective must be exposed in the interface so the collective and the synchronization can be optimized together.},
author = {Nishtala, Rajesh and Yelick, Katherine A},
booktitle = {Proceedings of the First USENIX conference on Hot topics in parallelism},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Nishtala, Yelick - 2009 - Optimizing Collective Communication on Multicores.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Nishtala, Yelick - 2009 - Optimizing Collective Communication on Multicores(2).pdf:pdf},
pages = {8--18},
publisher = {USENIX Association},
title = {{Optimizing Collective Communication on Multicores}},
url = {http://dl.acm.org/citation.cfm?id=1855591.1855609},
year = {2009}
}
@phdthesis{Chen2007,
abstract = {Unified Parallel C (UPC) is an example of a partitioned global address space language for high performance parallel computing. This programming model enables application to be written in a shared memory style, but still allows the programmer to control data layout and the assignment of work to processors. An open question is whether programs written in simple style, with fine-grained accesses and blocking communication, can achieve performance approaching that of hand-optimized code, especially for cluster environments with high network latencies. This dissertation proposes an optimization framework for UPC that automates the transformations currently performed manually by programmers. The goals of the optimizations are twofold: not only do we seek to aggregate fine-grained remote accesses to reduce the number and volume of message traffic, but we also want to overlap communication with computation to hide network latency. The framework first applies communication vectorization and strip-mining to optimize regular array accesses in loops. For irregular fine-grained accesses, we apply a partial redundancy elimination framework that also generates split-phase communication. The last phase targets the blocking bulk transfers in the program by utilizing runtime support to automatically schedule them and achieve overlap. Message aggregation is performed as part of the scheduling to further reduce the communication overhead. Finally, we present several techniques for optimizing the serial performance of a UPC program, in order to reduce both the overhead of UPC-specific constructs and our source-to-source translation. The optimization framework has been implemented in the Berkeley UPC compiler, and has been tested on a number of supercomputer clusters. The optimizations are validated on a variety of benchmarks exhibiting different communication patterns, from bulk synchronous benchmarks to dynamic shared data structure code. Experimental results reveal that our framework offers comparable performance to aggressive manual optimization, and can achieve significant speedup compared to the fine-grained and blocking communication code that programmers find much easier to implement. Our framework is completely transparent to the user, and therefore improves productivity by freeing programmers from the details of communication management.},
author = {Chen, Wei-Yu},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Chen - 2007 - Optimizing Partitioned Global Address Space Programs for Cluster Architectures.pdf:pdf},
month = {dec},
number = {UCB/EECS-2007-140},
school = {University of California, Berkeley},
title = {{Optimizing Partitioned Global Address Space Programs for Cluster Architectures}},
url = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2007/EECS-2007-140.html},
year = {2007}
}
@techreport{Sidelnik2011,
abstract = {It has been widely shown that GPGPU architectures offer large performance gains compared to their traditional CPU counterparts for many applications. The downside to these architectures is that the current programming models present numerous challenges to the programmer: lower-level languages, explicit data movement, loss of portability, and challenges in performance optimization. In this paper, we present novel methods and compiler transformations that increase productivity by enabling users to easily program GPGPU architectures using the high productivity programming language Chapel. Rather than resorting to different parallel libraries or annotations for a given parallel platform, we leverage a language that has been designed from first principles to address the challenge of programming for parallelism and locality. This also has the advantage of being portable across distinct classes of parallel architectures, including desktop multicores, distributed memory clusters, large-scale shared memory, and now CPU-GPU hybrids. We present experimental results from the Parboil benchmark suite which demonstrate that codes written in Chapel achieve performance comparable to the original versions implemented in CUDA.},
author = {Sidelnik, Albert and Chamberlain, Bradford L and Garzaran, Maria J and Padua, David},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Sidelnik et al. - 2011 - Using the High Productivity Language Chapel to Target GPGPU Architectures.pdf:pdf},
keywords = {Chapel,Compilers,Cuda,GPGPU,HPC,Languages,PGAS},
title = {{Using the High Productivity Language Chapel to Target GPGPU Architectures}},
url = {http://hdl.handle.net/2142/18874},
year = {2011}
}
@inproceedings{Sidelnik2012,
abstract = {It has been widely shown that high-throughput computing architectures such as GPUs offer large performance gains compared with their traditional low-latency counterparts for many applications. The downside to these architectures is that the current programming models present numerous challenges to the programmer: lower-level languages, loss of portability across different architectures, explicit data movement, and challenges in performance optimization. This paper presents novel methods and compiler transformations that increase programmer productivity by enabling users of the language Chapel to provide a single code implementation that the compiler can then use to target not only conventional multiprocessors, but also high-throughput and hybrid machines. Rather than resorting to different parallel libraries or annotations for a given parallel platform, this work leverages a language that has been designed from first principles to address the challenge of programming for parallelism and locality. This also has the advantage of providing portability across different parallel architectures. Finally, this work presents experimental results from the Parboil benchmark suite which demonstrate that codes written in Chapel achieve performance comparable to the original versions implemented in CUDA on both GPUs and multicore platforms.},
author = {Sidelnik, Albert and Maleki, Saeed and Chamberlain, Bradford L and Garzar{\'{a}}n, Mar$\backslash$'$\backslash$ia Jes{\'{u}}s and Padua, David},
booktitle = {Parallel {\&} Distributed Processing Symposium (IPDPS), 2012 IEEE 26th International},
doi = {10.1109/IPDPS.2012.60},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Sidelnik et al. - 2012 - Performance Portability with the Chapel Language.pdf:pdf},
isbn = {978-0-7695-4675-9},
issn = {1530-2075},
keywords = {Arrays,Benchmark testing,Chapel Language,Graphics processing unit,Multicore processing,Parallel processing,Parboil benchmark suite,Reactive power,compiler transformation,explicit data movement,high-throughput computing architectures,hybrid machines,lower-level languages,parallel architectures,parallel libraries,parallel platform,parallel programming,parallelising compilers,performance optimization,performance portability,programming models,single code implementation,software libraries,software performance evaluation,software portability},
pages = {582--594},
title = {{Performance Portability with the Chapel Language}},
url = {http://dx.doi.org/10.1109/IPDPS.2012.60},
year = {2012}
}
@article{Burkhart2011,
abstract = {While the performance of supercomputers has increased dramatically during the last 15 years, programming models and programming languages have more or less remained constant. Two de facto standards, the Message Passing Interface (MPI) for programming distributed memory architectures and OpenMP for programming shared-memory architectures still dominate the field of computational science and engineering. As current supercomputers are constellations with symmetric multiprocessor nodes that are coupled by high-speed inter-connects such as Infiniband, hybrid versions OpenMP/MPI have been introduced. From the programmer's point of view, this is rather complicated and the resulting software productivity is not at all comparable with corresponding practice of commercial software elsewhere. While today's desktop computers are already parallel systems, this effect will be even more accentuated in the coming years. Extrapolation from the current TOP500 list tells us that a desktop or laptop computer will have more than 1000 processor elements in 10 years from now. Thus, in order to use computers efficiently, all programmers today have to address parallelism issues.},
author = {Burkhart, Helmar and Christen, Matthias and Sathe, M and Schenk, O and Rietmann, M},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Burkhart et al. - 2014 - Run Stencil, Run.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Burkhart et al. - 2014 - Run Stencil, Run(2).pdf:pdf},
journal = {Workshop on Parallel Systems and Algorithms (PARS) - Mitteilungen 2011},
keywords = {Benchmarking,CUDA,Computer science,MPI,OpenMP,Tesla C2050,nVidia},
pages = {18},
title = {{Run, Stencil, Run! – A Comparison of Modern Parallel Programming Paradigms}},
year = {2011}
}
@phdthesis{Brunner2000,
author = {Brunner, Robert K},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Brunner - 2000 - Versatile Automatic Load Balancing with Migratable Objects.pdf:pdf},
month = {jan},
school = {University of Illinois at Urbana-Champaign},
title = {{Versatile Automatic Load Balancing with Migratable Objects}},
type = {TR},
url = {https://ppl.cs.illinois.edu/media/00-08},
year = {2000}
}
@article{Meneses2015,
abstract = {Supercomputers have seen an exponential increase in their size in the last two decades. Such a high growth rate is expected to take us to exascale in the timeframe 2018-2022. But, to bring a productive exascale environment about, it is necessary to focus on several key challenges. One of those challenges is fault tolerance. Machines at extreme scale will experience frequent failures and will require the system to avoid or overcome those failures. Various techniques have recently been developed to tolerate failures. The impact of these techniques and their scalability can be substantially enhanced by a parallel programming model called migratable objects. In this paper, we demonstrate how the migratable-objects model facilitates and improves several fault tolerance approaches. Our experimental results on thousands of cores suggest fault tolerance schemes based on migratable objects have low performance overhead and high scalability. Additionally, we present a performance model that predicts a significant benefit of using migratable objects to provide fault tolerance at extreme scale.},
author = {Meneses, Esteban and Ni, Xiang and Zheng, Gengbin and Mendes, Celso L and Kale, Laxmikant V},
doi = {10.1109/TPDS.2014.2342228},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Meneses et al. - 2014 - Using Migratable Objects to Enhance Fault Tolerance Schemes in Supercomputers.pdf:pdf},
issn = {1045-9219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Computational modeling,Fault tolerance,Fault tolerant systems,Migratable objects,Protocols,Runtime,Sockets,Supercomputers,checkpoint/restart,fault tolerance,fault tolerance schemes,message logging,migratable objects,parallel programming,parallel programming model,resilience,software fault tolerance,supercomputers},
pages = {2061--2074},
publisher = {IEEE Computer Society},
title = {{Using Migratable Objects to Enhance Fault Tolerance Schemes in Supercomputers}},
url = {http://ieeexplore.ieee.org/document/6862914/},
volume = {26},
year = {2015}
}
@techreport{Kale2012,
author = {Kale, Laxmikant K and Arya, Anshu and Jain, Nikhil and Langer, Akhil and Lifflander, Jonathan and Menon, Harshitha and Ni, Xiang and Sun, Yanhua and Totoni, Ehsan and Venkataraman, Ramprasad and Wesolowski, Lukasz},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Kale et al. - 2012 - Migratable Objects Active Messages Adaptive Runtime = Productivity Performance A Submission to 2012 HPC Class II.pdf:pdf},
institution = {Parallel Programming Laboratory},
month = {nov},
number = {12-47},
title = {{Migratable Objects + Active Messages + Adaptive Runtime = Productivity + Performance A Submission to 2012 HPC Class II Challenge}},
url = {http://charm.cs.illinois.edu/papers/12-47},
year = {2012}
}
@inproceedings{Acun:2014:PPM:2683593.2683664,
address = {Piscataway, NJ, USA},
author = {Acun, Bilge and Gupta, Abhishek and Jain, Nikhil and Langer, Akhil and Menon, Harshitha and Mikida, Eric and Ni, Xiang and Robson, Michael and Sun, Yanhua and Totoni, Ehsan and Wesolowski, Lukasz and Kale, Laxmikant},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1109/SC.2014.58},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Acun et al. - 2014 - Parallel Programming with Migratable Objects Charm in Practice.pdf:pdf},
isbn = {978-1-4799-5500-8},
pages = {647--658},
publisher = {IEEE Press},
series = {SC '14},
title = {{Parallel Programming with Migratable Objects: Charm++ in Practice}},
url = {https://doi.org/10.1109/SC.2014.58},
year = {2014}
}
@misc{kinaSUR,
    author = "{Ministry for Primary Industries}",
    title = {{Kina sea urchin regions in NZ}},
    howpublished = {\url{http://fs.fish.govt.nz/Page.aspx?pk=7&sc=SUR}},
    note = {Online; accessed 29 January 2014} ,
    year=2013,
}
@misc{charm_edu,
annote = {Accessed: 2015-05-30},
howpublished = {http://charm.cs.illinois.edu},
title = {{Charm++}}
}
@inproceedings{Cogumbreiro2015,
address = {New York, NY, USA},
author = {Cogumbreiro, Tiago and Hu, Raymond and Martins, Francisco and Yoshida, Nobuko},
booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
doi = {10.1145/2688500.2688519},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Cogumbreiro et al. - 2015 - Dynamic Deadlock Verification for General Barrier Synchronisation.pdf:pdf},
isbn = {978-1-4503-3205-7},
pages = {150--160},
publisher = {ACM},
series = {PPoPP 2015},
title = {{Dynamic Deadlock Verification for General Barrier Synchronisation}},
url = {http://doi.acm.org/10.1145/2688500.2688519},
year = {2015}
}
@inproceedings{Shirako2008,
author = {Shirako, Jun and Peixotto, David M and Sarkar, Vivek and Scherer, William N},
booktitle = {Proceedings of the 22nd annual international conference on Supercomputing},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Shirako et al. - 2008 - Phasers A Unified Deadlock-Free Construct for Collective and Point-to-Point Synchronization.pdf:pdf},
pages = {277--288},
title = {{Phasers: A Unified Deadlock-Free Construct for Collective and Point-to-Point Synchronization}},
year = {2008}
}
@incollection{Bensalem2006,
author = {Bensalem, Saddek and Havelund, Klaus},
booktitle = {Hardware and Software, Verification and Testing},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Bensalem, Havelund - 2006 - Dynamic Deadlock Analysis of Multi-Threaded Programs.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Bensalem, Havelund - 2006 - Dynamic Deadlock Analysis of Multi-Threaded Programs(2).pdf:pdf},
pages = {208--223},
publisher = {Springer},
title = {{Dynamic Deadlock Analysis of Multi-Threaded Programs}},
year = {2006}
}
@article{Raman2012,
author = {Raman, Raghavan and Zhao, Jisheng and Sarkar, Vivek and Vechev, Martin and Yahav, Eran},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Raman et al. - 2012 - Efficient Data Race Detection for Async-Finish Parallelism.pdf:pdf},
journal = {Formal Methods in System Design},
number = {3},
pages = {321--347},
publisher = {Springer},
title = {{Efficient Data Race Detection for Async-Finish Parallelism}},
volume = {41},
year = {2012}
}
@inproceedings{Raman2012a,
author = {Raman, Raghavan and Zhao, Jisheng and Sarkar, Vivek and Vechev, Martin and Yahav, Eran},
booktitle = {ACM SIGPLAN Notices},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Raman et al. - 2012 - Scalable and Precise Dynamic Datarace Detection for Structured Parallelism.pdf:pdf},
number = {6},
pages = {531--542},
title = {{Scalable and Precise Dynamic Datarace Detection for Structured Parallelism}},
volume = {47},
year = {2012}
}
@inproceedings{Park2011,
author = {Park, Chang-Seo and Sen, Koushik and Hargrove, Paul and Iancu, Costin},
booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - 2011 - Efficient Data Race Detection for Distributed Memory Parallel Programs.pdf:pdf},
pages = {51},
title = {{Efficient Data Race Detection for Distributed Memory Parallel Programs}},
year = {2011}
}
@inproceedings{Park2013,
author = {Park, Chang Seo and Sen, Koushik and Iancu, Costin},
booktitle = {Proceedings of the 27th international ACM conference on International conference on supercomputing},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Park, Sen, Iancu - 2013 - Scaling Data Race Detection for Partitioned Global Address Space Programs.pdf:pdf},
pages = {47--58},
title = {{Scaling Data Race Detection for Partitioned Global Address Space Programs}},
year = {2013}
}
@inproceedings{Serres2014,
author = {Serres, Olivier and Kayi, Abdullah and Anbar, Ahmad and El-Ghazawi, Tarek},
booktitle = {Proceedings of the 11th ACM Conference on Computing Frontiers},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Serres et al. - 2014 - Hardware support for address mapping in PGAS languages a UPC case study.pdf:pdf},
pages = {22},
title = {{Hardware support for address mapping in PGAS languages: a UPC case study}},
year = {2014}
}
@inproceedings{Gonzalez-Dom\\inguez2015,
author = {Gonz{\'{a}}lez-Dom$\backslash$'{\{}$\backslash$i{\}}nguez, Jorge and Schmidt, Bertil},
booktitle = {GPU Technology Conference (GTC 2015)},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Gonz{\'{a}}lez-Dom'{\{}i{\}}nguez, Schmidt - 2015 - Using a CUDA-Accelerated PGAS Model on a GPU Cluster for Bioinformatics.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Gonz{\'{a}}lez-Dom'{\{}i{\}}nguez, Schmidt - 2015 - Using a CUDA-Accelerated PGAS Model on a GPU Cluster for Bioinformatics(2).pdf:pdf},
title = {{Using a CUDA-Accelerated PGAS Model on a GPU Cluster for Bioinformatics}},
year = {2015}
}
@article{Sourbier2009,
author = {Sourbier, Florent and Operto, St{\'{e}}phane and Virieux, Jean and Amestoy, Patrick and L'Excellent, Jean-Yves},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Sourbier et al. - 2009 - FWT2D A massively parallel program for frequency-domain full-waveform tomography of wide-aperture seismic data.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Sourbier et al. - 2009 - FWT2D A massively parallel program for frequency-domain full-waveform tomography of wide-aperture seismic da(2).pdf:pdf},
journal = {Computers {\&} Geosciences},
number = {3},
pages = {487--495},
publisher = {Elsevier},
title = {{FWT2D: A massively parallel program for frequency-domain full-waveform tomography of wide-aperture seismic data - Part 1: Algorithm}},
volume = {35},
year = {2009}
}
@article{Gonzalez-Dom\\inguez2015a,
author = {Gonz{\'{a}}lez-Dom$\backslash$'$\backslash$inguez, Jorge and K{\"{a}}ssens, Jan C and Wienbrandt, Lars and Schmidt, Bertil},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Gonz{\'{a}}lez-Dom'inguez et al. - 2015 - Large-scale genome-wide association studies on a GPU cluster using a CUDA-accelerated PGAS programm.pdf:pdf},
journal = {International Journal of High Performance Computing Applications},
pages = {1094342015585846},
publisher = {SAGE Publications},
title = {{Large-scale genome-wide association studies on a GPU cluster using a CUDA-accelerated PGAS programming model}},
year = {2015}
}
@phdthesis{Sbirlea2011,
author = {Sbirlea, Alina Gabriela},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Sbirlea - 2011 - Mapping a Data-Flow Programming Model onto Heterogeneous Platforms.pdf:pdf},
school = {Rice University},
title = {{Mapping a Data-Flow Programming Model onto Heterogeneous Platforms}},
year = {2011}
}
@inproceedings{Chen2012,
author = {Chen, Yifeng and Cui, Xiang and Mei, Hong},
booktitle = {ACM SIGPLAN Notices},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Cui, Mei - 2012 - Parray A Unifying Array Representation for Heterogeneous Parallelism.pdf:pdf},
number = {8},
pages = {171--180},
title = {{Parray: A Unifying Array Representation for Heterogeneous Parallelism}},
volume = {47},
year = {2012}
}
@article{Lena2014,
author = {Lena, ODEN},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Lena - 2014 - GPI2 for GPUs A PGAS framework for efficient communication in hybrid clusters.pdf:pdf},
journal = {Parallel Computing: Accelerating Computational Science and Engineering (CSE)},
pages = {461},
publisher = {IOS Press},
title = {{GPI2 for GPUs: A PGAS framework for efficient communication in hybrid clusters}},
volume = {25},
year = {2014}
}
@inproceedings{Simmendinger2011,
author = {Simmendinger, Christian and J{\"{a}}gersk{\"{u}}pper, Jens and Machado, Rui and Lojewski, Carsten},
booktitle = {Proceedings of the 5th Conference on Partitioned Global Address Space Programming Models, PGAS},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Simmendinger et al. - 2011 - A PGAS-Based Implementation for the Unstructured CFD Solver TAU.pdf:pdf},
title = {{A PGAS-Based Implementation for the Unstructured CFD Solver TAU}},
volume = {11},
year = {2011}
}
@phdthesis{Sethi2011,
abstract = {High-performance architectures are becoming more and more complex with the
passage of time. These large scale, heterogeneous architectures and multi-core system
are difficult to program. New programming models are required to make expression of
parallelism easier, while keeping productivity of the developer higher.
Partition Global Address-space (PGAS) languages such as UPC appeared to augment
developer's productivity for distributed memory systems. UPC provides a simpler,
shared memory-like model with a user control over data layout. But it is developer's
responsibility to take care of the data locality, by using appropriate data layouts.
SMPSs/StarSs programming model tries to simplify the parallel programming on multicore architectures. It offers task level parallelism, where dependencies among the
tasks are determined at the run time. In addition, runtime take cares of the data
locality, while scheduling tasks. Hence, providing two-folds improvement in
productivity; first, saving developer's time by using automatic dependency detection,
instead of hard coding them. Second, save cache optimization time, as runtime take
cares of data locality.
The purpose of this thesis is to use the PGAS programming model e.g. UPC for different
nodes with the shared memory task based parallelization model i.e. StarSs to take the
advantage of the multi core systems and contrast this approach to the legacy MPI and
OpenMP combination. Performance as well as programmability is considered in the
evaluation.
The combination UPC + SMPSs, results in approximately the same execution time as
MPI and OpenMP. The current lack of features such as multi-dimensional data
distribution or virtual topologies in UPC, make the hybrid UPC + SMPSs/StarSs
programming model less programmable than MPI + OpenMP for the application
studied in this thesis.},
author = {Sethi, Muhammad Wahaj},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Sethi - 2011 - Hybrid parallel computing beyond MPI {\&}amp OpenMP - I ntroducing PGAS {\&}amp StarSs.pdf:pdf},
pages = {101},
school = {Universit{\"{a}}t Stuttgart},
title = {{Hybrid parallel computing beyond MPI {\&} OpenMP - I ntroducing PGAS {\&} StarSs}},
url = {http://elib.uni-stuttgart.de/handle/11682/6404 http://elib.uni-stuttgart.de/bitstream/11682/6404/1/MSTR{\_}3215.pdf},
year = {2011}
}
@article{Teijeiro2013,
author = {Teijeiro, Carlos and Sutmann, Godehard and Taboada, Guillermo L and Touri{\~{n}}o, Juan},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Teijeiro et al. - 2013 - Parallel simulation of Brownian dynamics on shared memory systems with OpenMP and Unified Parallel C.pdf:pdf},
journal = {The Journal of Supercomputing},
number = {3},
pages = {1050--1062},
publisher = {Springer},
title = {{Parallel simulation of Brownian dynamics on shared memory systems with OpenMP and Unified Parallel C}},
url = {https://link.springer.com/article/10.1007/s11227-012-0843-1},
volume = {65},
year = {2013}
}
@incollection{Mallon2009,
author = {Mall{\'{o}}n, Dami{\'{a}}n A and Taboada, Guillermo L and Teijeiro, Carlos and Touri{\~{n}}o, Juan and Fraguela, Basilio B and G{\'{o}}mez, Andr{\'{e}}s and Doallo, Ram{\'{o}}n and Mouri{\~{n}}o, J Carlos},
booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Mall{\'{o}}n et al. - 2009 - Performance Evaluation of MPI, UPC and OpenMP on Multicore Architectures.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Mall{\'{o}}n et al. - 2009 - Performance Evaluation of MPI, UPC and OpenMP on Multicore Architectures(2).pdf:pdf},
pages = {174--184},
publisher = {Springer},
title = {{Performance Evaluation of MPI, UPC and OpenMP on Multicore Architectures}},
year = {2009}
}
@article{Murata2014,
author = {Murata, Hiroki and Horie, Michihiro and Shirahata, Koichi and Doi, Jun and Tai, Hideki and Takeuchi, Mikio and Kawachiya, Kiyokuni},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Murata et al. - 2014 - Porting MPI based HPC Applications to X10.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Murata et al. - 2014 - Porting MPI based HPC Applications to X10(2).pdf:pdf},
journal = {Porting MPI based HPC Applications to X10},
title = {{Porting MPI based HPC Applications to X10}},
year = {2014}
}
@inproceedings{Kamil2014,
abstract = {Multidimensional arrays are an important data structure in many scientific applications. Unfortunately, built-in support for such arrays is inadequate in C++, particularly in the distributed setting where bulk communication operations are required for good performance. In this paper, we present a multidimensional library for partitioned global address space (PGAS) programs, supporting the one-sided remote access and bulk operations of the PGAS model. The library is based on Titanium arrays, which have proven to provide good productivity and performance. These arrays provide a local view of data, where each rank constructs its own portion of a global data structure, matching the local view of execution common to PGAS programs and providing maximum flexibility in structuring global data. Unlike Titanium, which has its own compiler with array-specific analyses, optimizations, and code generation, we implement multidimensional arrays solely through a C++ library. The main goal of this effort is to provide a library-based implementation that can match the productivity and performance of a compiler-based approach. We implement the array library as an extension to UPC++, a C++ library for PGAS programs, and we extend Titanium arrays with specializations to improve performance. We evaluate the array library by porting four Titanium benchmarks to UPC++, demonstrating that it can achieve up to 25{\%} better performance than Titanium without a significant increase in programmer effort.},
address = {Edinburgh, United Kingdom},
author = {Kamil, Amir and Zheng, Yili and Yelick, Katherine},
booktitle = {Proceedings of ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming},
doi = {10.1145/2627373.2627378},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Kamil, Zheng, Yelick - 2014 - A Local-View Array Library for Partitioned Global Address Space C Programs.pdf:pdf},
pages = {26},
title = {{A Local-View Array Library for Partitioned Global Address Space C++ Programs}},
url = {http://doi.acm.org/10.1145/2627373.2627378},
year = {2014}
}
@article{Barrett2006,
author = {Barrett, Richard and Yao, Yiyi and El-Ghazawi, Tarek},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Barrett, Yao, El-Ghazawi - 2006 - Evaluation of UPC on the Cray X1E.pdf:pdf},
journal = {Cray Users Group},
publisher = {Citeseer},
title = {{Evaluation of UPC on the Cray X1E}},
year = {2006}
}
@inproceedings{El-Ghazawi2005,
abstract = {UPC is parallel programming language which enables programmers to
expose parallelism and data locality in applications with an efficient syntax. Recently,
UPC has been gaining attention from vendors and users as an alternative programming
model for distributed memory applications. Therefore, it is important to understand how
such a potentially powerful language interacts with one of today's most powerful,
contemporary architectures: the Cray X1. In this paper, we evaluate UPC on the Cray
X1 and examine how the compiler exploits the important features of this architecture
including the use of the vector processors and multi-streaming. Our experimental results
on several benchmarks, such as STREAM, RandomAccess, and selected workloads from
the NAS Parallel Benchmark suite, show that UPC can provide a high-performance,
scalable programming model, and we show users how to leverage the power of X1 for
their applications. However, we have also identified areas where compiler analysis can
be more aggressive and potential performance caveats.},
address = {Albuquerque, NM},
author = {El-Ghazawi, Tarek A and Cantonnet, Fran{\c{c}}ois and Yao, Yiyi and Vetter, Jeffrey},
booktitle = {Cray User Group Proceedings},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/El-Ghazawi et al. - 2005 - Evaluation of UPC on the Cray X1.pdf:pdf},
title = {{Evaluation of UPC on the Cray X1}},
year = {2005}
}
@inproceedings{Bell2004,
author = {Bell, Christian and Chen, Wei-Yu and Bonachea, Dan and Yelick, Katherine},
booktitle = {Proceedings of the 18th annual international conference on Supercomputing},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Bell et al. - 2004 - Evaluating Support for Global Address Space Languages on the Cray X1.pdf:pdf},
pages = {184--195},
title = {{Evaluating Support for Global Address Space Languages on the Cray X1}},
year = {2004}
}
@inproceedings{Cantonnet2004,
author = {Cantonnet, Fran{\c{c}}ois and Yao, Yiyi and Zahran, Mohamed and El-Ghazawi, Tarek},
booktitle = {Parallel and Distributed Processing Symposium, 2004. Proceedings. 18th International},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Cantonnet et al. - 2004 - Productivity analysis of the UPC language.pdf:pdf},
pages = {254},
title = {{Productivity analysis of the UPC language}},
year = {2004}
}
@inproceedings{Cantonnet2003,
author = {Cantonnet, Fran{\c{c}}ois and Yao, Yiyi and Annareddy, Smita and Mohamed, Ahmed S and El-Ghazawi, Tarek A},
booktitle = {Parallel and Distributed Processing Symposium, 2003. Proceedings. International},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Cantonnet et al. - 2003 - Performance Monitoring and Evaluation of a UPC Implementation on a NUMA Architecture.pdf:pdf},
pages = {8--pp},
title = {{Performance Monitoring and Evaluation of a UPC Implementation on a NUMA Architecture}},
year = {2003}
}
@inproceedings{Preissl2011,
author = {Preissl, Robert and Wichmann, Nathan and Long, Bill and Shalf, John and Ethier, Stephane and Koniges, Alice},
booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Preissl et al. - 2011 - Multithreaded Global Address Space Communication Techniques for Gyrokinetic Fusion Applications on Ultra-Scale P.pdf:pdf},
pages = {78},
title = {{Multithreaded Global Address Space Communication Techniques for Gyrokinetic Fusion Applications on Ultra-Scale Platforms}},
year = {2011}
}
@inproceedings{Barrett2006a,
author = {Barrett, Richard},
booktitle = {The 48th Cray User Group meeting, Lugano, Italy},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Barrett - 2006 - Co-Array Fortran Experiences with Finite Differencing Methods.pdf:pdf;:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Barrett - 2006 - Co-Array Fortran Experiences with Finite Differencing Methods(2).pdf:pdf},
title = {{Co-Array Fortran Experiences with Finite Differencing Methods}},
year = {2006}
}
@incollection{Coarfa2004,
author = {Coarfa, Cristian and Dotsenko, Yuri and Eckhardt, Jason and Mellor-Crummey, John},
booktitle = {Languages and Compilers for Parallel Computing},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Coarfa et al. - 2004 - Co-array Fortran Performance and Potential An NPB Experimental Study.pdf:pdf},
pages = {177--193},
publisher = {Springer},
title = {{Co-array Fortran Performance and Potential: An NPB Experimental Study}},
year = {2004}
}
@inproceedings{Coarfa2005,
author = {Coarfa, Cristian and Dotsenko, Yuri and Mellor-Crummey, John and Cantonnet, Fran{\c{c}}ois and El-Ghazawi, Tarek and Mohanti, Ashrujit and Yao, Yiyi and Chavarr$\backslash$'$\backslash$ia-Miranda, Daniel},
booktitle = {Proceedings of the tenth ACM SIGPLAN symposium on Principles and practice of parallel programming},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Coarfa et al. - 2005 - An Evaluation of Global Address Space Languages Co-Array Fortran and Unified Parallel C.pdf:pdf},
pages = {36--47},
title = {{An Evaluation of Global Address Space Languages: Co-Array Fortran and Unified Parallel C}},
year = {2005}
}
@techreport{Mellor-Crummey2010,
author = {Mellor-Crummey, John},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Mellor-Crummey - 2010 - Co-Array Fortran for Terascale Clusters.pdf:pdf},
institution = {Rice University, Department of Computer Science},
title = {{Co-Array Fortran for Terascale Clusters}},
year = {2010}
}
@incollection{Kamil2007,
author = {Kamil, Amir and Yelick, Katherine},
booktitle = {Static Analysis},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Kamil, Yelick - 2007 - Hierarchical Pointer Analysis for Distributed Programs.pdf:pdf},
pages = {281--297},
publisher = {Springer},
title = {{Hierarchical Pointer Analysis for Distributed Programs}},
year = {2007}
}
@techreport{Guo2014,
author = {Guo, Chen},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Guo - 2014 - Parallel Prefix Scan in HPX Final Project Report Implementing Asynchronous Prefix Scan Algorithm in HPX Execution Model.pdf:pdf},
institution = {Louisiana State University},
title = {{Implementing Asynchronous Prefix Scan Algorithm in HPX Execution Model}},
year = {2014}
}
@phdthesis{Dekate2011,
abstract = {Traditional scientific applications such as Computational Fluid Dynamics, Partial Differential Equa- tions based numerical methods (like Finite Difference Methods, Finite Element Methods) achieve sufficient efficiency on state of the art high performance computing systems and have been widely studied / implemented using conventional programming models. For emerging application do- mains such as Graph applications scalability and efficiency is significantly constrained by the con- ventional systems and their supporting programming models. Furthermore technology trends like multicore, manycore, heterogeneous system architectures are introducing new challenges and pos- sibilities. Emerging technologies are requiring a rethinking of approaches to more effectively ex- pose the underlying parallelism to the applications and the end-users. This thesis explores the space of effective parallel execution of ephemeral graphs that are dynamically generated. The standard particle based simulation, solved using the Barnes-Hut algorithm is chosen to exemplify the dy- namic workloads. In this thesis the workloads are expressed using sequential execution semantics, a conventional parallel programming model - shared memory semantics and semantics of an inno- vative execution model designed for efficient scalable performance towards Exascale computing called ParalleX. The main outcomes of this research are parallel processing of dynamic ephemeral workloads, enabling dynamic load balancing during runtime, and using advanced semantics for exposing parallelism in scaling constrained applications.},
address = {Baton Rouge, LA, USA},
author = {Dekate, Chirag},
file = {:Users/parsa/Downloads/10.1.1.683.7516.pdf:pdf},
school = {Louisiana State University},
title = {{Extreme Scale Parallel N-Body Algorithm with Event-Driven Constraint-Based Execution Model}},
year = {2011}
}
@phdthesis{Potluri2014,
abstract = {Accelerators (such as NVIDIA GPUs) and coprocessors (such as Intel MIC/Xeon Phi) are fueling the growth of next-generation ultra-scale systems that have high compute density and high performance per watt. However, these many-core architectures cause systems to be heterogeneous by introducing multiple levels of parallelism and varying computation/communication costs at each level. Application developers also use a hierarchy of programming models to extract maximum performance from these heterogeneous systems. Models such as CUDA, OpenCL, LEO, and others are used to express parallelism across accelerator or coprocessor cores, while higher level programming models such as MPI or OpenSHMEM are used to express parallelism across a cluster. The presence of multiple programming models, their runtimes and the varying communication performance at different levels of the system hierarchy has hindered applications from achieving peak performance on these systems. Modern interconnects such as InfiniBand, enable asynchronous communication progress through RDMA, freeing up the cores to do useful computation. MPI and PGAS models offer one-sided communication primitives that extract maximum performance, minimize process synchronization overheads and enable better computation and communication overlap using the high performance networks. However, there is limited literature available to guide scientists in taking advantage of these one-sided communication semantics on high-end applications, more so on heterogeneous clusters. In our work, we present an enhanced model, MVAPICH2-GPU, to use MPI for data movement from both CPU and GPU memories, in a unified manner. We also extend the OpenSHMEM PGAS model to support such unified communication. These models considerably simplify data movement in MPI and OpenSHMEM applications running on GPU clusters. We propose designs in MPI and OpenSHMEM runtimes to optimize data movement on GPU clusters, using state-of-the-art GPU technologies such as CUDA IPC and GPUDirect RDMA. Further, we introduce PRISM, a proxy-based multi-channel framework that enables an optimized MPI library for communication on clusters with Intel Xeon Phi co-processors. We evaluate our designs using micro-benchmarks, application kernels and end-applications. We present the re-design of a petascale seismic modeling code to demonstrate the use of one-sided semantics in end-applications and their impact on performance. We finally demonstrate the benefits of using one-sided semantics on heterogeneous clusters.},
author = {Potluri, Sreeram},
file = {:Users/parsa/Downloads/thesis-sreeram-potluri.pdf:pdf},
pages = {209},
school = {Ohio State University},
title = {{Enabling Efficient Use of MPI and PGAS Programming Models ion Heterogeneous Clusters with High Performance Interconnects}},
url = {https://etd.ohiolink.edu/pg{\_}10?0::NO:10:P10{\_}ACCESSION{\_}NUM:osu1397797221},
year = {2014}
}
@book{Patterson2013,
abstract = {Computer Organization and Design, Fifth Edition, is the latest update to the classic introduction to computer organization. The text now contains new examples and material highlighting the emergence of mobile computing and the cloud. It explores this generational change with updated content featuring tablet computers, cloud infrastructure, and the ARM (mobile computing devices) and x86 (cloud computing) architectures. The book uses a MIPS processor core to present the fundamentals of hardware technologies, assembly language, computer arithmetic, pipelining, memory hierarchies and I/O.Because an understanding of modern hardware is essential to achieving good performance and energy efficiency, this edition adds a new concrete example, Going Faster, used throughout the text to demonstrate extremely effective optimization techniques. There is also a new discussion of the Eight Great Ideas of computer architecture. Parallelism is examined in depth with examples and content highlighting parallel hardware and software topics. The book features the Intel Core i7, ARM Cortex-A8 and NVIDIA Fermi GPU as real-world examples, along with a full set of updated and improved exercises. This new edition is an ideal resource for professional digital system designers, programmers, application developers, and system software developers. It will also be of interest to undergraduate students in Computer Science, Computer Engineering and Electrical Engineering courses in Computer Organization, Computer Design, ranging from Sophomore required courses to Senior Electives.},
author = {Patterson, David A and Hennessy, John L},
chapter = {7.1},
file = {:Users/parsa/Downloads/ComputerOrganizationAndDesign5thEdition2014.pdf:pdf},
isbn = {9780123747501},
pages = {800},
publisher = {Elsevier, Morgan Kaufmann Publishers Inc.},
series = {Morgan Kaufmann Series in Computer Graphics},
title = {{Computer Organization and Design: The Hardware/software Interface (The Morgan Kaufmann Series in Computer Architecture and Design)}},
url = {https://books.google.com/books?id=DMxe9AI4-9gC},
year = {2013}
}
@article{Hoefler2013,
abstract = {Hybrid parallel programming with the message passing interface (MPI) for internode communication in conjunction with a shared-memory programming model to manage intranode parallelism has become a dominant approach to scalable parallel programming. While this model provides a great deal of flexibility and performance potential, it saddles programmers with the complexity of utilizing two parallel programming systems in the same application. We introduce an MPI-integrated shared-memory programming model that is incorporated into MPI through a small extension to the one-sided communication interface. We discuss the integration of this interface with the MPI 3.0 one-sided semantics and describe solutions for providing portable and efficient data sharing, atomic operations, and memory consistency. We describe an implementation of the new interface in the MPICH2 and Open MPI implementations and demonstrate an average performance improvement of 40 {\%} to the communication component of a five-point stencil solver.},
author = {Hoefler, Torsten and Dinan, James and Buntinas, Darius and Balaji, Pavan and Barrett, Brian and Brightwell, Ron and Gropp, William and Kale, Vivek and Thakur, Rajeev},
doi = {10.1007/s00607-013-0324-2},
file = {:Users/parsa/Downloads/art{\%}3A10.1007{\%}2Fs00607-013-0324-2.pdf:pdf},
issn = {0010-485X},
journal = {Computing},
keywords = {Hybrid parallel programming,MPI-3.0,Shared memory},
month = {dec},
number = {12},
pages = {1121--1136},
publisher = {Springer},
title = {{MPI + MPI: a new hybrid approach to parallel programming with MPI plus shared memory}},
url = {http://link.springer.com/10.1007/s00607-013-0324-2},
volume = {95},
year = {2013}
}
@inproceedings{Rabenseifner2003,
abstract = {Most HPC systems are clusters of shared memory nodes. Parallel programming must combine the distributed mem- ory parallelization on the node inter-connect with the shared memory parallelization inside of each node. Various hybrid MPI+OpenMP programming models are compared with pure MPI. Benchmark results of several platforms are presented. This paper analyzes the strength and weakness of several parallel programming models on clusters of SMP nodes. Benchmark results on a Myrinet cluster and on re- cent Cray, NEC, IBM, Hitachi, SUN and SGI platforms show, that the hybrid-masteronly programming model can be used more efficiently on some vector-type systems, but also on clusters of dual-CPUs. On other systems, one CPU is not able to saturate the inter-node network and the commonly used masteronly programming model suffers from insufficient inter-node bandwidth. This paper analy- ses strategies to overcome typical drawbacks of this easily usable programming scheme on systems with weaker inter- connects. Best performance can be achieved with overlap- ping communication and computation, but this scheme is lacking in ease of use.},
address = {Aachen, Germany},
author = {Rabenseifner, Rolf},
booktitle = {Proceedings of the Fifth European Workshop on OpenMP, EWOMP},
file = {:Users/parsa/Downloads/300a7cbd734d6f0b595337626ea2c87501ba.pdf:pdf},
keywords = {HPC,Hybrid Parallel Programming,MPI,OpenMP,Performance,Threads and MPI},
month = {sep},
pages = {185--194},
publisher = {www.compunity.org},
series = {EWOMP '03},
title = {{Hybrid parallel programming on HPC platforms}},
url = {https://pdfs.semanticscholar.org/4d3d/300a7cbd734d6f0b595337626ea2c87501ba.pdf},
volume = {3},
year = {2003}
}
@inproceedings{Dinan2010,
abstract = {The Message Passing Interface (MPI) is one of the most widely used programming models for parallel computing. However, the amount of memory available to an MPI process is limited by the amount of local memory within a compute node. Partitioned Global Address Space (PGAS) models such as Unified Parallel C (UPC) are growing in popularity because of their ability to provide a shared global address space that spans the memories of multiple compute nodes. However, taking advantage of UPC can require a large recoding effort for existing parallel applications. In this paper, we explore a new hybrid parallel programming model that combines MPI and UPC. This model allows MPI programmers incremental access to a greater amount of memory, enabling memory-constrained MPI codes to process larger data sets. In addition, the hybrid model offers UPC programmers an opportunity to create static UPC groups that are connected over MPI. As we demonstrate, the use of such groups can significantly improve the scalability of locality-constrained UPC codes. This paper presents a detailed description of the hybrid model and demonstrates its effectiveness in two applications: a random access benchmark and the Barnes-Hut cosmological simulation. Experimental results indicate that the hybrid model can greatly enhance performance; using hybrid UPC groups that span two cluster nodes, RA performance increases by a factor of 1.33 and using groups that span four cluster nodes, Barnes-Hut experiences a twofold speedup at the expense of a 2{\%} increase in code size.},
address = {New York, New York, USA},
author = {Dinan, James and Balaji, Pavan and Lusk, Ewing and Sadayappan, P and Thakur, Rajeev},
booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers - CF '10},
doi = {10.1145/1787275.1787323},
file = {:Users/parsa/Downloads/p177-dinan.pdf:pdf},
isbn = {9781450300445},
month = {may},
pages = {177},
publisher = {ACM Press},
title = {{Hybrid parallel programming with MPI and unified parallel C}},
url = {http://portal.acm.org/citation.cfm?doid=1787275.1787323},
year = {2010}
}
@article{Jin2011,
abstract = {The rapidly increasing number of cores in modern microprocessors is pushing the current high performance computing (HPC) systems into the petascale and exascale era. The hybrid nature of these systems – distributed memory across nodes and shared memory with non-uniform memory access within each node – poses a challenge to application developers. In this paper, we study a hybrid approach to programming such systems – a combination of two traditional programming models, MPI and OpenMP. We present the performance of standard benchmarks from the multi-zone NAS Parallel Benchmarks and two full applications using this approach on several multi-core based systems including an SGI Altix 4700, an IBM p575+ and an SGI Altix ICE 8200EX. We also present new data locality extensions to OpenMP to better match the hierarchical memory structure of multi-core architectures.},
author = {Jin, Haoqiang and Jespersen, Dennis and Mehrotra, Piyush and Biswas, Rupak and Huang, Lei and Chapman, Barbara},
doi = {10.1016/j.parco.2011.02.002},
file = {:Users/parsa/Downloads/1-s2.0-S0167819111000159-main.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
month = {sep},
number = {9},
pages = {562--575},
publisher = {Elsevier},
title = {{High performance computing using MPI and OpenMP on multi-core parallel systems}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167819111000159},
volume = {37},
year = {2011}
}
@article{Chorley2010,
abstract = {The mixing of shared memory and message passing programming models within a single application has often been suggested as a method for improving scientific application performance on clusters of shared memory or multi-core systems. DL{\_}POLY, a large scale molecular dynamics application programmed using message passing programming has been modified to add a layer of shared memory threading and the performance analysed on two multi-core clusters. At lower processor numbers, the extra overheads from shared memory threading in the hybrid code outweigh performance benefits gained over the pure MPI code. On larger core counts the hybrid model performs better than pure MPI, with reduced communication time decreasing the overall runtime.},
author = {Chorley, Martin J and Walker, David W},
doi = {10.1016/j.jocs.2010.05.001},
file = {:Users/parsa/Downloads/1-s2.0-S1877750310000396-main.pdf:pdf},
issn = {18777503},
journal = {Journal of Computational Science},
keywords = {Hybrid programming,Message passing,Multi-core,Shared memory},
month = {aug},
number = {3},
pages = {168--174},
publisher = {Elsevier},
title = {{Performance analysis of a hybrid MPI/OpenMP application on multi-core clusters}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877750310000396},
volume = {1},
year = {2010}
}
@incollection{Lusk2008,
abstract = {The paper describes some very early experiments on new architectures that support the hybrid programming model. Our results are promising in that OpenMP threads interact with MPI as desired, allowing OpenMP-agnostic tools to be used. We explore three environments: a “typical” Linux cluster, a new large-scale machine from SiCortex, and the new IBM BG/P, which have quite different compilers and runtime systems for both OpenMP and MPI. We look at a few simple, diagnostic programs, and one “application-like” test program. We demonstrate the use of a tool that can examine the detailed sequence of events in a hybrid program and illustrate that a hybrid computation might not always proceed as expected.},
author = {Lusk, Ewing and Chan, Anthony},
booktitle = {OpenMP in a New Era of Parallelism},
doi = {10.1007/978-3-540-79561-2_4},
file = {:Users/parsa/Downloads/chp{\%}3A10.1007{\%}2F978-3-540-79561-2{\_}4.pdf:pdf;:Users/parsa/Downloads/bok{\%}3A978-3-540-79561-2.pdf:pdf},
pages = {36--47},
publisher = {Springer},
title = {{Early Experiments with the OpenMP/MPI Hybrid Programming Model}},
url = {http://link.springer.com/10.1007/978-3-540-79561-2{\_}4},
year = {2008}
}
@inproceedings{Li2010,
abstract = {Abstract: Power-aware execution of parallel programs is now a primary concern in large-scale HPC environments. Prior research in this area has explored models and algorithms based on dynamic voltage and frequency scaling (DVFS) and dynamic concurrency throttling (DCT) to achieve power-aware execution of programs written in a single programming model, typically MPI or OpenMP. However, hybrid programming models combining MPI and OpenMP are growing in popularity as emerging large-scale systems have many nodes with several processors per node and multiple cores per process or. In th is paper we present and evaluate solutions for power-efficient execution of programs written in this hybrid model targeting large-scale distributed systems with multicore nodes. We use a new power-aware performance prediction model of hybrid MPI/OpenMP applications to derive a novel algorithm for power-efficient execution of realis tic applications from th e ASCS equoia and N PB MZ bench marks. Our new algorithm yields substantial energy savings (4.18{\%} on average and up to 13.8{\%}) with either negligible performance loss or performance gain (up to 7.2{\%}).},
address = {Atlanta, GA, USA},
author = {Li, Dong and de Supinski, Bronis R and Schulz, Martin and Cameron, Kirk and Nikolopoulos, Dimitrios S},
booktitle = {2010 IEEE International Symposium on Parallel {\&} Distributed Processing (IPDPS)},
doi = {10.1109/IPDPS.2010.5470463},
file = {:Users/parsa/Downloads/05470463.pdf:pdf},
isbn = {978-1-4244-6442-5},
issn = {1530-2075},
pages = {1--12},
publisher = {IEEE},
title = {{Hybrid MPI/OpenMP power-aware computing}},
url = {http://ieeexplore.ieee.org/document/5470463/},
year = {2010}
}
@inproceedings{Rabenseifner2006,
author = {Rabenseifner, Rolf and Hager, Georg and Jost, Gabriele and Keller, Rainer},
booktitle = {PVM/MPI},
file = {:Users/parsa/Downloads/IWR{\_}11Jul2008{\_}mpi{\_}openmp{\_}2to1.pdf:pdf},
pages = {11},
title = {{Hybrid MPI and OpenMP parallel programming}},
year = {2006}
}
@article{Yang2011266,
abstract = {Nowadays, NVIDIA's CUDA is a general purpose scalable parallel programming model for writing highly parallel applications. It provides several key abstractions – a hierarchy of thread blocks, shared memory, and barrier synchronization. This model has proven quite successful at programming multithreaded many core GPUs and scales transparently to hundreds of cores: scientists throughout industry and academia are already using CUDA to achieve dramatic speedups on production and research codes. In this paper, we propose a parallel programming approach using hybrid CUDA OpenMP, and MPI programming, which partition loop iterations according to the number of C1060 GPU nodes in a GPU cluster which consists of one C1060 and one S1070. Loop iterations assigned to one MPI process are processed in parallel by CUDA run by the processor cores in the same computational node.},
author = {Yang, Chao-Tung and Huang, Chih-Lin and Lin, Cheng-Fang},
doi = {10.1016/j.cpc.2010.06.035},
file = {:Users/parsa/Downloads/1-s2.0-S0010465510002262-main.pdf:pdf},
issn = {00104655},
journal = {Computer Physics Communications},
month = {jan},
number = {1},
pages = {266--269},
publisher = {Elsevier},
title = {{Hybrid CUDA, OpenMP, and MPI parallel programming on multicore GPU clusters}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0010465510002262},
volume = {182},
year = {2011}
}
@inproceedings{Rabenseifner2003a,
abstract = {Most HPC systems are clusters of shared memory nodes. Parallel programming must combine the distributed mem- ory parallelization on the node inter-connect with the shared memory parallelization inside of each node. Various hybrid MPI+OpenMP programming models are compared with pure MPI. Benchmark results of several platforms are presented. This paper analyzes the strength and weak- ness of several parallel programming models on clusters of SMP nodes. Benchmark results show, that the hybrid- masteronly programming model can be used more effi- ciently on some vector-type systems, although this model suffers from sleeping application threads while the master thread communicates. This paper analyses strategies to overcome typical drawbacks of this easily usable program- ming scheme on systems with weaker inter-connects. Best performance can be achieved with overlapping communi- cation and computation, but this scheme is lacking in ease of use.},
author = {Rabenseifner, Rolf},
booktitle = {Proceedings of the 45th Cray User Group Conference, Ohio},
file = {:Users/parsa/Downloads/Rabensei.pdf:pdf},
keywords = {HPC,Hybrid Parallel Programming,MPI,OpenMP,Performance,Threads and MPI},
pages = {12--16},
title = {{Hybrid parallel programming: Performance problems and chances}},
url = {https://cug.org/5-publications/proceedings{\_}attendee{\_}lists/2003CD/S03{\_}Proceedings/Pages/Authors/Rabenseifner/Rabensei.pdf},
year = {2003}
}
@inproceedings{Rabenseifner2009,
abstract = {Today most systems in high-performance computing (HPC) feature a hierarchical hardware design: Shared memory nodes with several multi-core CPUs are connected via a network infrastructure. Parallel programming must combine distributed memory parallelization on the node interconnect with shared memory parallelization inside each node. We describe potentials and challenges of the dominant programming models on hierarchically structured hardware: Pure MPI (Message Passing Interface), pure OpenMP (with distributed shared memory extensions) and hybrid MPI+OpenMP in several ¿avors. We pinpoint cases where a hybrid programming model can indeed be the superior solution because of reduced communication needs and memory consumption, or improved load balance. Furthermore we show that machine topology has a signi¿cant impact on performance for all parallelization strategies and that topology awareness should be built into all applications in the future. Finally we give an outlook on possible standardization goals and extensions that could make hybrid programming easier to do with performance in mind.},
address = {Weimar, Germany},
author = {Rabenseifner, Rolf and Hager, Georg and Jost, Gabriele},
booktitle = {2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing},
doi = {10.1109/PDP.2009.43},
file = {:Users/parsa/Downloads/04912964.pdf:pdf},
isbn = {978-0-7695-3544-9},
issn = {1066-6192},
month = {feb},
pages = {427--436},
publisher = {IEEE},
series = {PDND '09},
title = {{Hybrid MPI/OpenMP Parallel Programming on Clusters of Multi-Core SMP Nodes}},
url = {http://ieeexplore.ieee.org/document/4912964/},
year = {2009}
}
@inproceedings{Jost2003,
abstract = {Clusters of SMP (Symmetric Multi-Processors) nodes provide support for a wide range of parallel programming paradigms. The shared address space within each node is suitable for OpenMP parallelization. Message passing can be employed within and across the nodes of a cluster. Multiple levels of parallelism can be achieved by combining message passing and OpenMP parallelization. Which programming paradigm is the best will depend on the nature of the given problem, the hardware components of the cluster, the network, and the available software. In this study we compare the performance of different implementations of the same CFD benchmark application, using the same numerical algorithm but employing different programming paradigms.},
address = {Aachen, Germany},
author = {Jost, Gabriele and Jin, Haoqiang},
booktitle = {Proceedings of the 4th European Workshop on OpenMP},
file = {:Users/parsa/Downloads/20030107321.pdf:pdf},
keywords = {APPLICATIONS PROGRAMS (COMPUTERS),CLUSTERS,COMPUTATIONAL FLUID DYNAMICS,PARALLEL PROGRAMMING},
mendeley-tags = {PARALLEL PROGRAMMING,CLUSTERS,APPLICATIONS PROGRAMS (COMPUTERS),COMPUTATIONAL FLUID DYNAMICS},
month = {sep},
publisher = {NASA},
series = {European Workshop on OpenMP and Applications 2003},
title = {{Comparing the OpenMP, MPI, and Hybrid Programming Paradigm on an SMP Cluster}},
year = {2003}
}
@inproceedings{Yelick2006,
author = {Yelick, Kathy},
booktitle = {LCPC},
file = {:Users/parsa/Downloads/LCPC06Yelick.pdf:pdf},
pages = {1},
title = {{Compilation Techniques for Partitioned Global Address Space Languages}},
year = {2006}
}
@misc{Khan2011,
annote = {Accessed: 2015-05-30},
author = {Khan, Rishi},
file = {:Users/parsa/Desktop/PEMWS-ETI.pdf:pdf},
howpublished = {http://www.eecis.udel.edu/{\~{}}szuckerm/pemws-2/PEMWS-ETI.pdf},
title = {{An Introduction to SWARM: SWift Adaptive Runtime Machine}},
url = {https://www.eecis.udel.edu/{~}szuckerm/pemws-2/PEMWS-ETI.pdf},
year = {2011}
}
@incollection{Barton2007,
abstract = {The main attraction of Partitioned Global Address Space (PGAS) languages to programmers is the ability to distribute the data to exploit the affinity of threads within shared-memory domains. Thus, PGAS languages, such as Unified Parallel C (UPC), are a promising programming paradigm for emerging parallel machines that employ hierarchical data- and task-parallelism. For example, large systems are built as distributed-shared memory architectures, where multicore nodes access a local, coherent address space and many such nodes are interconnected in a non-coherent address space to form a high-performance system. This paper studies the access patterns of shared data in UPC programs. By analyzing the access patterns of shared data in UPC we are able to make three major observations about the characteristics of programs written in a PGAS programming model: (i) there is strong evidence to support the development of automatic identification and automatic privatization of local shared data accesses; (ii) the ability for the programmer to specify how shared data is distributed among the executing threads can result in significant performance improvements; (iii) running UPC programs on a hybrid architecture will significantly increase the opportunities for automatic privatization of local shared data accesses.},
address = {Berlin, Heidelberg},
author = {Barton, Christopher and Caşcaval, Călin and Amaral, Jos{\'{e}} Nelson},
booktitle = {Languages and Compilers for Parallel Computing},
doi = {10.1007/978-3-540-72521-3_9},
file = {:Users/parsa/Downloads/barton-lcpc06-slides.pdf:pdf;:Users/parsa/Downloads/chp{\%}3A10.1007{\%}2F978-3-540-72521-3{\_}9.pdf:pdf;:Users/parsa/Downloads/bok{\%}3A978-3-540-72521-3.pdf:pdf},
isbn = {978-3-540-72520-6},
pages = {111--125},
publisher = {Springer Berlin Heidelberg},
title = {{A Characterization of Shared Data Access Patterns in UPC Programs}},
url = {http://link.springer.com/10.1007/978-3-540-72521-3{\_}9},
year = {2007}
}
@inproceedings{Yelick2007,
abstract = {Partitioned Global Address Space (PGAS) languages combine the programming convenience of shared memory with the locality and performance control of message passing. One such language, Unified Parallel C (UPC) is an extension of ISO C defined by a consortium that boasts multiple proprietary and open source compilers. Another PGAS language, Titanium, is a dialect of JavaTM designed for high performance scientific computation. In this paper we describe some of the highlights of two related projects, the Titanium project centered at U.C. Berkeley and the UPC project centered at Lawrence Berkeley National Laboratory. Both compilers use a source-to-source strategy that trans-lates the parallel languages to C with calls to a communication layer called GASNet. The result is portable high-performance compilers that run on a large variety of shared and distributed memory multiprocessors. Both projects combine compiler, runtime, and application efforts to demonstrate some of the performance and productivity advantages to these languages.},
address = {New York, New York, USA},
author = {Yelick, Katherine and Husbands, Parry and Iancu, Costin and Kamil, Amir and Nishtala, Rajesh and Su, Jimmy and Welcome, Michael and Wen, Tong and Bonachea, Dan and Chen, Wei-Yu and Colella, Phillip and Datta, Kaushik and Duell, Jason and Graham, Susan L and Hargrove, Paul and Hilfinger, Paul},
booktitle = {Proceedings of the 2007 international workshop on Parallel symbolic computation - PASCO '07},
doi = {10.1145/1278177.1278183},
file = {:Users/parsa/Downloads/p24-yelick.pdf:pdf},
isbn = {9781595937414},
keywords = {GASNet,NAS parallel benchmarks,PGAS,UPC,one-sided communication,partitioned global address space,titanium},
mendeley-tags = {GASNet,NAS parallel benchmarks,PGAS,UPC,one-sided communication,partitioned global address space,titanium},
month = {jul},
pages = {24},
publisher = {ACM Press},
title = {{Productivity and performance using partitioned global address space languages}},
url = {http://portal.acm.org/citation.cfm?doid=1278177.1278183},
year = {2007}
}
@inproceedings{Kale1993,
address = {New York, New York, USA},
author = {Kale, Laxmikant V and Krishnan, Sanjeev},
booktitle = {Proceedings of the eighth annual conference on Object-oriented programming systems, languages, and applications - OOPSLA '93},
doi = {10.1145/165854.165874},
file = {:Users/parsa/Downloads/p91-kale.pdf:pdf},
isbn = {0897915879},
month = {oct},
number = {10},
pages = {91--108},
publisher = {ACM Press},
series = {OOPSLA '93},
title = {{CHARM++}},
url = {http://portal.acm.org/citation.cfm?doid=165854.165874},
volume = {28},
year = {1993}
}
@book{Stitt2009,
author = {Stitt, Tim},
publisher = {Connexions, Rice University},
title = {{An Introduction to the Partitioned Global Address Space (PGAS) Programming Model}},
year = {2009}
}
@inproceedings{Barrett2015,
abstract = {The Bulk Synchronous Parallel programming model is showing performance limitations at high processor counts. We propose over-decomposition of the domain, operated on as tasks, to smooth out utilization of the computing resource, in particular the node interconnect and processing cores, and hide intra- and inter-node data movement. Our approach maintains the existing coding style commonly employed in computational science and engineering applications. Although we show improved performance on existing computers, up to 131,072 processor cores, the effectiveness of this approach on expected future architectures will require the continued evolution of capabilities throughout the codesign stack. Success then will not only result in decreased time to solution, but would also make better use of the hardware capabilities and reduce power and energy requirements, while fundamentally maintaining the current code configuration strategy.},
address = {New York, New York, USA},
author = {Barrett, Richard F and Stark, Dylan T and Vaughan, Courtenay T and Grant, Ryan E and Olivier, Stephen L and Pedretti, Kevin T},
booktitle = {Proceedings of the Sixth International Workshop on Programming Models and Applications for Multicores and Manycores - PMAM '15},
doi = {10.1145/2712386.2712388},
file = {:Users/parsa/Downloads/p30-barrett.pdf:pdf},
isbn = {9781450334044},
month = {feb},
pages = {30--39},
publisher = {ACM Press},
series = {PMAM '15},
title = {{Toward an evolutionary task parallel integrated MPI + X programming model}},
url = {http://dl.acm.org/citation.cfm?doid=2712386.2712388},
year = {2015}
}
@inproceedings{Daily2013,
author = {Daily, Jeffrey and Vishnu, Abhinav and Palmer, Bruce and van Dam, Hubertus},
booktitle = {The International Conference for High Performance Computing, Network, Storage and Analysis. IEEE Computer Society},
file = {:Users/parsa/Downloads/01302921.pdf:pdf},
month = {dec},
publisher = {IEEE Computer Society},
title = {{PGAS Models Using an MPI Runtime: Design Alternatives and Performance Evaluation}},
year = {2013}
}
@inproceedings{Kuchera2004,
abstract = {Abstract: Summary form only given. The memory consistency model underlying the Unified Parallel C (UPC) language remains a promising but underused feature. We report on our efforts to understand the UPC memory model and assess its potential benefits. We describe problems we have uncovered in the current language specification. These results have inspired an effort in the UPC community to create an alternative memory model definition that avoids these problems. We give experimental results confirming the promise of performance gains afforded by the memory model's relaxed constraints on consistency.},
address = {Santa Fe, NM, USA, USA},
author = {Kuchera, William and Wallace, Charles},
booktitle = {18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.},
doi = {10.1109/IPDPS.2004.1302921},
file = {:Users/parsa/Downloads/01302921.pdf:pdf},
isbn = {0-7695-2132-0},
month = {apr},
pages = {16--25},
publisher = {IEEE},
series = {IPDPS '04},
title = {{The UPC memory model: problems and prospects}},
url = {http://ieeexplore.ieee.org/document/1302921/},
year = {2004}
}
@inproceedings{Hower2014,
abstract = {Commodity heterogeneous systems (e.g., integrated CPUs and GPUs), now support a unified, shared memory address space for all components. Because the latency of global communication in a heterogeneous system can be prohibi-tively high, heterogeneous systems (unlike homogeneous CPU systems) provide synchronization mechanisms that only guarantee ordering among a subset of threads, which we call a scope. Unfortunately, the consequences and se-mantics of these scoped operations are not yet well under-stood. Without a formal and approachable model to reason about the behavior of these operations, we risk an array of portability and performance issues. In this paper, we embrace scoped synchronization with a new class of memory consistency models that add scoped synchronization to data-race-free models like those of C++ and Java. Called sequential consistency for heterogeneous-race-free (SC for HRF), the new models guarantee SC for programs with "sufficient" synchronization (no data races) of "sufficient" scope. We discuss two such models. The first, HRF-direct, works well for programs with highly regular parallelism. The second, HRF-indirect, builds on HRF-direct by allowing synchronization using different scopes in some cases involving transitive communication. We quanti-tatively show that HRF-indirect encourages forward-looking programs with irregular parallelism by showing up to a 10{\%} performance increase in a task runtime for GPUs.},
address = {New York, New York, USA},
author = {Hower, Derek R and Hechtman, Blake A and Beckmann, Bradford M and Gaster, Benedict R and Hill, Mark D and Reinhardt, Steven K and Wood, David A},
booktitle = {Proceedings of the 19th international conference on Architectural support for programming languages and operating systems - ASPLOS '14},
doi = {10.1145/2541940.2541981},
file = {:Users/parsa/Downloads/p427-hower.pdf:pdf},
isbn = {9781450323055},
keywords = {data-race-free,heterogeneous systems,memory consistency model,task runtime},
mendeley-tags = {data-race-free,heterogeneous systems,memory consistency model,task runtime},
pages = {427--440},
publisher = {ACM Press},
title = {{Heterogeneous-race-free memory models}},
url = {http://dl.acm.org/citation.cfm?doid=2541940.2541981},
year = {2014}
}
@inproceedings{Kamil2005,
abstract = {Abstract: The memory consistency model in shared memory parallel programming controls the order in which memory operations performed by one thread may be observed by another. The most natural model for programmers is to have memory accesses appear to take effect in the order specified in the original program. Language designers have been reluctant to use this strong semantics, called sequential consistency, due to concerns over the performance of memory fence instructions and related mechanisms that guarantee order. In this paper, we provide evidence for the practicality of sequential consistency by showing that advanced compiler analysis techniques are sufficient to eliminate the need for most memory fences and enable high-level optimizations. Our analyses eliminated over 97{\%} of the memory fences that were needed by a na¨{\'{y}}ve implementation, accounting for 87 to 100{\%} of the dynamically encountered fences in all but one benchmark. The impact of the memory model and analysis on runtime performance depends on the quality of the optimizations: more aggressive optimizations are likely to be invalidated by a strong memory consistency semantics. We consider two specific optimizations pipelining of bulk memory copies and communication aggregation and scheduling for irregular accesses and show that our most aggressive analysis is able to obtain the same performance as the relaxed model when applied to two linear algebra kernels. While additional work on parallel optimizations and analyses is needed, we believe these results provide important evidence on the viability of using a simple memory consistency model without sacrificing performance.},
address = {Seattle, WA, USA},
author = {Kamil, Amir and Su, Jimmy and Yelick, Katherine},
booktitle = {ACM/IEEE SC 2005 Conference (SC'05)},
doi = {10.1109/SC.2005.43},
file = {:Users/parsa/Downloads/27580015.pdf:pdf},
isbn = {1-59593-061-2},
month = {dec},
pages = {15--15},
publisher = {IEEE},
series = {SC '05},
title = {{Making Sequential Consistency Practical in Titanium}},
url = {http://ieeexplore.ieee.org/document/1559967/},
year = {2005}
}
@incollection{Mintz2014,
abstract = {The multicore generation of scientific high performance computing has provided a platform for the realization of Exascale computing, and has also underscored the need for new paradigms in coding parallel applications. The current standard for writing parallel applications requires programmers to use languages designed for sequential execution. These languages have abstractions that only allow programmers to operate on the process centric local view of data. To provide suitable languages for parallel execution, many research efforts have designed languages based on the Partitioned Global Address Space (PGAS) programming model. Chapel is one of the more recent languages to be developed using this model. Chapel supports multithreaded execution with high-level abstractions for parallelism. With Chapel in mind, we have developed a set of directives that serve as intermediate expressions for transitioning scientific applications from languages designed for sequential execution to PGAS languages like Chapel that are being developed with parallelism in mind.},
address = {Annapolis, MD, USA},
author = {Mintz, Tiffany M and Hernandez, Oscar and Bernholdt, David E},
booktitle = {OpenSHMEM and Related Technologies. Experiences, Implementations, and Tools},
doi = {10.1007/978-3-319-05215-1_9},
file = {:Users/parsa/Downloads/chp{\%}3A10.1007{\%}2F978-3-319-05215-1{\_}9.pdf:pdf;:Users/parsa/Downloads/bok{\%}3A978-3-319-05215-1.pdf:pdf},
isbn = {978-3-319-05215-1},
pages = {120--133},
publisher = {Springer International Publishing},
title = {{A Global View Programming Abstraction for Transitioning MPI Codes to PGAS Languages}},
url = {http://link.springer.com/10.1007/978-3-319-05215-1{\_}9},
year = {2014}
}
@inproceedings{Chen2007a,
abstract = {Overlapping communication with computation is an important optimization on current cluster architectures; its importance is likely to increase as the doubling of processing power far outpaces any improvements in communication latency. PGAS languages offer unique opportunities for communication overlap, because their one-sided communication model enables low overhead data transfer. Recent results have shown the value of hiding latency by manually applying language-level nonblocking data transfer routines, but this process can be both tedious and error-prone. In this paper, we present a runtime framework that automatically schedules the data transfers to achieve overlap. The optimization framework is entirely transparent to the user, and aggressively reorders and aggregates both remote puts and gets. We preserve correctness via runtime conflict checks and temporary buffers, using several techniques to lower the overhead. Experimental results on application benchmarks suggest that our framework can be very effective at hiding communication latency on clusters, improving performance over the blocking code by an average of 16{\%} for some of the NAS Parallel Benchmarks, 48{\%} for GUPS, and over 25{\%} for a multi-block fluid dynamics solver. While the system is not yet as effective as aggressive manual optimization, it increases programmers' productivity by freeing them from the details of communication management.},
address = {Seattle, Washington},
author = {Chen, Wei-Yu and Bonachea, Dan and Iancu, Costin and Yelick, Katherine},
booktitle = {Proceedings of the 21st annual international conference on Supercomputing - ICS '07},
doi = {10.1145/1274971.1274995},
file = {:Users/parsa/Downloads/p158-chen.pdf:pdf},
isbn = {9781595937681},
pages = {158},
publisher = {ACM Press},
title = {{Automatic nonblocking communication for partitioned global address space programs}},
url = {http://portal.acm.org/citation.cfm?doid=1274971.1274995},
year = {2007}
}
@inproceedings{Becker2009,
author = {Becker, Aaron and Miller, Phil and Kale, Laxmikant V},
booktitle = {1st Workshop on Asynchrony in the PGAS Programming Model APGAS},
file = {:Users/parsa/Downloads/paper.pdf:pdf},
month = {jun},
organization = {Parallel Programming Laboratory, Department of Computer Science, University of Illinois at Urbana-Champaign},
title = {{PGAS in the Message-Driven Execution Model}},
url = {http://charm.cs.illinois.edu/papers/09-14},
year = {2009}
}
@inproceedings{Kaiser2009,
abstract = {High performance computing (HPC) is experiencing a phase change with the challenges of programming and management of heterogeneous multicore systems architectures and large scale system configurations. It is estimated that by the end of the next decade Exaflops computing systems requiring hundreds of millions of cores demanding multi-billion-way parallelism with a power budget of 50 Gflops/watt may emerge. At the same time, there are many scaling-challenged applications that although taking many weeks to complete, cannot scale even to a thousand cores using conventional distributed programming models. This paper describes an experimental methodology, ParalleX, that addresses these challenges through a change in the fundamental model of parallel computation from that of the communicating sequential processes (e.g., MPI) to an innovative synthesis of concepts involving message-driven work-queue execution in the context of a global address space. The focus of this work is a new runtime system required to test, validate, and evaluate the use of ParalleX concepts for extreme scalability. This paper describes the ParalleX model and the HPX runtime system and discusses how both strategies contribute to the goal of extreme computing through dynamic asynchronous execution. The paper presents the first early experimental results of tests using a proof-of-concept runtime-system implementation. These results are very promising and are guiding future work towards a full scale parallel programming and runtime environment.},
address = {Vienna, Austria},
author = {Kaiser, Hartmut and Brodowicz, Maciek and Sterling, Thomas},
booktitle = {2009 International Conference on Parallel Processing Workshops},
doi = {10.1109/ICPPW.2009.14},
file = {:Users/parsa/Downloads/05364511.pdf:pdf},
isbn = {978-1-4244-4923-1},
issn = {1530-2016},
month = {sep},
pages = {394--401},
publisher = {IEEE},
series = {ICPPW '09},
title = {{ParalleX An Advanced Parallel Execution Model for Scaling-Impaired Applications}},
url = {http://ieeexplore.ieee.org/document/5364511/},
year = {2009}
}
@inproceedings{Daily2014,
abstract = {Abstract: Partitioned Global Address Space (PGAS) models are emerging as a popular alternative to MPI models for designing scalable applications. At the same time, MPI remains a ubiquitous communication subsystem due to its standardization, high performance, and availability on leading platforms. In this paper, we explore the suitability of using MPI as a scalable PGAS communication subsystem. We focus on the Remote Memory Access (RMA) communication in PGAS models which typically includes get, put, and atomic memory operations. We perform an in-depth exploration of design alternatives based on MPI. These alternatives include using a semantically-matching interface such as MPI-RMA, as well as not-so-intuitive interfaces such as MPI two-sided with a combination of multi-threading and dynamic process management. With an in-depth exploration of these alternatives and their shortcomings, we propose a novel design which is facilitated by the data-centric view in PGAS models. This design leverages a combination of highly tuned MPI two-sided semantics and an automatic, user-transparent split of MPI communicators to provide asynchronous progress. We implement the asynchronous progress ranks approach and other approaches within the Communication Runtime for Exascale which is a communication subsystem for Global Arrays. Our performance evaluation spans pure communication benchmarks, graph community detection and sparse matrix-vector multiplication kernels, and a computational chemistry application. The utility of our proposed PR-based approach is demonstrated by a 2.17x speedup on 1008 processors over the other MPI-based designs.},
address = {Dona Paula, India},
author = {Daily, Jeff and Vishnu, Abhinav and Palmer, Bruce and van Dam, Hubertus and Kerbyson, Darren},
booktitle = {2014 21st International Conference on High Performance Computing (HiPC)},
doi = {10.1109/HiPC.2014.7116712},
file = {:Users/parsa/Downloads/07116712.pdf:pdf},
isbn = {978-1-4799-5976-1},
month = {dec},
pages = {1--10},
publisher = {IEEE},
title = {{On the suitability of MPI as a PGAS runtime}},
url = {http://ieeexplore.ieee.org/document/7116712/},
year = {2014}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {http://hpc.pnl.gov/comex/},
title = {{ComEx}}
}
@techreport{Bonachea2002,
abstract = {This GASNet specification describes a network-independent and language-independent high-performance com- munication interface intended for use in implementing the runtime system for global address space languages (such as UPC or Titanium). GASNet stands for "Global-Address Space Networking".},
address = {Berkeley, CA, USA},
author = {Bonachea, Dan},
file = {:Users/parsa/Downloads/CSD-02-1207.pdf:pdf},
institution = {University of California at Berkeley},
month = {oct},
pages = {31},
publisher = {University of California at Berkeley},
title = {{GASNet Specification, V1.1}},
url = {http://gasnet.lbl.gov/CSD-02-1207.pdf},
year = {2002}
}
@techreport{Kamil2013,
address = {https://xstackwiki.modelado.org/images/4/43/DEGAS-Progress-Report-2013-03.pdf},
author = {Kamil, Amir and Yelick, Katherine},
institution = {Lawrence Berkeley National Laboratory},
month = {mar},
title = {{The Dynamic Exascale Global Address Space (DEGAS) Project}},
year = {2013}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {http://crd.lbl.gov/departments/computer-science/CLaSS/research/DEGAS/},
title = {{Dynamic Exascale Global Address Space Programming Environments}}
}
@inproceedings{Husbands2003,
abstract = {Unified Parallel C (UPC) is a parallel language that uses a Single Program Multiple Data (SPMD) model of parallelism within a global address space. The global address space is used to simplify programming, especially on applications with irregular data structures that lead to fine-grained sharing between threads. Recent results have shown that the performance of UPC using a commercial compiler is comparable to that of MPI [7]. In this paper we describe a portable open source compiler for UPC. Our goal is to achieve a similar performance while enabling easy porting of the compiler and runtime, and also provide a framework that allows for extensive optimizations. We identify some of the challenges in compiling UPC and use a combination of micro-benchmarks and application kernels to show that our compiler has low overhead for basic operations on shared data and is competitive, and sometimes faster than, the commercial HP compiler. We also investigate several communication optimizations, and show significant benefits by hand-optimizing the generated code.},
address = {New York, New York, USA},
author = {Husbands, Parry and Iancu, Costin and Yelick, Katherine},
booktitle = {Proceedings of the 17th annual international conference on Supercomputing - ICS '03},
doi = {10.1145/782814.782825},
file = {:Users/parsa/Downloads/p63-husbands.pdf:pdf},
isbn = {1-58113-733-8},
pages = {63--73},
publisher = {ACM},
series = {ICS '03},
title = {{A performance analysis of the Berkeley UPC compiler}},
url = {http://portal.acm.org/citation.cfm?doid=782814.782825},
year = {2003}
}
@techreport{Kamil2012,
address = {http://www.cs.berkeley.edu/{\~{}}kamil/papers/nsf-rdppc12.pdf},
author = {Kamil, Amir and Yelick, Katherine},
file = {:Users/parsa/Downloads/06877339.pdf:pdf},
institution = {UC Berkeley/LBNL},
month = {jun},
title = {{Three Challenges and Three Solutions for Exascale Computing}},
url = {http://parlab.eecs.berkeley.edu/sites/all/parlab/files/nsf-2.pdf},
year = {2012}
}
@inproceedings{Zheng2014,
abstract = {Partitioned Global Address Space (PGAS) languages are convenient for expressing algorithms with large, random-access data, and they have proven to provide high performance and scalability through lightweight one-sided communication and locality control. While very convenient for moving data around the system, PGAS languages have taken different views on the model of computation, with the static Single Program Multiple Data (SPMD) model providing the best scalability. In this paper we present UPC++, a PGAS extension for C++ that has three main objectives: 1) to provide an object-oriented PGAS programming model in the context of the popular C++ language, 2) to add useful parallel programming idioms unavailable in UPC, such as asynchronous remote function invocation and multidimensional arrays, to support complex scientific applications, 3) to offer an easy on-ramp to PGAS programming through interoperability with other existing parallel programming systems (e.g., MPI, OpenMP, CUDA). We implement UPC++ with a "compiler-free" approach using C++ templates and runtime libraries. We borrow heavily from previous PGAS languages and describe the design decisions that led to this particular set of language features, providing significantly more expressiveness than UPC with very similar performance characteristics. We evaluate the programmability and performance of UPC++ using five benchmarks on two representative supercomputers, demonstrating that UPC++ can deliver excellent performance at large scale up to 32K cores while offering PGAS productivity features to C++ applications.},
address = {Phoenix, AZ, USA},
author = {Zheng, Yili and Kamil, Amir and Driscoll, Michael B and Shan, Hongzhang and Yelick, Katherine},
booktitle = {2014 IEEE 28th International Parallel and Distributed Processing Symposium},
doi = {10.1109/IPDPS.2014.115},
file = {:Users/parsa/Downloads/06877339.pdf:pdf},
isbn = {978-1-4799-3800-1},
month = {may},
pages = {1105--1114},
publisher = {IEEE},
series = {IPDPS 14},
title = {{UPC++: A PGAS Extension for C++}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6877339},
year = {2014}
}
@misc{hpx5,
annote = {Accessed: 2017-03-30},
howpublished = {http://hpx.crest.iu.edu/},
title = {{HPX-5}},
organization = {{CREST at Indiana University}}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {https://xstack.exascale-tech.com/git/public?p=xstack.git;a=blob;f=ocr/spec/ocr-1.0.0.pdf;hb=HEAD},
title = {{Open Community Runtime Interface Specification v1.0.0 - June 2015}}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {https://01.org/open-community-runtime},
title = {{Open Community Runtime}}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {https://charm.cs.illinois.edu/charmWorkshop/slides/CharmWorkshop2015{\_}tutorial.pdf},
title = {{Charm++ Workshop 2015}}
}
@article{Chamberlain2007,
abstract = {In this paper we consider productivity challenges for parallel programmers and explore ways that parallel language design might help improve end-user productivity. We offer a candidate list of desirable qualities for a parallel programming language, and describe how these qualities are addressed in the design of the Chapel language. In doing so, we provide an overview of Chapel's features and how they help address parallel productivity. We also survey current techniques for parallel programming and describe ways in which we consider them to fall short of our idealized productive programming model.},
author = {Chamberlain, B.L. and Callahan, David and Zima, H.P.},
doi = {10.1177/1094342007078442},
file = {:Users/parsa/Downloads/1094342007078442.pdf:pdf},
issn = {1094-3420},
journal = {International Journal of High Performance Computing Applications},
month = {aug},
number = {3},
pages = {291--312},
publisher = {SAGE Publications},
title = {{Parallel Programmability and the Chapel Language}},
url = {http://hpc.sagepub.com/cgi/doi/10.1177/1094342007078442},
volume = {21},
year = {2007}
}
@misc{chapel_lang,
annote = {Accessed: 2015-05-30},
howpublished = {http://chapel.cray.com/},
title = {{Cascade High Productivity Language - Cray}}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {http://www.etinternational.com/index.php/products/swarmbeta},
title = {{SWift Adaptive Runtime Machine}}
}
@incollection{Saraswat2007,
address = {Berlin, Heidelberg},
author = {Saraswat, Vijay},
booktitle = {Programming Languages and Systems: 5th Asian Symposium, APLAS 2007, Singapore, November 29-December 1, 2007. Proceedings},
doi = {10.1007/978-3-540-76637-7_1},
editor = {Shao, Zhong},
file = {:Users/parsa/Downloads/chp{\%}3A10.1007{\%}2F978-3-540-76637-7{\_}1.pdf:pdf;:Users/parsa/Downloads/bok{\%}3A978-3-540-76637-7.pdf:pdf},
isbn = {978-3-540-76637-7},
pages = {1--1},
publisher = {Springer Berlin Heidelberg},
title = {{X10: Concurrent Programming for Modern Architectures}},
url = {http://link.springer.com/10.1007/978-3-540-76637-7{\_}1},
year = {2007}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {http://x10-lang.org/},
title = {{X10 Language}}
}
@article{Allen2005,
author = {Allen, Eric and Chase, David and Hallett, Joe and Luchangco, Victor and Maessen, Jan-Willem and Ryu, Sukyoung and {Steele Jr}, Guy L and Tobin-Hochstadt, Sam and Dias, Joao and Eastlund, Carl and Others},
journal = {Sun Microsystems},
pages = {140},
title = {{The Fortress language specification}},
volume = {139},
year = {2005}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {https://projectfortress.java.net/},
title = {{Project Fortress}}
}
@inproceedings{Saraswat2010,
author = {Saraswat, Vijay and Almasi, George and Bikshandi, Ganesh and Cascaval, Calin and Cunningham, David and Grove, David and Kodali, Sreedhar and Peshansky, Igor and Tardieu, Olivier},
booktitle = {The First Workshop on Advances in Message Passing},
file = {:Users/parsa/Downloads/amp2010.pdf:pdf},
keywords = {APGAS,PGAS},
mendeley-tags = {APGAS,PGAS},
pages = {1--8},
title = {{The Asynchronous Partitioned Global Address Space Model}},
year = {2010}
}
@inproceedings{Chapman:2010:IOS:2020373.2020375,
abstract = {The OpenSHMEM community would like to announce a new effort to standardize SHMEM, a communications library that uses one-sided communication and utilizes a partitioned global address space. OpenSHMEM is an effort to bring together a variety of SHMEM and SHMEM-like implementations into an open standard using a community-driven model. By creating an open-source specification and reference implementation of OpenSHMEM, there will be a wider availability of a PGAS library model on current and future architectures. In addition, the availability of an OpenSHMEM model will enable the development of performance and validation tools. We propose an OpenSHMEM specification to help tie together a number of divergent implementations of SHMEM that are currently available. To support an existing and growing user community, we will develop the OpenSHMEM web presence, including a community wiki and training material, and face-to-face interaction, including workshops and conference participation.},
address = {New York, New York, USA},
author = {Chapman, Barbara and Curtis, Tony and Pophale, Swaroop and Poole, Stephen and Kuehn, Jeff and Koelbel, Chuck and Smith, Lauren},
booktitle = {Proceedings of the Fourth Conference on Partitioned Global Address Space Programming Model - PGAS '10},
doi = {10.1145/2020373.2020375},
file = {:Users/parsa/Downloads/a2-chapman.pdf:pdf},
isbn = {9781450304610},
keywords = {ACM proceedings,OpenSHMEM,PGAS,SHMEM},
mendeley-tags = {ACM proceedings,OpenSHMEM,PGAS,SHMEM},
pages = {1--3},
publisher = {ACM Press},
series = {PGAS '10},
title = {{Introducing OpenSHMEM: SHMEM for the PGAS community}},
url = {http://dl.acm.org/citation.cfm?doid=2020373.2020375},
year = {2010}
}
@inproceedings{feind1995shared,
abstract = {ABSTRACT: This paper gives an overview of the shared memory access (SHMEM) routines in the Libsma library on CRAY T3D systems. Routines are available which perform synchroniza- tion, data cache tuning, remote memory read and write, and collective operations such as broad- casting and reduction. Specific program examples are provided to help illustrate the usefulness and usability of the SHMEM routines},
address = {Denver, Colorado},
author = {Feind, Karl},
booktitle = {Proc. CUG Spring},
file = {:Users/parsa/Downloads/303{\_}308.PDF:PDF},
month = {mar},
pages = {303--308},
series = {Cray User Group},
title = {{Shared Memory Access (SHMEM) Routines}},
year = {1995}
}
@article{CPE:CPE383,
abstract = {Titanium is a language and system for high-performance parallel scientific computing. Titanium uses Java as its base, thereby leveraging the advantages of that language and allowing us to focus attention on parallel computing issues. The main additions to Java are immutable classes, multidimensional arrays, an explicitly parallel SPMD model of computation with a global address space, and zone-based memory management. We discuss these features and our design approach, and report progress on the development of Titanium, including our current driving application: a three-dimensional adaptive mesh refinement parallel Poisson solver.},
author = {Yelick, Kathy and Semenzato, Luigi and Pike, Geoff and Miyamoto, Carleton and Liblit, Ben and Krishnamurthy, Arvind and Hilfinger, Paul and Graham, Susan and Gay, David and Colella, Phil and Aiken, Alex},
doi = {10.1002/(SICI)1096-9128(199809/11)10:11/13<825::AID-CPE383>3.0.CO;2-H},
file = {:Users/parsa/Downloads/Yelick{\_}et{\_}al-1998-Concurrency{\_}{\_}Practice{\_}and{\_}Experience.pdf:pdf},
issn = {1040-3108},
journal = {Concurrency: Practice and Experience},
month = {sep},
number = {11-13},
pages = {825--836},
title = {{Titanium: a high-performance Java dialect}},
url = {http://doi.wiley.com/10.1002/{\%}28SICI{\%}291096-9128{\%}28199809/11{\%}2910{\%}3A11/13{\%}3C825{\%}3A{\%}3AAID-CPE383{\%}3E3.0.CO{\%}3B2-H},
volume = {10},
year = {1998}
}
@article{Numrich:1998:CFP:289918.289920,
abstract = {Co-Array Fortran, formerly known as F--, is a small extension of Fortran 95 for parallel processing. A Co-Array Fortran program is interpreted as if it were replicated a number of times and all copies were executed asynchronously. Each copy has its own set of data objects and is termed an image. The array syntax of Fortran 95 is extended with additional trailing subscripts in square brackets to give a clear and straightforward representation of any access to data that is spread across images. References without square brackets are to local data, so code that can run independently is uncluttered. Only where there are square brackets, or where there is a procedure call and the procedure contains square brackets, is communication between images involved.There are intrinsic procedures to synchronize images, return the number of images, and return the index of the current image.We introduce the extension; give examples to illustrate how clear, powerful, and flexible it can be; and provide a technical definition.},
author = {Numrich, Robert W and Reid, John},
doi = {10.1145/289918.289920},
file = {:Users/parsa/Downloads/p1-numrich.pdf:pdf},
issn = {10617264},
journal = {ACM SIGPLAN Fortran Forum},
month = {aug},
number = {2},
pages = {1--31},
publisher = {ACM},
title = {{Co-array Fortran for parallel programming}},
url = {http://portal.acm.org/citation.cfm?doid=289918.289920},
volume = {17},
year = {1998}
}
@misc{upc_org,
annote = {Accessed: 2015-05-30},
howpublished = {https://upc-lang.org},
title = {{Unified Parallel C}}
}
@misc{pgasorg,
annote = {Accessed: 2015-05-30},
howpublished = {http://www.pgas.org},
title = {{PGAS - Partitioned Global Address Space Languages}}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {http://www.mpi-forum.org},
title = {{Message Passing Interface Forum}}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {http://www.cs.uoregon.edu/research/tau},
title = {{Tuning and Analysis Utilities}}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {http://icl.cs.utk.edu/papi/},
title = {{Performance Application Programming Interface}}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {http://www.openacc.org},
title = {{OpenACC}}
}
@misc{openmp_org,
annote = {Accessed: 2015-05-30},
file = {:Users/parsa/Downloads/openmp-4.5.pdf:pdf},
howpublished = {http://www.openmp.org/specifications/},
title = {{The OpenMP API specification for parallel programming}}
}
@book{Butenhof:1997:PPT:263953,
author = {Butenhof, David R},
file = {:Users/parsa/Downloads/David R. Butenhof Programming with POSIX threads.pdf:pdf},
isbn = {978-0201633924},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
title = {{Programming with POSIX threads}},
year = {1997}
}
@article{Amarasinghe091exascale,
abstract = {Computer systems anticipated in the 2015 – 2020 timeframe are referred to as Extreme Scale because they will be built using massive multi-core processors with 100's of cores per chip. The largest capability Extreme Scale system is expected to deliver Exascale performance of the order of 1018 operations per second. These systems pose new critical challenges for software in the areas of concurrency, energy efficiency and resiliency. In this paper, we discuss the implications of the concurrency and energy efficiency challenges on future software for Extreme Scale Systems. From an application viewpoint, the concurrency and energy challenges boil down to the ability to express and manage parallelism and locality by exploring a range of strong scaling and new-era weak scaling techniques. For expressing parallelism and locality, the key challenges are the ability to expose all of the intrinsic parallelism and locality in a programming model, while ensuring that this expression of parallelism and locality is portable across a range of systems. For managing parallelism and locality, the OS-related challenges include parallel scalability, spatial partitioning of OS and application functionality, direct hardware access for inter-processor communication, and asynchronous rather than interrupt-driven events, which are accompanied by runtime system challenges for scheduling, synchronization, memory management, communication, performance monitoring, and power management. We conclude by discussing the importance of software-hardware co-design in addressing the fundamental challenges for application enablement on Extreme Scale systems.},
author = {Sarkar, Vivek and Harrod, William and Snavely, Allan E},
doi = {10.1088/1742-6596/180/1/012045},
file = {:Users/parsa/Downloads/Vivek{\_}Sarkar{\_}2009{\_}J.{\_}Phys.{\%}3A{\_}Conf.{\_}Ser.{\_}180{\_}012045.pdf:pdf},
issn = {1742-6596},
journal = {Journal of Physics: Conference Series},
month = {jul},
pages = {012045},
title = {{Software challenges in extreme scale systems}},
url = {http://stacks.iop.org/1742-6596/180/i=1/a=012045?key=crossref.f67cd168995105550454fe5e8f7d8cf2},
volume = {180},
year = {2009}
}
@article{Gustafson:1988:RAL:42411.42415,
address = {New York, NY, USA},
author = {Gustafson, John L},
doi = {10.1145/42411.42415},
file = {:Users/parsa/Downloads/p532-gustafson.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {may},
number = {5},
pages = {532--533},
publisher = {ACM},
title = {{Reevaluating Amdahl's law}},
url = {http://doi.acm.org/10.1145/42411.42415 http://portal.acm.org/citation.cfm?doid=42411.42415},
volume = {31},
year = {1988}
}
@article{Hill:2008:ALM:1449375.1449387,
abstract = {Augmenting Amdahl's law with a corollary for multicore hardware makes it relevant to future generations of chips with multiple processor cores. Obtaining optimal multicore performance will require further research in both extracting more parallelism and making sequential cores faster.},
author = {Hill, Mark D and Marty, Michael R},
doi = {10.1109/MC.2008.209},
file = {:Users/parsa/Downloads/04563876.pdf:pdf},
issn = {0018-9162},
journal = {Computer},
keywords = {Amdahl's law,chip multiprocessors (CMPs),multicore chips},
mendeley-tags = {Amdahl's law,multicore chips,chip multiprocessors (CMPs)},
month = {jul},
number = {7},
pages = {33--38},
publisher = {IEEE Computer Society},
title = {{Amdahl's Law in the Multicore Era}},
url = {http://ieeexplore.ieee.org/document/4563876/},
volume = {41},
year = {2008}
}
@inproceedings{Amdahl:1967:VSP:1465482.1465560,
abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
address = {Atlantic City, New Jersey},
author = {Amdahl, Gene M.},
booktitle = {Proceedings of the April 18-20, 1967, spring joint computer conference on - AFIPS '67 (Spring)},
doi = {10.1145/1465482.1465560},
file = {:Users/parsa/Downloads/p483-amdahl.pdf:pdf},
pages = {483},
publisher = {ACM Press},
series = {AFIPS '67 (Spring)},
title = {{Validity of the single processor approach to achieving large scale computing capabilities}},
url = {http://portal.acm.org/citation.cfm?doid=1465482.1465560},
volume = {30},
year = {1967}
}
@article{Shalf2009,
author = {Shalf, John and Sterling, Thomas and Yelick, Katherine and Iancu, Costin and Kubiatowicz, John},
title = {{Operating Systems For Exascale Computing}},
url = {http://crd.lbl.gov/assets/pubs{\_}presos/CDS/ATG/ExascaleOSIESP.pdf},
year = {2009}
}
@article{Snir2014,
abstract = {We present here a report produced by a workshop on ‘Addressing failures in exascale computing' held in Park City, Utah, 4–11 August 2012. The charter of this workshop was to establish a common taxonomy about resilience across all the levels in a computing system, discuss existing knowledge on resilience across the various hardware and software layers of an exascale system, and build on those results, examining potential solutions from both a hardware and software perspective and focusing on a combined approach. The workshop brought together participants with expertise in applications, system software, and hardware; they came from industry, government, and academia, and their interests ranged from theory to implementation. The combination allowed broad and comprehensive discussions and led to this document, which summarizes and builds on those discussions.},
author = {Snir, Marc and Wisniewski, R. W. and Abraham, J. A. and Adve, S. V. and Bagchi, S. and Balaji, P. and Belak, J. and Bose, P. and Cappello, F. and Carlson, B. and Chien, A. A. and Coteus, P. and DeBardeleben, N. A. and Diniz, P. C. and Engelmann, C. and Erez, M. and Fazzari, S. and Geist, A. and Gupta, R. and Johnson, F. and Krishnamoorthy, S. and Leyffer, S. and Liberty, D. and Mitra, S. and Munson, T. and Schreiber, R. and Stearley, J. and Hensbergen, E. V.},
doi = {10.1177/1094342014522573},
file = {:Users/parsa/Downloads/1094342014522573.pdf:pdf},
issn = {1094-3420},
journal = {International Journal of High Performance Computing Applications},
month = {may},
number = {2},
pages = {129--173},
publisher = {Sage Publications},
title = {{Addressing failures in exascale computing}},
url = {http://www.osti.gov/bridge/servlets/purl/1078029/ http://hpc.sagepub.com/cgi/doi/10.1177/1094342014522573},
volume = {28},
year = {2014}
}
@article{2007JPhCS..78a2022S,
abstract = {With petascale computers only a year or two away there is a pressing need to anticipate and compensate for a probable increase in failure and application interruption rates. Researchers, designers and integrators have available to them far too little detailed information on the failures and interruptions that even smaller terascale computers experience. The information that is available suggests that application interruptions will become far more common in the coming decade, and the largest applications may surrender large fractions of the computer's resources to taking checkpoints and restarting from a checkpoint after an interruption. This paper reviews sources of failure information for compute clusters and storage systems, projects failure rates and the corresponding decrease in application effectiveness, and discusses coping strategies such as application-level checkpoint compression and system level process-pairs fault-tolerance for supercomputing. The need for a public repository for detailed failure and interruption records is particularly concerning, as projections from one architectural family of machines to another are widely disputed. To this end, this paper introduces the Computer Failure Data Repository and issues a call for failure history data to publish in it.},
author = {Schroeder, Bianca and Gibson, Garth A},
doi = {10.1088/1742-6596/78/1/012022},
file = {:Users/parsa/Downloads/jpconf.pdf:pdf},
isbn = {1742-6588$\backslash$r1742-6596},
issn = {1742-6588},
journal = {Journal of Physics: Conference Series},
month = {jul},
number = {1},
pages = {012022},
pmid = {9730340},
title = {{Understanding failures in petascale computers}},
url = {http://stacks.iop.org/1742-6596/78/i=1/a=012022?key=crossref.9709428d4e1fb93e813b531d1115dec2 http://www.ncbi.nlm.nih.gov/pubmed/9730340},
volume = {78},
year = {2007}
}
@article{Cappello2014,
abstract = {Over the past few years resilience has became a major issue for HPC systems, in particular in the perspective of large Petascale systems and future Exascale ones. These systems will typically gather from half a million to several millions of CPU cores running up to a billion of threads. From the current knowledge and observations of existing large systems, it is anticipated that Exascale systems will experience various kind of faults many times per day. It is also anticipated that the current approach for resilience, which relies on automatic or application level checkpoint-restart, will not work because the time for checkpointing and restarting will exceed the mean time to failure of a full system. This set of projections leaves the community of fault tolerance for HPC systems with a difficult challenge: finding new approaches, possibility radically disruptive, to run applications until their normal termination despite the essentially unstable nature of Exascale systems. Yet, the community has only five to six years to solve the problem. This white paper synthesizes the motivations, observations and research issues considered as determinant of several complementary experts of HPC in applications, programming model, distributed system and system management.},
address = {Chelyabinsk},
author = {Cappello, Franck and Geist, Al and Gropp, Bill and Kale, Sanjay and Kramer, Bill and Snir, Marc},
doi = {10.14529/jsfi140101},
file = {:Users/parsa/Downloads/14-676-1-PB.pdf:pdf},
isbn = {1094342009},
issn = {23138734},
journal = {Supercomputing Frontiers and Innovations},
keywords = {challenge,exascale,fault tolerance,high-performance computing,resilience},
month = {sep},
number = {1},
pages = {1--19},
publisher = {South Ural State University},
title = {{Toward Exascale Resilience: 2014 update}},
url = {http://superfri.org/superfri/article/view/14},
volume = {1},
year = {2014}
}
@misc{,
annote = {Accessed: 2015-05-30},
howpublished = {http://www.top500.org/statistics/perfdevel/},
title = {{Top500 supercomputers site}},
url = {http://www.top500.org/lists/2013/11/},
year = {2013}
}
@article{Sterling2009,
abstract = {Cloud computing is emerging as an important computational resource allocation trend in commercial, academic, and industrial sectors. Yet, because the business model doesn't currently meet all the needs of high-performance computing (HPC)-the demands of capability computing, for example-the relationship between clouds and HPC suggests a partly cloudy forecast.},
author = {Sterling, Thomas and Stark, Dylan},
doi = {10.1109/MCSE.2009.111},
file = {:Users/parsa/Downloads/05076318.pdf:pdf},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
keywords = {Availability,Business,Cloud computing,Computer industry,Computer networks,Costs,Delay,Distributed computing,Economies of scale,HPC forecast,Resource management,Web services,cloud computing,computational resource allocation,computer system implementation,computer systems,computer systems organization,emerging technologie,high-performance computing forecast,industrial sector,parallel processing,resource allocation},
mendeley-tags = {Web services,parallel processing,resource allocation,HPC forecast,cloud computing,computational resource allocation,high-performance computing forecast,industrial sector,Availability,Business,Cloud computing,Computer industry,Computer networks,Costs,Delay,Distributed computing,Economies of scale,Resource management,computer system implementation,computer systems,computer systems organization,emerging technologie},
month = {jul},
number = {4},
pages = {42--49},
publisher = {AIP Publishing},
title = {{A High-Performance Computing Forecast: Partly Cloudy}},
url = {http://ieeexplore.ieee.org/document/5076318/},
volume = {11},
year = {2009}
}
@article{Hill1990,
abstract = {Scalability is a frequently-claimed attribute of multiprocessor systems. While the basic notion is intuitive, scalability has no generally-accepted definition. For this reason, current use of the term adds more to marketing potential than technical insight.In this paper, I first examine formal definitions of scalability, but I fail to lind a useful, rigorous definition of it. I then question whether scalability is useful and conclude by challenging the technical community to either (1) rigorously define scalability or (2) stop using it to describe systems.},
author = {Hill, Mark D},
doi = {10.1145/121973.121975},
file = {:Users/parsa/Downloads/p18-hill.pdf:pdf},
isbn = {1219731219},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
keywords = {mulfiprocessor,parallel random access machine,pram,scalability and speedup},
month = {dec},
number = {4},
pages = {18--21},
publisher = {ACM},
title = {{What is scalability?}},
url = {ftp://ftp.cs.wisc.edu/markhill/Papers/can90{\_}scalability.pdf http://portal.acm.org/citation.cfm?doid=121973.121975},
volume = {18},
year = {1990}
}
@article{Fuller2011,
address = {Los Alamitos, CA, USA},
author = {Fuller, Samuel H and Millett, Lynette I},
doi = {10.1109/MC.2011.15},
file = {:Users/parsa/Downloads/05688147.pdf:pdf},
issn = {0018-9162},
journal = {Computer},
month = {jan},
number = {1},
pages = {31--38},
publisher = {IEEE Computer Society},
title = {{Computing Performance: Game Over or Next Level?}},
url = {http://ieeexplore.ieee.org/document/5688147/},
volume = {44},
year = {2011}
}
@phdthesis{Troska2016,
author = {Troska, Lukas},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Troska - 2016 - An HPX-based Parallelization of a Navier-Stokes solver.pdf:pdf},
pages = {88},
school = {Rheinische Friedrich-Wilhelms-Universit{\"{a}}t Bonn},
title = {{An HPX-based Parallelization of a Navier-Stokes solver}},
url = {http://stellar.cct.lsu.edu/pubs/bachelor{\_}thesis{\_}troska{\_}2016.pdf},
year = {2016}
}
@phdthesis{Grubel2016,
abstract = {As parallel computation enters the exascale era where applications may run
on millions to billions of processors concurrently, all aspects of the computational
model need to undergo a transformation to meet the challenges of scaling impaired
applications. One class of models aimed towards exascale computation is
the task-based parallel computational model. Task-based execution models and
their implementations aim to support parallelism through massive multi-threading
where an application is split into numerous tasks of varying size that execute concurrently.
Thread scheduling mechanisms used to manage application level tasks
are a fundamental part of the task-based parallel computational model.

In task-based systems, scheduling threads onto resources can incur large overheads
that vary with the underlying hardware. In this work, our goal is to dynamically
control task grain size to minimize these overheads. We use performance
studies to determine measurable events and metrics derived from them that indicate
how tuning task granularity will improve performance. We aim to build
a closed loop system that measures pertinent events and dynamically tunes task
grain size to improve performance of parallel applications. High Performance
ParalleX (HPX), the first implementation of the ParalleX execution model, is a
runtime system that employs asynchronous fine-grained tasks and asynchronous
communication for improved scaling of parallel applications. HPX is a modular
system that has a dynamic performance modeling capability and a variety of
thread scheduling policies and queuing models for work stealing and load balancing.
It provides the ideal framework for studying parallel applications with
the ability to make dynamic performance measurements and implement adaptive
mechanisms. Therefore, dynamic tuning of task granularity is developed within
the HPX framework.},
author = {Grubel, Patricia A},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Grubel - 2016 - Dynamic Adaptation in HPX - A Task-based Parallel Runtime System.pdf:pdf},
pages = {187},
school = {New Mexico State University},
title = {{Dynamic Adaptation in HPX - A Task-based Parallel Runtime System}},
url = {http://stellar.cct.lsu.edu/pubs/p{\_}grubel{\_}dissertation.pdf},
year = {2016}
}
@phdthesis{Wolf2016,
abstract = {The following thesis first takes a closer look at parallelisation of programs in combination with hardware architecture like multiprocessing and vector instruction sets. Classifications and differences are explained, as well as some popular solutions. The two main software components, ISPC and HPX, will also be introduced and explained here. Afterwards the implementation of a HPX backend for the asynchronous task system interface of ISPC is explained and two different versions developed which later get compared using different test programs like a simple benchmark, a Mandelbrot set and a sobel image processing edge detection filter. Tests and benchmarks show that the HPX task system backend is an equal option to other solutions like pthreads or OpenMP},
author = {Wolf, Julian},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Wolf - 2016 - Implementation of a backend to ISPC using HPX.pdf:pdf},
pages = {26},
school = {Friedrich-Alexander Universitat Erlangen-Nurnberg},
title = {{Implementation of a backend to ISPC using HPX}},
url = {http://stellar.cct.lsu.edu/pubs/Implementation{\_}of{\_}a{\_}backend{\_}to{\_}ISPC{\_}using{\_}HPX.pdf},
year = {2016}
}
@techreport{Guo2014a,
abstract = {This report describes and discusses the process of seeking a work-efficient design for the
parallel prefix scan algorithm that would fit into the HPX (High Performance Parallex) execution
model for C++ users, and the analytic results of the work done during the process. It is part of
the HPX project that is ongoing within the Center of Computation and Technology and the
STEllAR-GROUP, which aims to provide a general purpose C++ runtime system for parallel and
distributed application development of any scale. },
address = {Baton Rouge},
author = {Guo, Chen},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Guo - 2014 - Parallel Prefix Scan in HPX Final Project Report Implementing Asynchronous Prefix Scan Algorithm in HPX Execution Model.pdf:pdf},
institution = {Louisiana State University},
pages = {41},
title = {{Parallel Prefix Scan in HPX Final Project Report Implementing Asynchronous Prefix Scan Algorithm in HPX Execution Model Final Project Report – Parallel Prefix Scan in HPX Final Project Report – Parallel Prefix Scan in HPX}},
url = {http://stellar.cct.lsu.edu/wp-content/uploads/2015/02/Chen{\_}Final-Project-Report{\_}1.1.pdf},
year = {2014}
}
@phdthesis{Dekate11extremescale,
abstract = {Traditional scientific applications such as Computational Fluid Dynamics, Partial Differential Equations based numerical methods (like Finite Difference Methods, Finite Element Methods) achieve sufficient efficiency on state of the art high performance computing systems and have been widely studied / implemented using conventional programming models. For emerging application domains such as Graph applications scalability and efficiency is significantly constrained by the conventional systems and their supporting programming models. Furthermore technology trends like multicore, manycore, heterogeneous system architectures are introducing new challenges and possibilities. Emerging technologies are requiring a rethinking of approaches to more effectively expose the underlying parallelism to the applications and the end-users. This thesis explores the space of effective parallel execution of ephemeral graphs that are dynamically generated. The standard particle based simulation, solved using the Barnes-Hut algorithm is chosen to exemplify the dynamic workloads. In this thesis the workloads are expressed using sequential execution semantics, a conventional parallel programming model - shared memory semantics and semantics of an innovative execution model designed for efficient scalable performance towards Exascale computing called ParalleX. The main outcomes of this research are parallel processing of dynamic ephemeral workloads, enabling dynamic load balancing during runtime, and using advanced semantics for exposing parallelism in scaling constrained applications.},
author = {Dekate, Chirag},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Dekate - 2004 - Extreme Scale Parallel NBody Algorithm With Event Driven Constraint Based Execution Model.pdf:pdf},
pages = {139},
school = {Louisiana State University},
title = {{Extreme Scale Parallel NBody Algorithm With Event Driven Constraint Based Execution Model}},
url = {http://etd.lsu.edu/docs/available/etd-04192011-170837/unrestricted/thesis.pdf},
year = {2004}
}
@article{Anderson2011a,
abstract = {Several applications in astrophysics require adequately resolving many physical and temporal scales which vary over several orders of magnitude. Adaptive mesh refinement techniques address this problem effectively but often result in constrained strong scaling performance. The ParalleX execution model is an experimental execution model that aims to expose new forms of program parallelism and eliminate any global barriers present in a scaling-impaired application such as adaptive mesh refinement. We present two astrophysics applications using the ParalleX execution model: a tabulated equation of state component for neutron star evolutions and a cosmology model evolution. Performance and strong scaling results from both simulations are presented. The tabulated equation of state data are distributed with transparent access over the nodes of the cluster. This allows seamless overlapping of computation with the latencies introduced by the remote access to the table. Because of the expected size increases to the equation of state table, this type of table partitioning for neutron star simulations is essential while the implementation is greatly simplified by ParalleX semantics.},
archivePrefix = {arXiv},
arxivId = {1110.1131},
author = {Anderson, Matthew and Brodowicz, Maciej and Kaiser, Hartmut and Adelstein-Lelbach, Bryce and Sterling, Thomas},
eprint = {1110.1131},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Anderson et al. - Unknown - Adaptive Mesh Refinement for Astrophysics Applications with ParalleX.pdf:pdf},
keywords = {Index Terms—Adaptive mesh refinement,ParalleX,astrophysics appli-cations},
month = {oct},
title = {{Adaptive Mesh Refinement for Astrophysics Applications with ParalleX}},
url = {http://arxiv.org/abs/1110.1131},
year = {2011}
}
@article{2015mgm..conf..995N,
abstract = {The addition of nuclear and neutrino physics to general relativistic fluid codes allows for a more realistic description of hot nuclear matter in neutron star and black hole systems. This additional microphysics requires that each processor have access to large tables of data, such as equations of state, and in large simulations the memory required to store these tables locally can become excessive unless an alternative execution model is used. In this work we present relativistic fluid evolutions of a neutron star obtained using a message driven multi-threaded execution model known as ParalleX. These neutron star simulations would require substantial memory overhead dedicated entirely to the equation of state table if using a more traditional execution model. We introduce a ParalleX component based on Futures for accessing large tables of data, including out-of-core sized tables, which does not require substantial memory overhead and effectively hides any increased network latency.},
archivePrefix = {arXiv},
arxivId = {1205.5055},
author = {Anderson, Matthew and Brodowicz, Maciej and Kaiser, Hartmut and Adelstein-Lelbach, Bryce and Sterling, Thomas},
doi = {10.1142/9789814623995_0064},
eprint = {1205.5055},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Anderson et al. - Unknown - Neutron Star Evolutions using Tabulated Equations of State with a New Execution Model.pdf:pdf},
isbn = {978-981-4612-14-2},
journal = {The Thirteenth Marcel Grossmann Meeting},
keywords = {Fu-tures,HPX,Index Terms—Astrophysics applications,ParalleX},
month = {may},
pages = {995--998},
publisher = {WORLD SCIENTIFIC},
title = {{Neutron Star Evolutions using Tabulated Equations of State with a New Execution Model}},
url = {https://arxiv.org/pdf/1205.5055v1.pdf http://www.worldscientific.com/doi/abs/10.1142/9789814623995{\_}0064 http://arxiv.org/abs/1205.5055},
year = {2012}
}
@techreport{N4167,
abstract = {The goal of this paper is to widen the range of problems that the C++ standards proposal N4071, A Technical Specification for C++ Extensions for Parallelism, Revision 1 may encompass. Offering a greater use case for these algorithms may help further extend the significance of this Technical Specification. This document describes and outlines the problems we have encountered with the current algorithm requirements, as specified in standards proposal N4071. The algorithm described in this document is a solution to a general set of problems that the current algorithms do not support. Transform Reduce is to be considered as an addition to the current set of algorithms in proposal N4071, not a replacement.},
author = {Mercer, Grant and Berg{\'{e}}, Agust{\'{i}}n and Kaiser, Hartmut},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Mercer, Berg{\'{e}}, Kaiser - 2014 - Transform Reduce, an Additional Algorithm for C Extensions for Parallelism.pdf:pdf},
pages = {5},
publisher = {Open Standards Organization},
title = {{Transform Reduce, an Additional Algorithm for C++ Extensions for Parallelism}},
url = {http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4167.pdf},
year = {2014}
}
@techreport{P0234R0,
abstract = {This paper describes a direction for Massive parallelism design that aims to integrate the many domains for C++, rather than only for a few domains, based on the multiple authors' experience with designing massive parallelism within each of those domains and the existing Parallel and Concurrency TSs. This is based on ongoing telecons within SG14/SG1 where we are considering the latest practice and proposals for Heterogeneous device support. The world is moving towards massive parallelism with significant impetus from both ends of the usage spectrum. The High Performance Computation Scientists are driving towards Exascale computing with recent mandates from Department of Energy, while the consumer domain is driving towards it with specific requirements to support heterogeneous devices, drones, medical imaging, and embedded devices used in auto vision relied upon by self­driving cars. In between are the increasingly demanding graphics that is encoded in AAA games, as well as the movement towards VR/AR which require processing of massive amount of data in parallel. For this reason, the massive increase in local parallelism is one of the greatest challenges amongst the many issues imposed by today's and tomorrow's peta­ and exascale systems. At the same time, the efficient utilization of the prospective computer architectures will be challenging in many ways, very much because of a massive increase of on­node parallelism, and an increase of complexity of memory hierarchies. The goal for a future C++ standard should be to enable the seamless exposure and integration of various types of parallelism, such as iterative parallel execution, task­based parallelism, asynchronous execution flows, continuation style computation, and explicit fork­join control flow of independent and non­homogeneous code paths. A central focus should be to ensure full portability ­ in terms of code and performance ­ on a wide spectrum of heterogeneous computer architectures in an integrated manner for C++.},
author = {Wong, Michael and Kaiser, Hartmut and Heller, Thomas},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Wong, Kaiser, Heller - 2015 - Towards Massive Parallelism (aka Heterogeneous DevicesAcceleratorGPGPU) support in C with HPX.pdf:pdf},
pages = {16},
publisher = {Open Standards Organization},
title = {{Towards Massive Parallelism (aka Heterogeneous Devices/Accelerator/GPGPU) support in C++ with HPX}},
url = {http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0234r0.pdf},
year = {2015}
}
@techreport{P0058R1,
author = {Hoberock, Jared and Garland, Michael and Giroux, Olivier},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Hoberock, Garland, Giroux - 2016 - An Interface for Abstracting Execution P0058R1.pdf:pdf},
publisher = {Open Standards Organization},
title = {{An Interface for Abstracting Execution | P0058R1}},
url = {http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0058r1.pdf},
year = {2016}
}
@techreport{P0361R1,
abstract = {This paper describes new execution policies enabling the asynchronous execution of the parallel algorithms as defined by the Parallelism TS (N4507)[1]. This paper is part of an effort to design and propose uniform parallelism APIs in C++ with the goal to make the language independent from any external solutions (such as OpenMP or OpenACC). There have been several discussions in SG1 and SG14 during the recent committee meetings in Kona and Jacksonville expressing interest in enabling asynchronous execution of parallel algorithms. This paper also continues the specific features and needs towards supporting Heterogeneous Devices which was discussed in an evening session at [Jacksonville 2016][2]. In that evening session, Michael Wong presented the motivation to support Heterogeneous devices and how it has been done in OpenMP, and was followed by two C++ specific designs. Hartmut Kaiser presented the HPX design which caters more to a highperformance computing viewpoint. Lee Howes presented the Khronos SYCL/OpenCL design which caters more to a consumer device viewpoint. The discussion that followed, indicated enthusiastic support to move C++ towards full support for Heterogeneous computing by 2020, likely through an initial TS. In general, all parallel algorithms as defined in N4507 are synchronous. This means that the execution of an algorithm returns only after its operation has completely finished. It is well known, that this form of fork/join parallelism imposes an implicit barrier onto the parallel execution flow. This is also currently the case in OpenMP parallel regions. This barrier impedes parallel efficiency and efficient resource utilization of the used processing units as the execution has to wait for the thread of execution which performs the necessary join operation at the end of the execution of the algorithm. The user has no means of controlling how and when this barrier is imposed and also has no means of avoiding the resource starvation associated with it. A possible remedy for this problem is to allow for the algorithms to be executed asynchronously. While this does not remove the implicit barrier at the end of the execution of any of the algorithms, it allows to reduce the resource starvation by allowing to perform other, unrelated tasks while the join-operation (and the associated tapering of parallel work) is being executed. This paper proposes to enable such an asynchronous execution of all algorithms as defined by N4507 by introducing special execution policies which essentially launch the execution of the algorithm on a new thread of execution while the algorithm invocation itself now returns a std::future representing the result of its execution. Returning a Future object from the algorithm has the additional advantage of being able to integrate the parallel algorithms with other asynchronous codes which also rely on representing their results through std::future. This is especially important in light of the proposed additions to std::future as described by the [Concurrency TS (4501)][3]. The proposed extensions have been implemented in [HPX][4] which has an implementation of N4507. They are in use in production codes for some time.},
author = {Kaiser, Hartmut and Heller, Thomas and Lelbach, Bryce Adelstein and Biddiscombe, John and Wong, Michael},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Kaiser, Heller, Wong - Unknown - Invoking Algorithms Asynchronously.pdf:pdf},
pages = {9},
publisher = {Open Standards Organization},
title = {{Invoking Algorithms Asynchronously}},
url = {http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0361r0.pdf},
year = {2016}
}
@inproceedings{5364511,
abstract = {High performance computing (HPC) is experiencing a phase change with the challenges of programming and management of heterogeneous multicore sys-tems architectures and large scale system configu-rations. It is estimated that by the end of the next decade Exaflops computing systems requiring hun-dreds of millions of cores demanding multi-billion-way parallelism with a power budget of 50 Gflops/watt may emerge. At the same time, there are many scaling-challenged applications that al-though taking many weeks to complete, cannot scale even to a thousand cores using conventional distributed programming models. This paper de-scribes an experimental methodology, ParalleX, that addresses these challenges through a change in the fundamental model of parallel computation from that of the communicating sequential processes (e.g., MPI) to an innovative synthesis of concepts involving message-driven work-queue execution in the context of a global address space. The focus of this work is a new runtime system required to test, validate, and evaluate the use of ParalleX concepts for extreme scalability. This paper describes the ParalleX model and the HPX runtime system and discusses how both strategies contribute to the goal of extreme computing through dynamic asynchron-ous execution. The paper presents the first early experimental results of tests using a proof-of-concept runtime-system implementation. These results are very promising and are guiding future work towards a full scale parallel programming and runtime environment. 2. INTRODUCTION An important class of parallel applications is emerging as scaling impaired. These are problems that require substantial execution time, sometimes exceeding a month, but which are unable to make effective use of more than a few hundred processors. One such example is numerical relativity used to model colliding neutron stars to simulate gamma ray bursts (GRB) and simulta-neously identify the gravitational wave signature for detection with such massive instruments as LIGO (Laser Interferometer Gravitational Observatory). These codes exploit the efficiencies of Adaptive Mesh Refinement (AMR) algorithms to concentrate processing effort at the most active parts of the computation space at any one time. However, conventional parallel programming methods using MPI [1] and systems such as distributed memory MPPs and Linux clusters exhibit poor efficiency and constrained scalability, severely limiting scientific advancement. Many other applications exhibit similar properties. To achieve dramatic improvements for such problems and prepare them for exploitation of Peta-flops systems comprising millions of cores, a new execu-tion model and programming methodology is required [2]. This paper briefly presents such a model, ParalleX, and provides early results from an experimental im-plementation of the HPX runtime system that suggests the future promise of such a computing strategy. It is recognized that technology trends have forced high performance system architectures into the new regime of heterogeneous multicore structures. With multicore becoming the new Moore's Law, performance advances even for general commercial applications are requiring parallelism in what was once the domain of purely se-quential computing. In addition, accelerators including, but not limited, to GPUs are being applied for significant performance gains, at least for certain applications where inner loops exhibit numeric intensive operation on relatively small data. Future high end systems will integrate thousands of " nodes " , each comprising many hundreds of cores by means of system area networks. Future applications like AMR algorithms will involve the processing of large time-varying graphs with embedded meta-data. Also of this general class are informatics problems that are of increasing importance for know-ledge management and national security.},
address = {Vienna, Austria},
author = {Kaiser, Hartmut and Brodowicz, Maciek and Sterling, Thomas},
booktitle = {2009 International Conference on Parallel Processing Workshops},
doi = {10.1109/ICPPW.2009.14},
editor = {Barolli, Leonard (FIT) and Feng, Wu-chun (Virginia Tech.)},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Kaiser, Brodowicz, Sterling - Unknown - ParalleX An Advanced Parallel Execution Model for Scaling- Impaired Applications.pdf:pdf},
isbn = {978-1-4244-4923-1},
issn = {0190-3918},
keywords = {Computational modeling,Computer architecture,Concurrent computing,Context modeling,HPC,HPX runtime system,High performance computing,Large-scale systems,Multicore processing,ParalleX model,Parallel processing,Power system management,Power system modeling,advanced parallel execution model,communicating sequential processes,distributed programming models,dynamic asynchronous execution,heterogeneous multicore system architecture,high performance ParalleX runtime system,high performance computing,large scale system configurations,message-driven work-queue execution,microprocessor chips,next decade exaflops computing systems,parallel model of computation,parallel programming,power budget,reconfigurable architectures,runtime system},
mendeley-tags = {communicating sequential processes,microprocessor chips,parallel programming,reconfigurable architectures,HPX runtime system,ParalleX model,advanced parallel execution model,distributed programming models,dynamic asynchronous execution,heterogeneous multicore system architecture,high performance ParalleX runtime system,high performance computing,large scale system configurations,message-driven work-queue execution,next decade exaflops computing systems,power budget,Computational modeling,Computer architecture,Concurrent computing,Context modeling,High performance computing,Large-scale systems,Multicore processing,Parallel processing,Power system management,Power system modeling,HPC,parallel model of computation,runtime system},
month = {sep},
pages = {394--401},
publisher = {IEEE},
series = {ICPPW},
title = {{ParalleX An Advanced Parallel Execution Model for Scaling-Impaired Applications}},
url = {http://stellar.cct.lsu.edu/pubs/icpp09.pdf http://ieeexplore.ieee.org/document/5364511/},
year = {2009}
}
@inproceedings{Huck:2013:EPA:2491661.2481434,
abstract = {Extreme-scale computing requires a new perspective on the role of performance observation in the exascale system soft-ware stack. Because of the anticipated high concurrency and dynamic operation in these systems, it is no longer reason-able to expect that a post-mortem performance measure-ment and analysis methodology will suffice. Rather, there is a strong need for performance observation that merges first-and third-person observation, in situ analysis, and introspec-tion across stack layers that serves online dynamic feedback and adaptation. In this paper we describe the DOE-funded XPRESS project and the role of autonomic performance support in exascale systems. XPRESS will build an inte-grated exascale software stack (called OpenX) that supports the ParalleX execution model and is targeted towards future exascale platforms. An initial version of an autonomic per-formance environment called APEX has been developed for OpenX using the current TAU performance technology and results are presented that highlight the challenges of highly integrative observation and runtime analysis.},
address = {Eugene, Oregon},
author = {Huck, Kevin and Shende, Sameer and Malony, Allen and Kaiser, Hartmut and Porterfield, Allan and Fowler, Rob and Brightwell, Ron},
booktitle = {Proceedings of the 3rd International Workshop on Runtime and Operating Systems for Supercomputers - ROSS '13},
doi = {10.1145/2491661.2481434},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Huck et al. - Unknown - An Early Prototype of an Autonomic Performance Environment for Exascale.pdf:pdf},
isbn = {9781450321464},
pages = {1},
publisher = {ACM Press},
title = {{An early prototype of an autonomic performance environment for exascale}},
url = {http://stellar.cct.lsu.edu/pubs/ross2013.pdf http://dl.acm.org/citation.cfm?doid=2491661.2481434},
year = {2013}
}
@inproceedings{6785457,
abstract = {The continuing technological progress resulted in sustained increase in the number of transistors per chip as well as improved energy efficiency per FLOPS. This spurred a dramatic growth in aggregate computational performance of the largest supercomputing systems, yielding multiple Petascale implementations deployed in various locations over the world. Unfortunately, these advances did not translate to the required extent into accompanying I/O systems, which primarily saw the improvement in cumulative storage sizes required to match the ever expanding volume of scientific data sets, but little more in terms of architecture or effective access latency. Moreover, while new models of computations are formulated to handle the burden of efficiently structuring the parallel computations in anticipation of the arrival of Exascale systems, a meager progress is observed in the area of storage subsystems. New classes of algorithms developed for massively parallel applications, that gracefully handle the challenges of asynchrony, heavily multithreaded distributed codes, and message-driven computation, must be matched by similar advances in I/O methods and algorithms to produce a well performing and balanced supercomputing system. This paper discusses PXFS, a file system model for persistent objects inspired by the ParalleX model of execution that addresses many of these challenges. An early implementation of PXFS utilizing a well known Orange parallel file system as its back-end via asynchronous I/O layer is also described along with the preliminary performance data. The results show perfect scalability and 3× to 20× times speedup of I/O throughput performance comparing to OrangeFS user interface. Also the PXFS module on OrangeFS with 24 clients sees a 5× to 10× times more throughput than NFS.},
address = {Honolulu, HI, USA},
author = {{Shuangyang Yang} and Brodowicz, Maciej and Ligon, Walter B. and Kaiser, Hartmut},
booktitle = {2014 International Conference on Computing, Networking and Communications (ICNC)},
doi = {10.1109/ICCNC.2014.6785457},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - Unknown - PXFS A Persistent Storage Model for Extreme Scale.pdf:pdf},
isbn = {978-1-4799-2358-8},
keywords = {Computational modeling,Computer architecture,Educational institutions,FLOPS,Orange parallel file system,OrangeFS user interface,PXFS module,ParalleX model,Runtime,Scalability,Servers,Throughput,aggregate computational performance,balanced supercomputing system,continuing technological progress,dramatic growth,electronic engineering computing,energy efficiency,exascale systems,file system model,message-driven computation,multithreaded distributed codes,parallel applications,parallel computations,parallel machines,persistent storage model,scientific data sets,storage subsystems,user interfaces},
mendeley-tags = {electronic engineering computing,parallel machines,user interfaces,FLOPS,Orange parallel file system,OrangeFS user interface,PXFS module,ParalleX model,aggregate computational performance,balanced supercomputing system,continuing technological progress,dramatic growth,energy efficiency,exascale systems,file system model,message-driven computation,multithreaded distributed codes,parallel applications,parallel computations,persistent storage model,scientific data sets,storage subsystems,Computational modeling,Computer architecture,Educational institutions,Runtime,Scalability,Servers,Throughput},
month = {feb},
pages = {900--906},
publisher = {IEEE},
title = {{PXFS: A persistent storage model for extreme Scale}},
url = {http://stellar.cct.lsu.edu/pubs/yang{\_}ICNC2014.pdf http://ieeexplore.ieee.org/document/6785457/},
year = {2014}
}
@article{TranTan2016,
abstract = {Providing high level tools for parallel programming while sustaining a high level of performance has been a challenge that techniques like Domain Specific Embedded Languages try to solve. In previous works, we investi-gated the design of such a DSEL – NT 2 – providing a Matlab -like syntax for parallel numerical computations inside a C++ library. In this paper, we show how NT 2 has been redesigned for shared memory systems in an extensible and portable way. The new NT 2 design relies on a tiered Parallel Skeleton system built using asynchronous task management and automatic compile-time task-ification of user level code. We describe how this system can operate various shared memory runtimes and evaluate the design by using several benchmarks implementing linear algebra algorithms.},
author = {{Tran Tan}, Antoine and Falcou, Joel and Etiemble, Daniel and Kaiser, Hartmut},
doi = {10.1007/s10766-015-0354-9},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Tran Tan et al. - 2014 - Automatic Task-based Code Generation for High Performance Domain Specific Embedded Language.pdf:pdf},
issn = {0885-7458},
journal = {International Journal of Parallel Programming},
keywords = {C++,asynchronous programming,generative programming,parallel skeletons},
month = {jun},
number = {3},
pages = {449--465},
publisher = {Springer},
title = {{Automatic Task-Based Code Generation for High Performance Domain Specific Embedded Language}},
url = {http://stellar.cct.lsu.edu/pubs/dsl{\_}paper.pdf http://link.springer.com/10.1007/s10766-015-0354-9},
volume = {44},
year = {2016}
}
@inproceedings{7424766,
abstract = {Many HPC applications developed over the past two decades have used Fortran and MPI-based parallelization. As the size of today's HPC resources continues to increase, these codes struggle to efficiently utilize the million-way parallelism of these platforms. Rewriting these codes from scratch to leverage modern programming paradigms would be time-consuming and error-prone. We evaluate a robust approach for interfacing with next-generation C++-based libraries and drivers. We have successfully used this technique to modify the Fortran code DGSWEM (Discontinuous Galerkin Shallow Water Equation Model), allowing it to take advantage of the new parallel runtime system HPX. Our goal was to make as few modifications to the DGSWEM Fortran source code as possible, thereby minimizing the chances of introducing bugs and reducing the amount of re-verification that needed to be done.},
address = {Sapporo, Japan},
author = {Byerly, Zachary D and Kaiser, Hartmut and Brus, Steven and Schafer, Andreas},
booktitle = {2015 Third International Symposium on Computing and Networking (CANDAR)},
doi = {10.1109/CANDAR.2015.71},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Byerly et al. - Unknown - A Non-intrusive Technique for Interfacing Legacy Fortran Codes with Modern C Runtime Systems.pdf:pdf},
isbn = {978-1-4673-9797-1},
keywords = {Algorithms,Arrays,C++,C++ language,C++ runtime systems,C++-based libraries,DGSWEM,FORTRAN,Fortran,HPC applications,HPX,Legacy Applications,Libraries,Mathematical model,Migration,Reactive power,Runtime,device drivers,discontinuous Galerkin shallow water equation mode,drivers,legacy Fortran codes,parallel processing,parallel runtime system,software libraries,software maintenance,source code,source code (software)},
mendeley-tags = {C++ language,FORTRAN,device drivers,parallel processing,software libraries,software maintenance,source code (software),C++ runtime systems,C++-based libraries,DGSWEM,HPC applications,HPX,discontinuous Galerkin shallow water equation mode,drivers,legacy Fortran codes,parallel runtime system,source code,Algorithms,Arrays,Libraries,Mathematical model,Reactive power,Runtime,C++,Fortran,Legacy Applications,Migration},
month = {dec},
pages = {503--507},
publisher = {IEEE},
series = {CANDAR},
title = {{A Non-intrusive Technique for Interfacing Legacy Fortran Codes with Modern C++ Runtime Systems}},
url = {http://stellar.cct.lsu.edu/pubs/lham2015{\_}byerly.pdf https://doi.org/10.1109/CANDAR.2015.71 http://ieeexplore.ieee.org/document/7424766/},
year = {2015}
}
@inproceedings{Kaiser:2015:HPL:2832241.2832244,
abstract = {One of the biggest challenges on the way to exascale computing is pro-grammability in the context of performance portability. The efficient uti-lization of the prospective architectures of exascale supercomputers will be challenging in many ways, very much because of a massive increase of on-node parallelism, and an increase of complexity of memory hierarchies. Parallel programming models need to be able to formulate algorithms that allow exploiting these architectural peculiarities. The recent revival of inter-est in the industry and wider community for the C++ language has spurred a remarkable amount of standardization proposals and technical specifica-tions. Among those efforts is the development of seamlessly integrating various types of parallelism, such as iterative parallel execution, task-based parallelism, asynchronous execution flows, continuation style computation, and explicit fork-join control flow of independent and non-homogeneous code paths. Those proposals are the foundation of a powerful high-level ab-straction that allows C++ codes to deal with an ever increasing architectural complexity in recent hardware developments. In this paper, we present the results of developing those higher level par-allelization facilities in HPX, a general purpose C++ runtime system for applications of any scale. The developed higher-level parallelization APIs have been designed to overcome the limitations of today's prevalently used programming models in C++ codes. HPX exposes a uniform higher-level API which gives the application programmer syntactic and semantic equiv-alence of various types of on-node and off-node parallelism, all of which are well integrated into the C++ type system. We show that these higher level facilities which are fully aligned with modern C++ programming concepts, are easily extensible, fully generic, and enable highly efficient paralleliza-tion on par with or better than existing equivalent applications based on OpenMP and/or MPI.},
address = {New York, New York, USA},
author = {Kaiser, Hartmut and Heller, Thomas and Bourgeois, Daniel and Fey, Dietmar},
booktitle = {Proceedings of the First International Workshop on Extreme Scale Programming Models and Middleware - ESPM '15},
doi = {10.1145/2832241.2832244},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Kaiser et al. - 2015 - Higher-level Parallelization for Local and Distributed Asynchronous Task-Based Programming.pdf:pdf},
isbn = {9781450339964},
keywords = {HPX,distributed asynchronous computing,task-based parallelism},
mendeley-tags = {HPX,distributed asynchronous computing,task-based parallelism},
pages = {29--37},
publisher = {ACM Press},
series = {ESPM '15},
title = {{Higher-level parallelization for local and distributed asynchronous task-based programming}},
url = {http://dx.doi.org/10.1145/2832241.2832244 http://doi.acm.org/10.1145/2832241.2832244 http://dl.acm.org/citation.cfm?doid=2832241.2832244},
year = {2015}
}
@inproceedings{7530070,
abstract = {The ability to measure performance characteristics of an application at runtime is essential for monitoring the behavior of the application and the runtime system on the underlying architecture. Traditional performance measurement tools do not adequately provide measurements of asynchronous task-based parallel applications, either in real-time or for post-mortem analysis. We propose that this capability is best performed directly by the runtime system for ease in use and to minimize conflicts and overheads potentially caused by traditional measurement tools. In this paper, we describe and illustrate the use of the performance monitoring capabilities in the HPX runtime system. We describe and detail existing performance counters made available through HPX's performance counter framework and demonstrate how they are useful to understanding application efficiency and resource usage at runtime. This extensive framework provides the ability to asynchronously query software and hardware counters and could potentially be used as the basis for runtime adaptive resource decisions. We demonstrate the ease of porting the Inncabs benchmark suite to the HPX runtime system, the improved performance of benchmarks that employ fine-grained task parallelism when ported to HPX, and the capabilities and advantages of using thein-situ performance monitoring system in HPX to give detailed insight to the performance and behavior of the benchmarks and the runtime system.},
author = {Grubel, Patricia and Kaiser, Hartmut and Huck, Kevin and Cook, Jeanine},
booktitle = {2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
doi = {10.1109/IPDPSW.2016.115},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Grubel et al. - Unknown - Using Intrinsic Performance Counters to Assess Efficiency in Task-based Parallel Applications.pdf:pdf},
isbn = {978-1-5090-3682-0},
keywords = {Benchmark testing,HPX,HPX performance counter,HPX runtime system,Index Terms—runtime instrumentation,Inncabs benchmark,Libraries,Monitoring,Parallel processing,Radiation detectors,Runtime,Standards,application efficiency,asynchronous task-based parallel applications,execution monitoring,hardware counters,many asyn-chronous tasks,many asynchronous tasks,parallel processing,performance counters,performance measurement tools,performance monitoring capabilities,performance monitoring system,resource allocation,resource usage,runtime adaptive resource decisions,runtime instrumentation,software counters,system monitoring,task based runtime system,task parallelism,task-based parallelism},
mendeley-tags = {parallel processing,resource allocation,system monitoring,HPX performance counter,HPX runtime system,Inncabs benchmark,application efficiency,asynchronous task-based parallel applications,hardware counters,performance measurement tools,performance monitoring capabilities,performance monitoring system,resource usage,runtime adaptive resource decisions,software counters,task based runtime system,task parallelism,Benchmark testing,Libraries,Monitoring,Parallel processing,Radiation detectors,Runtime,Standards,HPX,execution monitoring,many asynchronous tasks,performance counters,runtime instrumentation,task-based parallelism},
month = {may},
pages = {1692--1701},
publisher = {IEEE},
series = {IPDPSW},
title = {{Using Intrinsic Performance Counters to Assess Efficiency in Task-Based Parallel Applications}},
url = {http://stellar.cct.lsu.edu/pubs/hpcmaspa2016.pdf http://ieeexplore.ieee.org/document/7530070/},
year = {2016}
}
@inproceedings{Tan:2016:ECC:2935323.2935332,
abstract = {The current trend of large scientific computing problems is to align as much as possible to a Single Programming Multiple Data (or SPMD) scheme when the application algorithms are conducive to parallelization and vectorization. This reduces the complexity of code because the processors or (computational nodes) perform the same instructions which allows for better performance as algorithms work on local data sets instead of continuously transferring data from one locality to another. However, certain applications, such as stencil problems, demonstrate the need to move data to or from remote localities. This involves an additional degree of complexity, as one must know with which localities to exchange data. In order to solve this issue, Fortran has extended its scalar element indexing approach to distributed structures of elements. In this extension, a structure of scalar elements is attributed a ”co-index” and lives in a specific locality. A co-index provides the application with enough information to retrieve the corresponding data reference. In C++, containers present themselves as a ”smarter” alternative of Fortran arrays but there are still no corresponding standardized features similar to the Fortran co-indexing approach. In this paper, we present an implementation of such features in HPX, a general purpose C++ runtime system for applications of any scale. We describe how the combination of the HPX features and the actual C++ Standard makes it easy to define a high performance API similar to Co-Array Fortran.},
address = {New York, New York, USA},
author = {Tan, Antoine Tran and Kaiser, Hartmut},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming - ARRAY 2016},
doi = {10.1145/2935323.2935332},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Tran Tan, Kaiser - Unknown - Extending C with Co-Array Semantics.pdf:pdf},
isbn = {9781450343848},
keywords = {API,Co-array,PGAS,Performance,Standardization Keywords C++,distributed containers},
pages = {63--68},
publisher = {ACM Press},
series = {ARRAY 2016},
title = {{Extending C++ with co-array semantics}},
url = {http://stellar.cct.lsu.edu/pubs/extending{\_}cpp{\_}with{\_}coarray{\_}semantics{\_}2016.pdf http://doi.acm.org/10.1145/2935323.2935332 http://dl.acm.org/citation.cfm?doid=2935323.2935332},
year = {2016}
}
@inproceedings{Khatami2016,
abstract = {Computer scientists and programmers face the difficultly of improving the scalability of their applications while using conventional programming techniques only. As a base-line hypothesis of this paper we assume that an advanced runtime system can be used to take full advantage of the available parallel resources of a machine in order to achieve the highest parallelism possible. In this paper we present the capabilities of HPX -- a distributed runtime system for parallel applications of any scale -- to achieve the best possible scalability through asynchronous task execution [1]. OP2 is an active library which provides a framework for the parallel execution for unstructured grid applications on different multi-core/many-core hardware architectures [2]. OP2 generates code which uses OpenMP for loop parallelization within an application code for both single-threaded and multi-threaded machines. In this work we modify the OP2 code generator to target HPX instead of OpenMP, i.e. port the parallel simulation backend of OP2 to utilize HPX. We compare the performance results of the different parallelization methods using HPX and OpenMP for loop parallelization within the Airfoil application. The results of strong scaling and weak scaling tests for the Airfoil application on one node with up to 32 threads are presented. Using HPX for parallelization of OP2 gives an improvement in performance by 5{\%}-21{\%}. By modifying the OP2 code generator to use HPX's parallel algorithms, we observe scaling improvements by about 5{\%} as compared to OpenMP. To fully exploit the potential of HPX, we adapted the OP2 API to expose a future and dataflow based programming model and applied this technique for parallelizing the same Airfoil application. We show that the dataflow oriented programming model, which automatically creates an execution tree representing the algorithmic data dependencies of our application, improves the overall scaling results by about 21{\%} compared to OpenMP. Our results show the advantage of using the asynchronous programming model implemented by HPX.},
address = {Philadelphia, PA, USA},
author = {Khatami, Zahra and Kaiser, Hartmut and Ramanujam, J},
booktitle = {2016 45th International Conference on Parallel Processing Workshops (ICPPW)},
doi = {10.1109/ICPPW.2016.39},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Khatami, Kaiser, Ramanujam - Unknown - Using HPX and OP2 for Improving Parallel Scaling Performance of Unstructured Grid Applications.pdf:pdf},
isbn = {978-1-5090-2825-2},
issn = {2332-5690},
keywords = {Asynchronous Task Execution,Automotive components,HPX,Hardware,High Performance Computing,Instruction sets,OP2,Parallel processing,Programming,Runtime,Scalability},
month = {aug},
pages = {190--199},
publisher = {IEEE},
series = {ICPPW},
title = {{Using HPX and OP2 for Improving Parallel Scaling Performance of Unstructured Grid Applications}},
url = {http://doi.ieeecomputersociety.org/10.1109/ICPPW.2016.39 http://dx.doi.org/10.1109/ICPPW.2016.39 http://ieeexplore.ieee.org/document/7576468/},
year = {2016}
}
@incollection{Heller2016,
abstract = {On the way to Exascale, programmers face the increasing challenge of having to support multiple hardware architectures from the same code base. At the same time, portability of code and performance are increasingly difficult to achieve as hardware architectures are becom-ing more and more diverse. Today's heterogeneous systems often include two or more completely distinct and incompatible hardware execution models, such as GPGPU's, SIMD vector units, and general purpose cores which conventionally have to be programmed using separate tool chains representing non-overlapping programming models. The recent revival of interest in the industry and the wider community for the C++ lan-guage has spurred a remarkable amount of standardization proposals and technical specifications in the arena of concurrency and parallelism. This recently includes an increasing amount of discussion around the need for a uniform, higher-level abstraction and programming model for par-allelism in the C++ standard targeting heterogeneous and distributed computing. Such an abstraction should perfectly blend with existing, already standardized language and library features, but should also be generic enough to support future hardware developments. In this paper, we present the results from developing such a higher-level programming abstraction for parallelism in C++ which aims at enabling code and per-formance portability over a wide range of architectures and for various types of parallelism. We present and compare performance data obtained from running the well-known STREAM benchmark ported to our higher level C++ abstraction with the corresponding results from running it natively. We show that our abstractions enable performance at least as good as the comparable base-line benchmarks while providing a uniform programming API on all compared target architectures.},
address = {Cham},
author = {Heller, Thomas and Kaiser, Hartmut and Diehl, Patrick and Fey, Dietmar and Schweitzer, Marc Alexander},
booktitle = {High Performance Computing: ISC High Performance 2016 International Workshops, ExaComm, E-MuCoCoS, HPC-IODC, IXPUG, IWOPH, P{\^{}}3MA, VHPC, WOPSSS, Frankfurt, Germany, June 19--23, 2016, Revised Selected Papers},
doi = {10.1007/978-3-319-46079-6_2},
editor = {Taufer, Michela and Mohr, Bernd and Kunkel, Julian M},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Heller et al. - 2016 - Closing the Performance Gap with Modern C.pdf:pdf},
isbn = {978-3-319-46079-6},
pages = {18--31},
publisher = {Springer International Publishing},
title = {{Closing the Performance Gap with Modern C++}},
url = {http://dx.doi.org/10.1007/978-3-319-46079-6{\_}2 http://link.springer.com/chapter/10.1007/978-3-319-46079-6{\_}2 http://link.springer.com/10.1007/978-3-319-46079-6{\_}2},
year = {2016}
}
@inproceedings{Khatami:2016:MPD:3019094.3019102,
abstract = {One of the major challenges in parallelization is the difficulty of improving application scalability with conventional techniques. HPX provides efficient scalable parallelism by sig-nificantly reducing node starvation and effective latencies while controlling the overheads. In this paper, we present a new highly scalable parallel distributed N-Body application using a future-based algorithm, which is implemented with HPX. The main difference between this algorithm and prior art is that a future-based request buffer is used between different nodes and along each spatial direction to send/receive data to/from the remote nodes, which helps removing synchronization barriers. HPX provides an asynchronous programming model which results in improving the parallel performance. The results of using HPX for parallelizing Octree construction on one node and the force computation on the distributed nodes show the scalability improvement on an average by about 45{\%} compared to an equivalent OpenMP implementation and 28{\%} compared to a hybrid implementation (MPI+OpenMP) [1] respectively for one billion particles running on up to 128 nodes with 20 cores per each.},
address = {Salt Lake City, Utah},
author = {Khatami, Zahra and Kaiser, Hartmut and Grubel, Patricia and Serio, Adrian and Ramanujam, J},
booktitle = {Proceedings of the 7th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (ScalA '16)},
doi = {10.1109/ScalA.2016.12},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Khatami et al. - Unknown - A Massively Parallel Distributed N-Body Application Implemented with HPX.pdf:pdf},
isbn = {978-1-5090-5222-6},
keywords = {Asynchronous Task Ex-ecution,HPX,High Performance Computing,Index Terms—HPX,N-Body Application,N-body application,Parallel Runtime Systems,asynchronous task execution,high performance computing,parallel runtime systems},
mendeley-tags = {HPX,N-body application,asynchronous task execution,high performance computing,parallel runtime systems},
pages = {57--64},
publisher = {IEEE Press},
series = {ScalA '16},
title = {{A Massively Parallel Distributed N-Body Application Implemented with HPX}},
url = {http://dl.acm.org/citation.cfm?id=3019102},
year = {2016}
}
@article{doi:10.1177/1094342012440585,
abstract = {The scalability and efficiency of graph applications are significantly constrained by conventional systems and their supporting programming models. Technology trends like multicore, manycore, and heterogeneous system architectures are introducing further challenges and possibilities for emerging application domains such as graph applications. This paper explores the space of effective parallel execution of ephemeral graphs that are dynamically generated using the Barnes-Hut algorithm to exemplify dynamic workloads. The workloads are expressed using the semantics of an Exascale computing execution model called ParalleX. For comparison, results using conventional execution model semantics are also presented. We find improved load balancing during runtime and automatic parallelism discovery improving efficiency using the advanced semantics for Exascale computing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1109.5190v1},
author = {Dekate, Chirag and Anderson, Matthew and Brodowicz, Maciej and Kaiser, Hartmut and Adelstein-Lelbach, Bryce and Sterling, Thomas},
doi = {10.1177/1094342012440585},
eprint = {arXiv:1109.5190v1},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Dekate et al. - 2011 - Improving the scalability of parallel N-body applications with an event driven constraint based execution model.pdf:pdf},
issn = {1094-3420},
journal = {International Journal of High Performance Computing Applications},
keywords = {()},
month = {aug},
number = {3},
pages = {319--332},
publisher = {SAGE Publications},
title = {{Improving the scalability of parallel N-body applications with an event-driven constraint-based execution model}},
url = {https://arxiv.org/pdf/1109.5190v1.pdf http://dx.doi.org/10.1177{\%}2F1094342012440585 http://hpc.sagepub.com/cgi/doi/10.1177/1094342012440585},
volume = {26},
year = {2012}
}
@article{0067-0049-212-2-23,
abstract = {We test a new "hybrid" scheme for simulating dynamical fluid flows in which cylindrical components of the momentum are advected across a rotating Cartesian coordinate mesh. This hybrid scheme allows us to conserve angular momentum to machine precision while capitalizing on the advantages offered by a Cartesian mesh, such as a straightforward implementation of mesh refinement. Our test focuses on measuring the real and imaginary parts of the eigenfrequency of unstable axisymmetric modes that naturally arise in massless polytropic tori having a range of different aspect ratios, and quantifying the uncertainty in these measurements. Our measured eigenfrequencies show good agreement with the results obtained from the linear stability analysis of Kojima (1986) and from nonlinear hydrodynamic simulations performed on a cylindrical coordinate mesh by Woodward et al. (1994). When compared against results conducted with a traditional Cartesian advection scheme, the hybrid scheme achieves qualitative convergence at the same or, in some cases, much lower grid resolutions and conserves angular momentum to a much higher degree of precision. As a result, this hybrid scheme is much better suited for simulating astrophysical fluid flows, such as accretion disks and mass-transferring binary systems.},
archivePrefix = {arXiv},
arxivId = {1404.5942},
author = {Byerly, Zachary D and Adelstein-Lelbach, Bryce and Tohline, Joel E and Marcello, Dominic C},
doi = {10.1088/0067-0049/212/2/23},
eprint = {1404.5942},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Byerly et al. - Unknown - A Hybrid Advection Scheme for Conserving Angular Momentum on a Refined Cartesian Mesh.pdf:pdf},
issn = {0067-0049},
journal = {The Astrophysical Journal Supplement Series},
month = {apr},
number = {2},
pages = {23},
title = {{A Hybrid Advection Scheme for Conserving Angular Momentum on a Refined Cartesian Mesh}},
url = {http://stacks.iop.org/0067-0049/212/i=2/a=23 https://arxiv.org/pdf/1404.5942v1.pdf http://stacks.iop.org/0067-0049/212/i=2/a=23?key=crossref.8705f58caf070ce7be9c6836992d42b4 http://arxiv.org/abs/1404.5942 http://dx.doi.org/10.1088/0067-0049/212/2/23},
volume = {212},
year = {2014}
}
@article{JSFI64,
abstract = {Exascale systems will require new approaches to performance observation, analysis, and runtime decision-making to optimize for performance and efficiency. The standard "first-person" model, in which multiple operating system processes and threads observe themselves and record first-person performance profiles or traces for offline analysis, is not adequate to observe and capture interactions at shared resources in highly concurrent, dynamic systems. Further, it does not support mechanisms for runtime adaptation. Our approach, called APEX (Autonomic Performance Environment for eXascale), provides mechanisms for sharing information among the layers of the software stack, including hardware, operating and runtime systems, and application code, both new and legacy. The performance measurement components share information across layers, merging first-person data sets with information collected by third-person tools observing shared hardware and software states at node- and global-levels. Critically, APEX provides a policy engine designed to guide runtime adaptation mechanisms to make algorithmic changes, re-allocate resources, or change scheduling rules when appropriate conditions occur.},
author = {Huck, Kevin A. and Porterfield, Allan and Chaimov, Nick and Kaiser, Hartmut and Malony, Allen D. and Sterling, Thomas and Fowler, Rob},
doi = {10.14529/jsfi150305},
file = {:Users/parsa/Library/Application Support/Mendeley Desktop/Downloaded/Huck et al. - 2015 - An Autonomic Performance Environment for Exascale.pdf:pdf},
issn = {23138734},
journal = {Supercomputing Frontiers and Innovations},
month = {jul},
number = {3},
pages = {49--66},
title = {{An Autonomic Performance Environment for Exascale}},
url = {http://superfri.org/superfri/article/view/64 http://dx.doi.org/10.14529/jsfi150305},
volume = {2},
year = {2015}
}

@webpage{octo_repo,
Url = {OctoTiger on Github}
}
@misc{octotiger_repo,
annote = {Accessed: 2017-09-07},
howpublished = {https://github.com/STEllAR-GROUP/octotiger},
title = {{OctoTiger on Github}}
}
@misc{hpx_repo,
annote = {Accessed: 2017-09-07},
howpublished = {https://github.com/STEllAR-GROUP/hpx},
title = {{HPX on Github}}
}
@misc{kokkos_repo,
annote = {Accessed: 2017-09-07},
howpublished = {https://github.com/kokkos/kokkos},
title = {{Kokkos on Github}}
}
@article{kokkos_paper,
title = "Kokkos: Enabling manycore performance portability through polymorphic memory access patterns ",
journal = "Journal of Parallel and Distributed Computing ",
volume = "74",
number = "12",
pages = "3202 - 3216",
year = "2014",
note = "Domain-Specific Languages and High-Level Frameworks for High-Performance Computing ",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2014.07.003",
url = "http://www.sciencedirect.com/science/article/pii/S0743731514001257",
author = "H. Carter Edwards and Christian R. Trott and Daniel Sunderland"
}
@inproceedings{grubel2015performance,
  title={The performance implication of task size for applications on the hpx runtime system},
  author={Grubel, Patricia and Kaiser, Hartmut and Cook, Jeanine and Serio, Adrian},
  booktitle={Cluster Computing (CLUSTER), 2015 IEEE International Conference on},
  pages={682--689},
  year={2015},
  organization={IEEE}
}
@misc{hartmut_kaiser_2015_33656,
  author       = {Hartmut Kaiser and
                  Bryce Adelstein-Lelbach and
                  Thomas Heller and
                  Agustín Bergé and
                  Anton Bikineev and
                  Grant Mercer and
                  Jeroen Habraken and
                  Matthew Anderson and
                  Adrian Serio and
                  John Biddiscombe and
                  Martin Stumpf and
                  Andreas Schäfer and
                  Steven R. Brandt and
                  Daniel Bourgeois and
                  Patricia Grubel and
                  Vinay Amatya and
                  Kevin Huck and
                  Devang Bacharwar and
                  Lars Viklund and
                  Shuangyang Yang and
                  Erik Schnetter and
                  Bcorde5 and
                  Maciej Brodowicz and
                  Zach Byerly and
                  bghimire and
                  Pyry Jahkola and
                  Christopher Bross and
                  andreasbuhr and
                  Chen Guo and
                  atrantan},
  title        = {{hpx: HPX V0.9.11: A general purpose C++ runtime 
                   system for parallel and distributed applications
                   of any scale}},
  month        = nov,
  year         = 2015,
  doi          = {10.5281/zenodo.33656},
  url          = {https://doi.org/10.5281/zenodo.33656}
}
@inproceedings{kadam2017numerical,
  title={Numerical Simulations of Close and Contact Binary Systems Having Bipolytropic Equation of State},
  author={Kadam, Kundan and Clayton, Geoffrey C and Motl, Patrick M and Marcello, Dominic and Frank, Juhan},
  booktitle={American Astronomical Society Meeting Abstracts},
  volume={229},
  year={2017}
}
@inproceedings{wagle2018methodology,
  title={Methodology for Adaptive Active Message Coalescing in Task Based Runtime Systems},
  author={Wagle, Bibek and Kellar, Samuel and Serio, Adrian and Kaiser, Hartmut},
  booktitle={2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={1133--1140},
  year={2018},
  organization={IEEE}
}
